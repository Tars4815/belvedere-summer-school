{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Teaching Material of the Belvedere Glacier Summer School","text":"<p>This repository contains the learning material for the Summer School \"Design and Implementation of Topographic Surveys for Territorial Monitoring in Mountain Environments\", organized at the Belvedere Glacier (Italian Alps) by the Section of Geodesy and Geomatics of the Department of Civil and Environmental Engineering (DICA), Politecnico di Milano.</p> <p>This Teaching Material was supported by the EGU Higher Education Teaching Material Grant 2023</p>"},{"location":"#the-summer-school","title":"The Summer School","text":"<p>In 2023, the 10th edition of the Summer School, organized by the Section of Geodesy and Geomatics within the Department of Civil and Environmental Engineering (DICA) at Politecnico di Milano, was curated. The Summer School, titled \"Design and Implementation of Topographic Surveys for Territorial Monitoring in Mountain Environments\", is aimed at undergraduate and graduate students in Civil and Environmental Engineering, Geoinformatics and Architecture.</p> <p>The main objective of the Summer School is to provide students with practical experience in the field of topographic land monitoring in mountain environments, a crucial topic that is often only taught in theory during classes. In particular, the Summer School focuses on surveying the Belvedere Glacier, a temperate debris-covered alpine glacier, located in the Anzasca Valley, municipality of Macugnaga, Italy, to monitor the glacier's annual evolution. The 10th edition of the Summer School took place from July 23rd to July 28th 2023.</p> <p>The Summer School is part of a long-term monitoring activity of the Belvedere Glacier, where annual in-situ GNSS and UAV photogrammetry surveys have been carried out since 2015 to derive accurate and complete 3D models of the entire glacier, allowing the derivation of glacier velocity and volume variations over the last 10 years (Ioli et al., 2022). We believe that the integration of the Summer School into an ongoing research project adds value to the experience, as it allows students to work in close contact with young tutors who are passionate about the topic and contribute to the acquisition of data that will be effectively used by researchers.</p> <p>The Summer School is part of the \"Passion in action\" initiative organized by Politecnico di Milano. At the end of the experience, students receive a certificate of attendance, and the course is awarded 3 CFUs, which are included in the Diploma Supplement attached to the degree. We expect between six and twelve students to attend the Summer School. In previous editions of the Summer School, an average of ten enrolled students participated.</p> <p>The program of the Summer School lasts one week and uses the Zamboni Zappa Hut (2070 m. a.s.l.) as the main base for the activities. These begin on Sunday, with the transportation of survey material to the Zamboni Zappa Refuge and a plenary seminar introducing the participants to the Summer School activities. From Monday to Thursday field activities take place on the glacier. These activities include the planning of topographic and photogrammetric surveys with UAV's, measurements with GNSS and laser scanner instruments, and initial data processing. On Friday, students organize the preliminary results that can be derived from the measured data and present their work to a panel of professors and researchers from the DICA department.</p> <p>All data collected on Belvedere Glacier, including the 3D models of the glacier, the orthophotos, and the Digital Surface Models (DSM), are published as open data in a public repository on Zenodo (https://doi.org/10.5281/zenodo.7842348).</p> <p>On this website we have created a comprehensive section for teaching material on the basics of photogrammetry, GNSS and GIS, including the basics of raster processing, planning UAV photogrammetry acquisitions, and using common structure-from-motion software packages such as Agisoft Metashape. More information about the research activities on Belvedere Glacier can be found on the research group's website https://labmgf.dica.polimi.it/projects/belvedere/.</p> <p>We strongly believe that this summer school and its learning material offer a unique opportunity for students to gain practical experience in the field of topographic land monitoring, which is crucial in the current scenario of climate change.</p>"},{"location":"#course-contents","title":"Course contents","text":"<ol> <li> <p>Introduction: This module gives a brief introduction to the course, providing a general overview of the summer school and the main topics covered during the course.</p> </li> <li> <p>Module 1: this module describes the monitoring activity carried out on the Belvedere Glacier with UAV photogrammetry and GNSS.</p> </li> <li> <p>Module 2: this modules give an introduction to photogrammetry.</p> </li> <li> <p>Module 3: this module gives an introduction to GNSS positioning.</p> </li> <li> <p>Module 4: this module gives an introduction to GIS and spatial data analysis.</p> </li> <li> <p>Module 5: this modules gives an introduction on the stereo processing from fixed-time-lapse cameras.</p> </li> <li> <p>Belvedere Open Data: this module describes the open data collected during the summer school.</p> </li> <li> <p>Software: this module describes the software used during the summer school.</p> </li> <li> <p>References: this module provides a list of references used during the summer school.</p> </li> </ol>"},{"location":"credits/","title":"Credits","text":"<p>The teaching material of the Belvedere glacier summer school collected on this openly accessible website has been prepared by members of the LabMGF of the Department of Civil and Environmental Engineering of Politecnico di Milano under the coordination of full professor Livio Pinto:</p> <ul> <li>Federico Barbieri, research fellow</li> <li>Rebecca Fascia, PhD candidate</li> <li>Federica Gaspari, PhD candidate</li> <li>Francesco Ioli, PhD candidate,</li> <li>Lorenzo Rossi, permanent research fellow</li> </ul>"},{"location":"credits/#author-contributions","title":"Author Contributions","text":""},{"location":"credits/#conceptualisation","title":"Conceptualisation","text":"<p>Federica Gaspari, Francesco Ioli, Livio Pinto</p>"},{"location":"credits/#data-preparation","title":"Data preparation","text":"<p>Francesco Ioli</p>"},{"location":"credits/#writing","title":"Writing","text":"<ul> <li>Module 1: Federico Barbieri</li> <li>Module 2: Federico Barbieri, Rebecca Fascia</li> <li>Module 3: Lorenzo Rossi</li> <li>Module 4: Federica Gaspari</li> <li>Module 5: Francesco Ioli</li> <li>Module 6: Federica Gaspari</li> <li>Belvedere Open Data: Francesco Ioli</li> </ul>"},{"location":"credits/#supervision","title":"Supervision","text":"<p>Livio Pinto</p>"},{"location":"credits/#website-implementation","title":"Website implementation","text":"<p>Federica Gaspari, Francesco Ioli</p>"},{"location":"data/","title":"Data","text":"<p>The Belvedere dataset contains extensive, long-term monitoring data on the Belvedere Glacier, a debris-covered glacier located on the east face of Monte Rosa in the Anzasca Valley of the Italian Alps. The data can be accessed from the Zenodo repository https://zenodo.org/record/7842348</p> <p>The data is derived from photogrammetric 3D reconstruction of the full Belvedere Glacier and includes:</p> <ul> <li>dense point clouds obtained with Structure-from-Motion (SfM) covering the entire glacier body</li> <li>high-resolution orthophotos</li> <li>high-resolution DEMs</li> </ul> <p>Since 2015, in-situ survey of the glacier have been conducted annually using fixed-wing UAVs until 2020 and quadcopters from 2021 to 2022 to remotely sense the glacier and build high-resolution photogrammetric models. A set of ground control points (GCPs) were materialized all over the glacier area, both inside the glacier and along the moraines, and surveyed (nearly-) yearly with topographic-grade GNSS receivers (Ioli et al., 2022).</p> <p>For the period from 1977 to 2001, historical analog images, digitalized with photogrammetric scanners and acquired from aerial platforms, were used in combination with GCPs obtained from recent photogrammetric models (De Gaetani et al., 2021).</p>"},{"location":"data/#data-organization","title":"Data organization","text":"<p>The data are organized by year in compressed zip folders named belvedere_YYYY.zip, which can be downloaded independently. Each folder contains all data available for that year (i.e. photogrammetric point clouds,  orthophotos and DEMs) with the corresponding metadata. Metadata is provided as a .json file with the same name as the data it refers to. Point clouds are saved in compressed las format (.laz) and they can be inspected e.g., with CloudCompare. Orthophotos and DEMs will be uploaded as georeferenced .tif images at a later date.</p>"},{"location":"data/#data-usage","title":"Data Usage","text":"<p>This dataset can be used to estimate glacier velocities, volume variations, study geomorphological processes such as the process of moraine collapse, or derive other information on glacier dynamics. If you have any request on the data provided, data acquisition or on the raw data themselves, you are encouraged to contact us.</p>"},{"location":"data/#contributions","title":"Contributions","text":"<p>The monitoring activity carried out on the Belvedere Glacier was designed and conducted jointly by the Department of Civil and Environmental Engineering (DICA) of Politecnico di Milano and the Department of Environment, Land and Infrastructure Engineering (DIATI) of Politecnico di Torino. The DREAM projects (DRone tEchnnology for wAter resources and hydrologic hazard Monitoring), involving teachers and students from Alta Scuola Politecnica (ASP) of Politecnico di Torino and Milano, contributed to the campaign from 2015 to 2017.</p>"},{"location":"data/#if-you-use-the-data-please-cite-these-our-publications","title":"If you use the data, please, cite these our publications:","text":"<p>Ioli, F.; Bianchi, A.; Cina, A.; De Michele, C.; Maschio, P.; Passoni, D.; Pinto, L. Mid-Term Monitoring of Glacier\u2019s Variations with UAVs: The Example of the Belvedere Glacier. Remote Sensing, 2022, 14, 28. https://doi.org/10.3390/rs14010028</p> <p>De Gaetani, C.I.; Ioli, F.; Pinto, L. Aerial and UAV Images for Photogrammetric Analysis of Belvedere Glacier Evolution in the Period 1977\u20132019. Remote Sensing, 2021, 13, 3787. https://doi.org/10.3390/rs13183787</p>"},{"location":"introduction/","title":"Belvedere","text":"<p>This module introduce the Belvedere Glacier.</p>"},{"location":"introduction/#table-of-contents","title":"Table of contents","text":"<ol> <li> <p>The Belvedere Glacier: a brief description of the Belvedere Glacier.</p> </li> <li> <p>Glacier Hazards and surge event of 2001-2002: a brief description of the glacier hazards and the surge event of 2001-2002.</p> </li> <li> <p>Main morphological sectors: a brief description of the main morphological sectors of the Belvedere Glacier.</p> </li> </ol>"},{"location":"introduction/#the-belvedere-glacier","title":"The Belvedere Glacier","text":"<p>The Belvedere Glacier (Randolph Glacier Inventory code RGI60-11.02858) is an alpine glacier in Valle Anzasca (Italy), on the east side of the Monte Rosa Massif (located at approximately 45\u25e6 58\u2019 N 7\u25e6 55\u2019 E) (Fig. 1a). The lower part of the Belvedere Glacier is a temperate debris-covered glacier, that covers an area of \u223c1.8 km2 and extends from an altitude of \u223c2250 m a.s.l. to \u223c1800 m a.s.l. This region is characterized by a gentle slope, and it is fed by ice falls and snow avalanches coming from the Monte Rosa East Face. It covers an area of \u223c1.8 km2 and it is mainly elongated in South-North direction, with a length of \u223c3000 m and a maximum width of only \u223c500 m. Similarly to Miage glacier (Monte Bianco, Valle d\u2019Aosta), the Belvedere Glacier is almost completely covered by rocks and debris. In its low-relief sector, the Belvedere Glacier splits into two lobes. The two glacier lobes reach \u223c1800 m a.s.l. The northern lobe ends with a prominent ice cliff, from which the River Anza springs. Except from the terminal ice cliff, the glacier tongue is completely covered by rocks and boulders with dimensions ranging from few decimetres to some meters.</p> <p> Figure 1: Location of Belvedere Glacier (base map source: Swiss Federal Office of Topography);</p>"},{"location":"introduction/#glacier-hazards-and-surge-event-of-2001-2002","title":"Glacier Hazards and surge event of 2001-2002","text":"<p>In the past, several hazardous events originated by Belvedere Glacier, such as floods and slope instability, threatened the nearby village of Macugnaga and the Zamboni Zappa Hut, at 2070 m a.s.l. (K\u00e4\u00e4b et al., 2004). At the beginning of the 21st century, the Belvedere Glacier was characterized by a particular surge-type dynamics (Haeberli et al., 2002): a wave of compression-decompression stresses was generated by an accelerated ice flow derived from the glacier accumulation area. Surface velocities reached up to 200 m y\u22121 (K\u00e4\u00e4b et al., 2004) and the ice thickness increased more than 20 m. A more detailed description of the characteristics of the Belvedere Glacier and the glacier-related hazards can be found in previous studies, such as (Ioli et al., 2022; K\u00e4\u00e4b et al., 2004; Haeberli et al., 2002).</p> <p> Figure 2: The bifurcation of the frontal tongue of Belvedere Glacier: the mighty swelling of the glacier mass is clearly visible (photo G. Mortara, 21.06.2002);</p> <p> Figure 3: Right lateral moraine, early October 2001. Note the increase in thickness and the invasion by ice (in the foreground) of the breccia on the right moraine (effect of the Locce glacial lake rout, which occurred on July 19, 1979) (photo G. Mortara, October 2001);</p>"},{"location":"introduction/#main-morphological-sectors","title":"Main morphological sectors","text":"<p>From the morphological point of view, the Belvedere Glacier can be roughly divided into three sectors:</p> <ul> <li> <p>Upper sector (labelled as S1 in Fig. 4):   it consists of the accumulation zone. It is located at about 2250 m a.s.l., at the feet of the steep Monte Rosa and the North Locce Glaciers, from which recurrent ice and snow avalanches feed the Belvedere Glacier. This sector is also the main deposition area for rocks and debris (Diolaiuti et al.,, 2003; Molg et al., 2020).</p> </li> <li> <p>Central sector (S2 in Fig. 4):   it is the transfer zone and it is enclosed by two sinuous moraines. It starts from an altitude of \u223c2250 m a.s.l. and it extends downwards for \u223c1500 m. This sector shows the highest ice flow velocities and the most irregular surfaces, with the presence of several crevasses.</p> </li> <li> <p>Lower sector (S3 in Fig. 4):   it is the low relief sector. Here, in proximity of the Belvedere hill, the glacier splits in two different tongues: the north-west tongue is the largest and it reaches the lowest altitude of about 1800 m a.s.l. From the north-west tongue, the Anza River springs. A smaller tongue extends from the Belvedere hill towards East and reaches an altitude of about 1850 m a.s.l.</p> </li> </ul> <p> Figure 4: Location of the Belvedere Glacier with superimposed the three main morphological sectors: sector 1 (S1) is the accumulation zone, sector 2 (S2) is the transfer zone, sector 3 (S3) is the low-relief zone with the two glacier tongues. Coordinates are framed in ETRF2000(2008)\u2014UTM 32N. [Basemap source: Swisstopo (geo.admin.ch) ].</p>"},{"location":"references/","title":"References","text":"<p>De Gaetani, C.I.; Ioli, F.; Pinto, L. Aerial and UAV Images for Photogrammetric Analysis of Belvedere Glacier Evolution in the Period 1977\u20132019. Remote Sensing, 2021, 13, 3787. https://doi.org/10.3390/rs13183787</p> <p>Diolaiuti, G., D'Agata, C., &amp; Smiraglia, C. (2003). Belvedere Glacier, Monte Rosa, Italian Alps: tongue thickness and volume variations in the second half of the 20th century. Arctic, Antarctic, and Alpine Research, 35(2), 255-263. https://doi.org/10.1657/1523-0430(2003)035[0255:BGMRIA]2.0.CO;2</p> <p>Girardeau-Montaut, D. (2016). CloudCompare. France: EDF R&amp;D Telecom ParisTech, 11(5).</p> <p>Haeberli, W., K\u00e4\u00e4b, A., Paul, F., Chiarle, M., Mortara, G., Mazza, A., Deline, P., Richardson, S., 2002. A surge-type movement at Ghiacciaio del Belvedere and a developing slope instability in the east face of Monte Rosa, Macugnaga, Italian Alps. Nor. Geogr. Tidsskr. 56, 104\u2013111. https://doi.org/10.1080/002919502760056422</p> <p>Ioli, F.; Bianchi, A.; Cina, A.; De Michele, C.; Maschio, P.; Passoni, D.; Pinto, L. Mid-Term Monitoring of Glacier\u2019s Variations with UAVs: The Example of the Belvedere Glacier. Remote Sensing, 2022, 14, 28. https://doi.org/10.3390/rs14010028</p> <p>Ioli, F., Bruno, E., Calzolari, D., Galbiati, M., Mannocchi, A., Manzoni, P., Martini, M., Bianchi, A., Cina, A., De Michele, C., Pinto, L., 2023. a Replicable Open-Source Multi-Camera System for Low-Cost 4D Glacier Monitoring. Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. - ISPRS Arch. 48, 137\u2013144. https://doi.org/10.5194/isprs-archives-XLVIII-M-1-2023-137-2023</p> <p>Ioli, F., Dematteis, N., Giordan, D. et al. Deep Learning Low-cost Photogrammetry for 4D Short-term Glacier Dynamics Monitoring. PFG (2024). https://doi.org/10.1007/s41064-023-00272-w</p> <p>K\u00e4\u00e4b, A., Huggel, C., Barbero, S., Chiarle, M., Cordola, M., Epifani, F., Haeberli, W., 2004. Glacier Hazards At Belvedere Glacier and the Monte Rosa East Face , Italian Alps\u202f: Processes and Mitigation. Inter Natl. Symp. INTERPRAEVENT 2004 \u2013 RIVA / TRIENT 67\u201378.</p> <p>McKenna, J., &amp; PostGIS Team. (2021). PostGIS 3.2.0, The Olivier Courtin Release (3.2.0). Zenodo. https://doi.org/10.5281/zenodo.5879632</p> <p>M\u00f6lg, N., Ferguson, J., Bolch, T., &amp; Vieli, A. (2020). On the influence of debris cover on glacier morphology: How high-relief structures evolve from smooth surfaces. Geomorphology, 357, 107092. https://doi.org/10.1016/j.geomorph.2020.107092</p> <p>QGIS Contributors. (2022). QGIS (3.22.3). Zenodo. https://doi.org/10.5281/zenodo.5869838</p>"},{"location":"software/","title":"Software","text":"<p>[...]</p>"},{"location":"module1/instrumental-techniques/","title":"Instrumental techniques","text":"<p>Surveying activities are based on two measurement techniques: UAV (Unmanned Aerial Vehicles) photogrammetry and GNSS (Global Navigation Satellite System) surveying. </p>"},{"location":"module1/instrumental-techniques/#uav-photogrammetry","title":"UAV Photogrammetry","text":"<p>UAV photogrammetry involves using drone aerial images to create a three-dimensional model of the terrain. To ensure accurate results, the images must meet specific criteria such as having sufficient overlaps and being taken at a precise altitude relative to the terrain. Further details on these criteria will be addressed in Module 2. </p> <p>As a recent technique, this field is advancing rapidly with varied technological developments over time. Specifically, in Belvedere surveys there has been a transition from fixed-wing drones (showing higher flight autonomy but lesser carrying capability) to multi-copter drones (capable of carrying more weight and thus improved sensors).</p> <p></p> <p></p>"},{"location":"module1/instrumental-techniques/#gnss-survey","title":"GNSS Survey","text":"<p>Precision GNSS measurement in high mountain environments involves seeveral issues that must be considered, in order to achieve optimal accuracy (sub-decimetre level). These aspects will be dealt with in more detail in Module 3 of the course. In any case, it is essential to use high-precision GNSS instrumentation. Depending on the measurement mode, it may be sufficient to use a single receiver (rover) or a two-receiver system (base, stationary on one point, and rover that moves).</p> <p></p> <p></p> <p>GNSS receivers are used to periodically acquire the position of a series of points distributed all over the area. During the first measurement campaign (year 2015) this point network was designed and materialised, trying to distribute them completely and uniformly, both inside and outside the glacier. These points in fact perform two very important functions: the first is to act as georeferencing and control points for the photogrammetric models, while the second is to obtain information on the surface displacements of the glacier, through several measurements of the same point at different years. The points are materialised by plastic chequerboard targets, permanently fixed to the ground on large boulders. Their arrangement was decided bearing in mind the visibility of the marker from the drone, the accessibility of their position and the ease of finding the points in future campaigns. Due to the severe weather and atmospheric conditions to which they are subjected, each year each marker is checked, and replaced if it is too worn.</p> <p></p>"},{"location":"module1/module1/","title":"Overview","text":"<p>The module 1 of the course is dedicated to the presentation of the monitoring activities of the Belvedere glacier surface. </p>"},{"location":"module1/module1/#learning-outcomes","title":"Learning outcomes","text":"<p>The aim of this module is to show students some methods of topographic monitoring applied to a natural environment undergoing profound and rapid transformation, such as the Belvedere glacier. It will also be possible to assess the results that this kind of campaigns can lead to, in terms of quantitative evaluation of the morphological changes occurred.</p>"},{"location":"module1/module1/#table-of-contents","title":"Table of contents","text":"<ol> <li> <p>Instrumental techniques: brief introduction to the main survey techniques: UAV photogrammetry and GNSS.</p> </li> <li> <p>Monitoring campaigns: this section provides a summary of the techniques and approaches used in the campaigns carried out between 2015 and 2022.</p> </li> <li> <p>Monitoring results: this page will present the main results obtained from the monitoring. In particular, we will investigate the surface velocity field of the glacier and its volumetric variation.</p> </li> </ol>"},{"location":"module1/monitoring-campaigns/","title":"Belvedere Glacier Monitoring Campaigns","text":"<p>Starting from the year 2015, several research teams from the Alta Scuola Politecnica (ASP Politecnico di Milano and Politecnico di Torino), and from Department of Civil and Environmental Engineering (DICA, Politecnico di Milano), have annually conducted UAV surveys over the entire Belvedere glacier with the aim of creating three-dimensional models. Comparing these models would then allow for the estimation of annual changes in the glacier's surface and the surrounding terrain.  Over the years, the equipment used has changed and evolved, consequently altering the methods and outcomes of the surveys. In the following paragraphs, a summary will be provided with the main characteristics of each measurement campaign.</p> <p>A full description of the monitoring campaign carried out between 2015 and 2023 can be found in Ioli et al. (2022).</p>"},{"location":"module1/monitoring-campaigns/#2015","title":"2015","text":"<p>The 2015 campaign flight was carried out by the DREAM1 Team of the Alta Scuola Politecnica (ASP) during the month of October using the eBee SenseFly professional fixed-wing drone. The camera mounted on board had the following characteristics:</p> <p></p> Model Focal length Image dimension Canon PowerShot S110 5,2 [mm] 4048x3048 [pixel] <p>The model was composed from 2118 images. The average resolution (ground pixel size) was 7 cm and the final model accuracy was about 11.5 cm.</p>"},{"location":"module1/monitoring-campaigns/#2016","title":"2016","text":"<p>In 2016, the ASP DREAM2 Team was in charge of the Belvedere Glacier measurement campaigns.  The flight dates back to 20 October 2016, carried out with the same fixed-wing drone used during the 2015 campaign: eBee SenseFly, with the same Canon PowerShot S110 camera on board. The model was composed from 1085 images. The average ground resolution was 9 cm and the final accuracy 11 cm.</p>"},{"location":"module1/monitoring-campaigns/#2017","title":"2017","text":"<p>The 2017 campaign was characterised by 3 different surveys, carried out by the ASP DREAM3 Team. The flights were carried out during the months of October and November: </p> <ol> <li> <p>2017, October 5th:\u202fSenseFly eBee fixed-wing drone equipped with Canon PowerShot S110 camera; </p> </li> <li> <p>2017, November 15th: SenseFly eBee plus PPK fixed-wing drone equipped with SenseFly S.O.D.A camera with the following characteristics:</p> </li> </ol> Model Focal length Image dimension SenseFly S.O.D.A. 10.2 [mm] 5472x3648 [pixel] <p>This drone, compared to the previously used standard eBee, is equipped with a precision GNSS system, which allows for accurate georeferencing of image locations. </p> <ol> <li>2017, November 16th: DJI Phantom 4 Pro quadricopter drone, with integrated DJI FC6310 camera:</li> </ol> <p></p> Model Focal length Image dimension DJI FC6310\u202f 8.8 [mm] 5472x3648 [pixel] <p>The photogrammetric models obtained were then merged, reconstructing the entire glacier. The complete model was composed from 1221 images. The average ground resolution was 5.3 cm and the final accuracy 5.5 cm.</p>"},{"location":"module1/monitoring-campaigns/#2018","title":"2018","text":"<p>Since 2018, the measurement campaigns have all been carried out by the DICA summer school of the Politecnico di Milano, which always takes place at the end of July. For the 2018 campaign, the Parrot Disco FPV fixed-wing drone was used.  This drone, which is no longer in production, was marketed for recreational purposes. However, thanks to some modifications developed by the DICA staff, it was possible to make it effective for photogrammetric purposes. In fact,  through the necessary modifications, a housing was created inside that allowed the HawkEye FireFly 8S action cam to be mounted. </p> <p></p> <p></p> Model Focal length Image dimension HawkEye FireFly 8S\u202f 3.8 [mm] 4608x3456 [pixel] <p>The compact size and low weight of this camera make it an ideal choice despite its lower quality respect to the previous ones. Nevertheless, the model's quality is consistent with that of previous years. The model was created using 1421 images, with an average ground resolution of 5.4 cm and a final accuracy of 3.5 cm.</p>"},{"location":"module1/monitoring-campaigns/#2019","title":"2019","text":"<p>For the 2019 survey, the same setup as in 2018 (Parrot Disco PSV drone, HawkEye FireFly 8S camera) was used. The drone, camera, and frame acquisition and orientation methods remain the same as the previous year. The model was composed from 1442 images, reaching an average ground resolution of 5.7cm, and final accuracy of 3.8 cm. </p>"},{"location":"module1/monitoring-campaigns/#2020","title":"2020","text":"<p>During the 2020 expedition, the identical drone from previous years was deployed (Parrot Disco PSV equipped with HawkEye FireFly 8S). Nevertheless, operational difficulties were encountered due to camera damage, which resulted in capturing corrupted images of the glacier\u2019s upper section. Subsequently, a DJI Phantom 4 Pro flight was executed over the upper region 15 days later. The georeferencing of this survey presented several challenges, as the glacier surface had shifted significantly in the intervening 15 days since the GNSS measurements were taken. Consequently, it was deemed necessary to create separate models for the upper and lower parts. The low-part model comprises of 725 images, with an average resolution of 5.4 cm and accuracy of 10.5 cm. The upper-part model consists of 729 images, with a resolution of 5 cm and accuracy of 18 cm.</p>"},{"location":"module1/monitoring-campaigns/#2021","title":"2021","text":"<p>In the year 2021, the vehicle was changed once again: a DJI Matrice 210, a professional multi-rotor drone equipped with the DJI Zenmuse X5 camera, was used. The drone also featured a precision GNSS system, built by DICA staff, which was able to provide the position of the images at the times of the shooting.</p> <p></p> Model Focal length Image dimension DJI Zenmuse X5\u202f 15 [mm] 5280 x 3956 [pixel] <p>1473 images were taken, the average resolution obtained was 3.7 cm and the error obtained was 10 cm.</p>"},{"location":"module1/monitoring-campaigns/#2022","title":"2022","text":"<p>In 2022, the drone with which the survey was carried out was further renewed: the DJI Matrice 300, an advanced version of the 210, was used. The new features are a better quality camera (DJI Zenmuse P1) and a directly embedded GNSS system capable of correcting camera positions in real time.</p> <p></p> Model Focal length Image dimension DJI Zenmuse X5\u202f 35 [mm] 8192\u00d75460 [pixel] <p>2469 images were taken, the average resolution obtained was 2.7 cm and the error obtained was 11.5 cm.</p>"},{"location":"module1/monitoring-campaigns/#uav-surveys-summary","title":"UAV Surveys Summary","text":"<p>The table below summarizes all UAV campaigns and their main features.</p> YEAR PERIOD UAS CAMERA AUTHOR NUMBER OF IMAGES ACCURACY [cm] 2015 October Ebee SenseFly Canon PowerShot S110 ASP 2118 11.5 2016 October Ebee SenseFly Canon PowerShot S110 ASP 1085 11 2017 October-November Ebee SenseFly, Ebee SenseFly plus PPK, DJI Phantom 4 Pro Canon PowerShot S110 SenseFly S.O.D.A DJI FC6310 ASP 1221 5.5 2018 July Parrot Disco HawkEye FireFly 8S Summer School DICA 1421 3.5 2019 July Parrot Disco HawkEye FireFly 8S Summer School DICA 1442 3.8 2020 July-August Parrot Disco, DJI Phantom 4 Pro HawkEye FireFly 8S, DJI FC6310 Summer School DICA 1473 10.5 (low area), 18.8 (high area) 2021 July DJI Matrice 210 Dji Zenmuse X5 Summer School DICA 1473 9.9 2022 July DJI Matrice 300 RTK Dji Zenmuse P1 Summer School DICA 2469 11.5"},{"location":"module1/monitoring-results/","title":"Monitoring Campaign Results","text":"<p>This chapter will briefly describe the results derived from comparing three-dimensional models of successive surveys in terms of morphological changes of the glacier surface.  </p> <p>In particular, these changes will be described in terms of surface velocity field and volumetric variation.</p>"},{"location":"module1/monitoring-results/#surface-velocity-field","title":"Surface Velocity Field","text":"<p>A glacier is not a stationary object, but behaves like a very viscous fluid that moves slowly under gravity effect. The entity of this movement depends on many factors, such as the slope of the bedrock, the presence of water beneath the ice surface and the temperature. These phenomena and their interactions can be very complex and are still being studied by glaciologists. Reliable and repeated morphological data over time are therefore essential for a better interpretation of these dynamics. </p> <p>On Belvedere Glacier, the combination of GNSS and photogrammetric data allowed the estimation of surface velocity fields and their variation during the monitoring period. Two different datasets were used for this purpose:</p> <ol> <li> <p>GNSS points: These are the points measured annually inside the glacier, the same points used as ground control points in the photogrammetric processing. By calculating the annual displacement of the targets and dividing it by the time period, it was possible to obtain the annual velocity of each target. These measurements are very reliable (centimetre accuracy). They were therefore considered as a reference source for the estimation of flow velocity.</p> </li> <li> <p>Manual Points: The number of GNSS points is not sufficient to estimate a complete glacier velocity field. Therefore, orthophotos derived from photogrammetric models were used to identify certain characteristic points (such as boulder edges on the glacier surface). An orthophoto is a rectified and georeferenced image, which makes it effectively a cartography, as will be explained in more detail in Module 2 of this course. It was therefore possible to assess the degradation at a greater number of points, even though with less reliability than GNSS points.</p> </li> </ol> <p></p> <p></p> <p>All points contributed to a comprehensive estimate of the glacier's velocity fields. The results showed velocities between 2 m/y and 22 m/y for the period 2015-2020. Three distinct sectors can be identified:  </p> <ol> <li> <p>Upstream zone: corresponds to the accumulation zone and is characterised by velocities between 2 and 15 m/y; </p> </li> <li> <p>Central zone: this is the zone with the highest velocities (15-22 m/y). Since 2018, there seems to be a further acceleration in this area compared to previous years. </p> </li> <li> <p>Tongue zone: slower speeds, never exceeding 10 m/y.</p> </li> </ol> <p></p>"},{"location":"module1/monitoring-results/#volume-variation","title":"Volume Variation","text":"<p>One of the most known effects of climate change is glaciers reduction. Belvedere is no exception, as the thermal insulation provided by debris cover is not sufficient to preserve the ice from melting. By comparing DTMs (Digital Terrain Models, obtained from photogrammetric surveys, Module 2), it is possible to quantify annual volumetric changes. The figure below shows the year-by-year changes with their uncertainties. It can be seen that in the period of the analysis, the volume loss over the entire glacier was about 3 million of m3 per year. The year 2107-2018 seems much lower than the others, but this depends on the surveys period: 2017 survey was carried out in November, while the 2018 one in July: in this way, the melt of summer 2017 was not considered and therefore the overall melt results lower than the average.</p> <p></p> <p>For the purpose of assessing the variation in ice thickness, comparisons of several characteristic cross sections of the glacier in different years were made. In each profile, the ice section is clearly recognizable, as it is sunken between the lateral moraines. The greatest reductions in thickness (about 2 m/y) are observed on sections AA' and BB', at the terminal tongues. In these sections, the profile shape remains almost constant.  In the central part of the glacier (sections CC' and DD'), the ice reduction is less regular, due to the numerous crevasses in this area, but it\u2019s always about 1 m/y.</p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"module2/intro-metashape/","title":"Introduction to Agisoft Metashape","text":""},{"location":"module2/intro-metashape/#what-is-agisoft-metashape","title":"What is Agisoft Metashape?","text":"<p>Agisoft Metashape is a stand-alone software product that processes digital images (aerial and close-up) through photogrammetry to generate 3D spatial data. The data is suitable for use in a range of applications such as GIS, cultural heritage documentation, visual effects production, and indirect measurement of objects of varying scales. The technology also enables the integration of laser scans and the seamless fusion of LiDAR and camera data within a single project. The software then allows images to be processed from RGB to spatial information in the form of point clouds, textured polygonal models and DEM.</p>"},{"location":"module2/intro-metashape/#what-are-the-input-and-output-data","title":"What are the input and output data?","text":"<p>Input data</p> <ul> <li> <p>Photos: in the practical case they were taken by drone flight; these photos are also geolocated due to the GNSS system on board the drone.</p> </li> <li> <p>Camera Calibration.</p> </li> <li> <p>GPS coordinates of known fixed points, called Ground Control Points (GCP). </p> </li> </ul> <p>Output data</p> <ul> <li> <p>Dense cloud </p> </li> <li> <p>Mesh </p> </li> <li> <p>DTM </p> </li> <li> <p>Orthophoto</p> </li> </ul>"},{"location":"module2/intro-metashape/#graphic-user-interface","title":"Graphic User Interface","text":"<p>The GUI of Agisoft Metashape Desktop is mainly composed of: </p> <ol> <li> <p>Toolbar: this is the main menu, where you can find all the necessary commands. </p> </li> <li> <p>Workspace panel: this section presents all the components and results that make up this project. </p> </li> <li> <p>Reference panel: this section presents all the technical components that make up this project. </p> </li> <li> <p>Photo view: the selected photographs for the project are on display here. </p> </li> <li> <p>Console: in this section, users can access the command history, auxiliary information, and error messages. </p> </li> <li> <p>Model view: it is where 3D data are displayed and for editing meshes and point clouds.</p> </li> </ol> <p></p>"},{"location":"module2/intro/","title":"Introduction to photogrammetry","text":""},{"location":"module2/intro/#what-is-photogrammetry","title":"What is photogrammetry?","text":"<p>Photogrammetry is a technique that allows the reconstruction of three-dimensional objects from images, both in terms of geometry and metric position.</p> <p>This technique is used for:</p> <ol> <li> <p>Production and updating of cartography;</p> </li> <li> <p>Production of Digital Elevation Models;</p> </li> <li> <p>3D modelling;</p> </li> <li> <p>Terrestrial photogrammetry (architectural surveys).</p> </li> </ol>"},{"location":"module2/intro/#geometrical-principles-of-photogrammetry","title":"Geometrical Principles of Photogrammetry","text":"<p>Photogrammetry is theoretically based on the principle of collinearity, which is the relationship between:</p> <ol> <li> <p>Image point</p> </li> <li> <p>Acquisition point</p> </li> <li> <p>Object point</p> </li> </ol> <p></p> <p>In other words, it is possible to mathematically describe an optical ray joining these three points. This description is called collinearity equation.</p> <p>Knowing the position and orientation of the images, it is possible to determine the position of the object point in space by intersecting the homologous rays. It is therefore necessary for each point to be visible in at least two images.</p> <p>Collinearity equations relate image space (2D) to object space (3D) by modelling:</p> <ul> <li> <p>Position in image space of the point (x, y);</p> </li> <li> <p>Camera calibration parameters: principal point (x0, y0), principal distance (c) and lens distortion coefficients;</p> </li> <li> <p>External camera parameters: position coordinates (X0, Y0, Z0) and rotation angles (w, f, k);</p> </li> <li> <p>Coordinates in real space of the homologous point (X, Y, Z).</p> </li> </ul> <p></p> <p></p> <p></p>"},{"location":"module2/intro/#the-photogrammetric-process","title":"The photogrammetric process","text":"<p>A photogrammetric survey consists of a process composed of well-defined steps. Each of these contains several aspects that must be considered account to obtain a good result. The main steps are described in the figure, and will be discussed in more detail later.</p> <p></p>"},{"location":"module2/intro/#image-acquisition","title":"Image acquisition","text":"<p>The acquisition of images should be planned carefully. In territorial surveys, like the Belvedere Glacier one, images are obtained through a series of parallel 'flight strips' in order to cover the whole area of interest.</p> <p>The key parameter to set is the Ground Sample Distance (GSD), which refers to the size of the pixel footprint on the ground. This parameter is crucial as it indicates the smallest visible detail in the images, directly impacting the accuracy and resolution of the final model.</p> <p></p> <p>The GSD is determined by the camera's characteristics (focal length and sensor size) and flight altitude, following this formula:</p> <p></p> <p>Where:</p> <ul> <li> <p>h: relative flight height [m]</p> </li> <li> <p>c: principal distance [mm]</p> </li> <li> <p>\ud835\udc53\ud835\udc64 (\ud835\udc53\u210e): sensor size [mm] along width (length) direction</p> </li> <li> <p>\ud835\udc4a (\ud835\udc3b): image footprint along width (length) [m]</p> </li> <li> <p>\ud835\udc5b\ud835\udc5d\ud835\udc65 (\ud835\udc5b\ud835\udc5d\ud835\udc66): number of pixels along sensor width/length direction</p> </li> </ul> <p>Once the GSD has been set, the overlap between images should be configured, divided into the following:</p> <ul> <li> <p>Longitudinal: in the direction of the flight path (70-80%)</p> </li> <li> <p>Transverse: between two adjacent flight paths (60-70%)</p> </li> </ul> <p>Additionally, it is important to consider practical factors including terrain features, battery capacity, flight speed, camera shutter speed, and airspace regulations in the relevant area. Flight planning is currently carried out by specialised software (such as UgCS, DJI Pilot and many others) which can create plans that optimise all the outlined aspects.</p>"},{"location":"module2/intro/#camera-calibration-and-internal-orientation","title":"Camera Calibration and Internal Orientation","text":"<p>The process for determining the internal orientation parameters of a camera is called lens calibration. These parameters include the principal distance (c), the position of the principal point (x0, y0), and lens distortion coefficients. Every lens, even the most precise one, produces image deformations. Accurately modelling these deformations is necessary to achieve good results in photogrammetric restitution.</p> <p>The most common distortion model is the Brown\u2019s model. It is characterized by 10 parameters that represent the different components of the distortion:</p> <ul> <li> <p>Radial distortion: it is modelled by an odd polynomial function. It is the main component (coefficients: K1, K2, K3);</p> </li> <li> <p>Tangential distortion: it represents the not exact concentricity of the lens to the optical axis. It is an order of magnitude smaller than the radial distortion (coefficients: P1, P2);</p> </li> <li> <p>Affine distortion: it models the fact that the pixel is not exactly a square (coefficient: b1);</p> </li> <li> <p>Skew distortion: it models the fact that the pixel can be parallelogram instead of a square (coefficient: b2).</p> </li> </ul> <p></p> <p>Camera calibration can be performed in two ways:</p> <ul> <li> <p>Laboratory calibration: Procedure conducted in laboratory employing specialised software and guided systems for acquiring and analysing data;</p> </li> <li> <p>Self-calibration: performed in-flight during image processing, with the aim of seeking calibration parameters that maximize the quality of the results on a statistical level.</p> </li> </ul>"},{"location":"module2/intro/#external-orienntation","title":"External Orienntation","text":"<p>External orientation in photogrammetry refers to the determination of the spatial position and orientation of a camera or sensor in relation to the objects being photographed. It involves establishing the exact location (3D coordinates X0, Y0, Z0) and orientation (rotation angles w, f, k) of the camera or sensor at the moment each photograph is taken. This information is crucial for accurately measuring and mapping objects or terrain from the photographs.</p> <p>External orientation can be:</p> <ul> <li> <p>Relative: determines the geometric relationships between multiple images taken from different positions and angles, with the goal of obtaining the spatial orientation of these images relative to one another. Therefore, the real position in the object reference system remains unknown. It is based exclusively on the positions of homologous points identified in common between the images and it does not require any external information.</p> </li> <li> <p>Absolute: determines the orientation of the block of images directly in the object reference system. It requires some information coming from external sensors (GNSS, IMU) about the position of images or specific measured points (GCP, Ground Control Points) on the object.</p> </li> </ul> <p>Digital photogrammetry makes possible to automate the simultaneous orientation of many images. Software based on Structure-from-Motion algorithms, as Metashape, are able to automatically recognise and match a large number of homologous points (tie points) and restore the external orientation of the 'image block'.</p> <p>This operation is called Bundle Block Adjustment (BBA) and consists of the statistical solution of the system of equations given by the collinearity equations (2 for each tie point per image) and the GCP information's.</p> <p>The solution provides estimation values of:</p> <ul> <li> <p>Object coordinates of tie points;</p> </li> <li> <p>External image orientation;</p> </li> <li> <p>Camera internal orientation (in case of self-calibration)</p> </li> </ul> <p></p> <p>The large number of equations generates a redundant system, that can be solved by least squares adjustment, to minimize the differences between observations and estimated values.</p> <p>If feasible, it is possible to use a priori information regarding the external orientation of the drone's cameras, obtained from sensors like GNSS (for positions) and IMU (for angles), in the BBA. This is often the case of Belvedere glacier surveys.</p>"},{"location":"module2/intro/#ground-control-points","title":"Ground Control Points","text":"<p>Ground Control Points (GCPs) are crucial components for successful photogrammetry, as they enable the assignment of the desired reference system to the survey and the estimation of model errors. These points are measured directly on the area/object of interest using GNSS or classical topographic instruments.</p> <p>GCPs should be:</p> <ul> <li> <p>Homogeneously distributed throughout the area;</p> </li> <li> <p>Clearly visible in the images: characteristic details or specially positioned targets;</p> </li> <li> <p>Measured at roughly the same time of flight, as their position should not change between GNSS measures and flight.</p> </li> </ul> <p>On Belvedere Glacier, GCPs are materialised by plastic targets with high colour contrast. They are measured annually by GNSS instrumentation on the same days as the drone flights: in fact, the glacier surface where the GCPs are positioned is in continuous motion.</p> <p></p>"},{"location":"module2/intro/#restitution","title":"Restitution","text":"<p>Restitution is the final stage of the photogrammetric process. Using the information from the external orientation, a large number of homologous points can be identified in the images, which will form a dense point cloud of sufficient number and density to reconstruct the object three-dimensionally. The dense cloud can be further processed in order to generate other photogrammetric products, among which are:</p> <ul> <li> <p>Mesh: Three-dimensional digital surface model of the object surveyed;</p> </li> <li> <p>DEM: Digital Elevation Model. Dataset representing surface elevation. It usually consists of a raster dataset, i.e. a matrix of regular cells with each cell having a corresponding elevation value;</p> </li> <li> <p>Orthomosaic: A georeferenced and orthorectified image of the surveyed area is produced by projecting the 3D surface (DEM, mesh) onto a specific plane and stitching together the individual images.</p> </li> </ul>"},{"location":"module2/module2/","title":"Overview","text":"<p>The module 2 of the course is dedicated to photogrammetry and photogrammetric data processing using Agisoft Metashape software.</p>"},{"location":"module2/module2/#learning-outcomes","title":"Learning outcomes","text":"<p>The aim of this module is to provide students with a basic knowledge of photogrammetry and 3D representation of complex areas using the Belvedere Glacier as an example. This will be achieved through a combination of theoretical concepts and practical exercises that will allow the student to understand the logical and practical approach of the subject.</p>"},{"location":"module2/module2/#table-of-contents","title":"Table of contents","text":"<ol> <li> <p>Introduction to photogrammetry: this section explains the basic concepts of photogrammetry, which are important for understanding the operation of Agisoft Metashape software.</p> </li> <li> <p>Introduction to Agisoft Metashape: this section explains what Agisoft Metashape is, what data is required, how the input data should be prepared and what the output data will be. An overview of the Graphical User Interface is also given.</p> </li> <li> <p>Workflow: this section explains the data processing procedure in Agisoft Metashape, starting from the creation of the point cloud to the main photogrammetric products.</p> </li> </ol>"},{"location":"module2/module2/#software","title":"Software","text":"<p>The adopted software for the practical sessions of this module is Agisoft Metashape.</p> <p>Agisoft Metashape (formerly Agisoft PhotoScan) is a stand-alone photogrammetry software. It was created in 2006 by Agisoft LLC, based in St Petersburg, Russia. The software comes in both Standard and Pro versions. The Standard version is appropriate for interactive multimedia purposes, whereas the Pro edition is tailored for the creation of GIS content.</p>"},{"location":"module2/module2/#installation","title":"Installation","text":"<p>The version to be used for the course exercises is the Agisoft Metashape 1.8.4, which can be downloaded from the Downloads page of the project website: https://www.agisoft.com/downloads/installer/</p> <ol> <li> <p>On the download page of the official site, select the Agisoft Metashape 1.8.4 version dedicated to your operating system (Windows, macOS, Linux).</p> </li> <li> <p>Save and execute the installation file.</p> </li> <li> <p>Once the procedure is complete, start Agisoft Metashape Desktop. A license is required to activate the software in the Pro version, or you can select the \"free 30-day trial\" mode or the demo version.</p> </li> </ol>"},{"location":"module2/workflow-metashape/","title":"Metashape Workflow","text":"<p>In Metashape environment, the process of photogrammetry to generate a dense point cloud and associated products comprises several sequential stages as presented below:</p> <ol> <li> <p>Import Images: Upload images</p> </li> <li> <p>Camera Calibration: Set interior orientation</p> </li> <li> <p>Setting Coordinate Reference System</p> </li> <li> <p>Insert GCPs coordinates: measure GCP on images</p> </li> <li> <p>Align Photos: Block formation (external orientation) -&gt; Sparse cloud</p> </li> <li> <p>Create Dense Cloud: Dense image matching -&gt; Dense cloud</p> </li> <li> <p>Create Mesh: 3D polygonal model</p> </li> <li> <p>Create DEM: Generate Digital Elevation Model</p> </li> <li> <p>Create Orthomosaic: Orthoimages</p> </li> <li> <p>Export Results: Export products and quality report</p> </li> </ol> <p></p>"},{"location":"module2/workflow-metashape/#import-images","title":"Import images","text":"<p>The images can be imported directly from the original folder.</p> <p>Command: Workflow \u2192 Add Photos</p> <p></p>"},{"location":"module2/workflow-metashape/#camera-calibration","title":"Camera calibration","text":"<p>If a camera calibration is available, it can be imported before the orientation phase, as a \u201cprecalibrated\u201d certificate. During the orientation, the parameters will be re-estimated starting from the original values. It is also possible to fix the coefficients to the initial values. If there is no prior orientation available, the software can carry out an estimation of internal orientation parameters, only based on BBA informations (auto-calibration).</p> <p>Command: Tools \u2192 Camera Calibration\u2192 Load a camera certificate</p> <p></p>"},{"location":"module2/workflow-metashape/#setting-coordinate-reference-system","title":"Setting Coordinate Reference System","text":"<p>It is crucial to assign the appropriate Reference System (RS) for the project, which can be a local coordinate system or a geographic/cartographic system. Metashape can manage different systems separately for cameras, GCPs, and the model. Additionally, it is possible to perform coordinate conversion.</p> <p>Finally, the Reference panel allows for setting the a priori accuracies of observations.</p> <p>Command: Reference \u2192 Settings</p> <p></p>"},{"location":"module2/workflow-metashape/#insert-gcps-coordinates","title":"Insert GCPs coordinates","text":"<p>GCP coordinates, acquired with topographic or GNSS instruments, can be imported from a text file.</p> <p>Command: Reference panel \u2192 Import reference</p> <p></p> <p>GCPs must be collimated on each image in which they are visible. This operation associates their object coordinates with the corresponding image coordinates, adding these crucial observations to the BBA.</p> <p>Command: Double-click on a photo \u2192 Right-click on marker \u2192 Place marker</p> <p></p>"},{"location":"module2/workflow-metashape/#align-photos","title":"Align photos","text":"<p>This is the actual external orientation phase, in which the BBA is calculated. Many homologous tie points were found by automatic matching, generating the Sparse Cloud.</p> <p>Command: Workflow \u2192 Align photos</p> <p>Parameters to be set:</p> <p>Accuracy (image subsample rate):</p> <ul> <li> <p>Highest: pixel oversample 4:1</p> </li> <li> <p>High: pixel original resolution 1:1</p> </li> <li> <p>Medium: pixel subsample 1:4</p> </li> <li> <p>Low: 1:16</p> </li> <li> <p>Lowest: pixel subsample 1:64</p> </li> </ul> <p>Reference preselection (preferential order in which homologous points are sought in an image pair):</p> <ul> <li> <p>Source: camera positions at shot epochs (if available)</p> </li> <li> <p>Estimated: estimated camera positions</p> </li> <li> <p>Sequential: progressive image name/number</p> </li> </ul> <p></p> <p>Alignment is the fundamental step in the entire process. in fact, at its end, the quality of the result must be evaluated in terms of residual errors on GCP.</p> <p></p>"},{"location":"module2/workflow-metashape/#create-dense-cloud","title":"Create Dense Cloud","text":"<p>Dense cloud is the most complete reconstruction of the object surveyed. It consists of millions of points, whose color and coordinates are known.</p> <p>Command: Workflow \u2192 Build Dense Cloud</p> <p>Parameters to be set:</p> <p>Quality (image subsample rate):</p> <ul> <li> <p>Ultra High: original pixel dimension</p> </li> <li> <p>High: pixel subsample 1:4</p> </li> <li> <p>Medium: : pixel subsample 1:16</p> </li> <li> <p>Low: : pixel subsample 1:64</p> </li> <li> <p>Lowest: : pixel subsample 1:256</p> </li> </ul> <p>Depht filtering (noise point filtering in colud generation phase):</p> <ul> <li> <p>Mild: low smoothing effect</p> </li> <li> <p>Moderate: intermediate smoothing effect</p> </li> <li> <p>Aggressive: high smoothing effect</p> </li> </ul> <p></p> <p></p> <p>Once computed, the dense cloud can be filtered, subsampled or manually edited.</p>"},{"location":"module2/workflow-metashape/#create-mesh","title":"Create Mesh","text":"<p>Mesh is a three-dimensional digital surface model of the object surveyed. It is made by the triangulation of the points of the dense cloud.</p> <p>Command: Workflow \u2192 Build Mesh</p> <p>Parameters to be set:</p> <p>Source data (input dataset):</p> <ul> <li> <p>Sparse Cloud</p> </li> <li> <p>Dense Cloud</p> </li> <li> <p>Depht Maps</p> </li> </ul> <p>Surface type:</p> <ul> <li> <p>Arbitrary: three-dimensional object</p> </li> <li> <p>Height field: planar surfaces (terrain)</p> </li> </ul> <p>Face Count (faces number):</p> <ul> <li> <p>High: 1/5 respect to cloud numerosity</p> </li> <li> <p>Medium: 1:15 respect to cloud numerosity</p> </li> <li> <p>Low: 1:45 respect to cloud numerosity</p> </li> </ul> <p></p> <p></p>"},{"location":"module2/workflow-metashape/#create-dem","title":"Create DEM","text":"<p>DEM is a grid file (raster) representing surface elevation. It consists of a matrix of regular cells with each cell having a corresponding elevation value. It is generated by rejecting a 3D input data (point cloud, mesh) on a plane.</p> <p>Command: Workflow \u2192 Build DEM</p> <p>Parameters to be set:</p> <p>Projection:</p> <ul> <li> <p>Geographic</p> </li> <li> <p>Planar</p> </li> <li> <p>Cylindrical</p> </li> </ul> <p>Source data:</p> <ul> <li> <p>Sparse Cloud</p> </li> <li> <p>Dense Cloud</p> </li> <li> <p>Mesh</p> </li> </ul> <p>Resolution: Spatial resolution of the final product (cell size)</p> <p></p> <p></p>"},{"location":"module2/workflow-metashape/#create-orthomosaic","title":"Create Orthomosaic","text":"<p>A georeferenced and orthorectified image of the surveyed area is produced by projecting the 3D surface (DEM, mesh) onto a specific plane and stitching together the individual images.</p> <p>Command: Workflow \u2192 Build Orthomosaic</p> <p>Parameters to be set:</p> <p>Projection:</p> <ul> <li> <p>Geographic</p> </li> <li> <p>Planar</p> </li> <li> <p>Cylindrical</p> </li> </ul> <p>Surface:</p> <ul> <li> <p>Mesh</p> </li> <li> <p>DEM</p> </li> <li> <p>Disabled</p> </li> </ul> <p>Pixel Size: Spatial resolution final of the product</p> <p></p> <p></p>"},{"location":"module2/workflow-metashape/#export-results","title":"Export Results","text":"<p>As products are generated, they can be exported outside the Metashape project for use in other software environments. It is also possible to generate the Quality Report, a PDF document in which all the results are reported.</p> <p>Command: File \u2192 Export \u2192 </p> <ul> <li> <p>Export Points</p> </li> <li> <p>Export Mesh</p> </li> <li> <p>Export DEM</p> </li> <li> <p>Export Orthomosaic</p> </li> <li> <p>Generate Report</p> </li> </ul> <p></p> <p></p>"},{"location":"module3/gnss/","title":"GNSS signal and observations","text":"<p>Generally, the signal coming from satellites is composed by</p> <ul> <li>carrier wave (sinusoid)</li> <li>a spreading code (a sequence of 0 and 1 bits/chips)</li> <li>low-rate navigation data message.</li> </ul> <p>Both the code and navigation message are phase-modulated on the carrier wave. There are more carrier phases on different frequencies on each satellite.</p> <p>[image source]</p> <p>The spreading code is usually publicly available and is embedded in receivers to properly observe it. There are two kinds of observations: code observations (pseudo-range) or phase observations.</p>"},{"location":"module3/gnss/#code-observations-pseudo-range","title":"Code observations (pseudo-range)","text":"<p>The time shift between the received signal and the receiver internal replica is observed. The observed time Is converted in terms of distance by considering the speed of light (), determining the so-called pseudo-range. This observation has an accuracy in the range 30 cm \u2013 3 m.</p> <p></p> <p>Nevertheless, the carrier phase is travelling at a lower speed than the one of light c, due to the presence of troposphere and ionosphere and satellite and receiver clocks are not synchronized with GPS time (and the error of receiver clock is usually larger). Therefore, the pseudo-range observation can be written as</p> \\[ {P}_{R}^{S} = c \\, \\Delta t = \\rho + c \\left( d{t}_{R} + d{t}^{S} \\right) + {I}_{R}^{S} + {T}_{R}^{S} \\] <p>where \\(\\rho\\) is the satellite-receiver distance, \\(d{t}_{R}\\) is the receiver clock error, \\({I}_{R}^{S}\\) and \\({T}_{R}^{S}\\) are the ionospheric and tropospheric delay, respectively. \\(\\rho\\) is expressed as follows:</p> \\[\\rho = \\sqrt{{\\left({X}^{S} - {X}_{R}\\right)}^{2} + {\\left({Y}^{S} - {Y}_{R}\\right)}^{2} + {\\left({Z}^{S} - {Z}_{R}\\right)}^{2}} \\] <p>The unknown of the problem are represented by the coordinates of the receiver \\({X}_{R}\\), \\({Y}_{R}\\), \\({Z}_{R}\\), since the coordinates of the satellites are known from the ephemerides.</p> <p></p>"},{"location":"module3/gnss/#phase-observations","title":"Phase observations","text":"<p>This observation is analogous to the pseudo-range one. However, due to the small wavelength (in the order of 20 cm) also the integer number \\(N\\) of cycles between satellite and receiver must be considered and introduced into the equation.</p> \\[ {L}_{R}^{S} = \\rho + c \\left( d{t}_{R} + d{t}^{S} \\right) + {I}_{R}^{S} + {T}_{R}^{S} + {N}^{S} \\lambda \\] <p></p> <p>Once the receiver \u201clock\u201d the carrier phase, the number of cycles between subsequent epochs can be observed and it is no more an unknown. Therefore, only the initial phase ambiguity is an unknown, unless the \u201clock\u201d is missed (e.g., for a cycle-sleep). Phase observations have generally an observation accuracy better than 5 mm.</p>"},{"location":"module3/gnss/#observation-errors-biases","title":"Observation errors: biases","text":"<p>Possible biases present in the observations due to:</p> <ul> <li>Multipath, namely the reflection of the signal over obstacles present around the receiver position</li> </ul> <p></p> <ul> <li>Atmosphere delays, due to the fact that the electromagnetic signal is not travelling at the speed of light because of atmosphere mass density.</li> </ul> <p></p> <ul> <li>Ephemerides prediction / computation errors.</li> </ul> <p></p> <ul> <li>Cycle-sleeps, namely missing the satellite phase observations during consequent epochs (e.g. due to the presence of obstacles)</li> </ul> <p></p>"},{"location":"module3/intro/","title":"Introduction","text":"<p>Nowadays there are many positioning systems, owned and managed by different nations / government. In particular, the main GNSS are:</p> <ul> <li>GPS (Global Navigation Satellite System) \u2013 USA</li> <li>GLONASS \u2013 Russia</li> <li>GALILEO \u2013 European Union</li> <li>BeiDou \u2013 China</li> <li>QZSS (Quasi-Zenith Satellite System) \u2013 Japan</li> <li>IRNSS (Indian Regional Navigation Satellite System) \u2013 India</li> </ul> <p></p>"},{"location":"module3/intro/#components-of-a-satellite-positioning-system","title":"Components of a satellite positioning system","text":"<p>A satellite position system is generally composed by the space segment, the control segment, and the user segment.</p>"},{"location":"module3/intro/#space-segment","title":"Space segment","text":"<p>It is constituted by the satellites orbiting around the Earth, usually guaranteeing an almost global ground coverage. Therefore, the visibility of a minimum number of satellites is possible almost everywhere. The main satellite tasks are:</p> <ul> <li>Transmit their positions (ephemeris) as well as the signal required to user segment for navigation / positioning applications,</li> <li>Receive and store information from control segment,</li> <li>Correct the orbits by on-board rockets, which managed by control segment.</li> </ul> <p></p>"},{"location":"module3/intro/#control-segment","title":"Control segment","text":"<p>It is composed by the ground stations used to control and manage the entire satellite constellation. Main tasks of the control segment are:</p> <ul> <li>Tracking the satellites of the space segment</li> <li>Determination, prediction, and distribution of satellite ephemeris.</li> </ul> <p></p>"},{"location":"module3/intro/#user-segment","title":"User segment","text":"<p>It is composed by the users that would know their position on the Earth. They need a receiver and an antenna. There are many \u00ablevel\u00bb of instruments, depending on the quality of the on-board electronics:</p> <p>Geodetic type receivers:</p> <ul> <li>for monitoring / long static survey purposes</li> </ul> <p></p> <ul> <li>for real time applications</li> </ul> <p></p> <p>Low-cost receivers:</p> <ul> <li>with the possibility of accessing to raw-data</li> </ul> <p></p> <ul> <li>Embedded in other devices:</li> </ul> <p></p>"},{"location":"module3/module3/","title":"Overview","text":"<p>The module 3 of the course is dedicated to GNSS theory.</p>"},{"location":"module3/module3/#learning-outcomes","title":"Learning outcomes","text":"<p>Students will learn the basics of GNSS positioning theory.</p> <p>Upon completing the GNSS module, students will:</p> <ul> <li>Understand GNSS systems: Grasp the functioning of major GNSS systems like GPS, GLONASS, and GALILEO, and their global applications.</li> <li>Undestand the GNSS observations: Learn essential skills in processing pseudo-range and phase observations, considering error factors.</li> <li>Undestrand abosolute and relative positioning approaches: Implement advanced techniques like single and double differences for accurate relative positioning between receivers.</li> </ul>"},{"location":"module3/module3/#table-of-contents","title":"Table of contents","text":"<ol> <li> <p>Introduction:</p> </li> <li> <p>GNSS signal and observations:</p> </li> <li> <p>Positioning methods:</p> </li> </ol>"},{"location":"module3/positioning/","title":"Positioning methods","text":""},{"location":"module3/positioning/#absolute-positioning","title":"Absolute positioning","text":"<p>Combining observation from one receiver to multiple satellites the unknown coordinates of the receiver, as well as the clock error, can be estimated. For each epoch, there are four unknowns, therefore at least four satellites are required (with pseudo-range observations). Tropospheric and Ionospheric delays need to be modelled.</p> <p></p> <p>For static positioning, observations coming from multiple epochs can be processed together, improving the estimation accuracy and allowing introducing also phase observations. Absolute positioning usually leads to accuracy of the order of few meters without very long point occupation (some hours) and/or Precise Point Positioning (PPP) solutions.</p>"},{"location":"module3/positioning/#relative-positioning","title":"Relative positioning","text":"<p>Combining observations from two receivers to the same satellites, the vector (baseline) between them can be estimated instead of the absolute position of the receiver.</p> <p></p> <p>This combination usually allows to reduce the estimation error. If one of the two receivers (base station) has known coordinates, the position of the other one (rover) can be estimated. Different solution approaches can be implemented, e.g. single differences or double differences.</p>"},{"location":"module3/positioning/#single-differences","title":"Single differences","text":"<p>Single differences are computed by subtracting observations from the two receivers to the same satellite.</p> \\[ {L}_{R1}^{S} - {L}_{R2}^{S} = {\\rho}_{R1}^{S} - {\\rho}_{R2}^{S} + c \\left( d{t}_{R1} - d{t}_{R2} \\right) + c \\left( d{t}_{R1} - d{t}_{R2} \\right) + {I}_{R1}^{S} - {I}_{R2}^{S} + {T}_{R1}^{S} - {T}_{R2}^{S} + \\left( {N}_{R1}^{S} - {N}_{R2}^{S} \\right) \\lambda \\] <p></p> <p>By means of this combination, if the two receivers are close enough (max few km), the effect of tropospheric and ionospheric model error can be mitigated, since the carrier phase is passing almost through the \u201csame\u201d atmosphere. Moreover, the difference of phase ambiguities is still an integer number. On the other hand, nothing happens to receiver clock error, since the two receivers are different.</p>"},{"location":"module3/positioning/#double-differences","title":"Double differences","text":"<p>Double differences are computed by subtracting two single difference equations, considering different reference satellites.</p> \\[ {\\phi}_{R1}^{S1} - {\\phi}_{R2}^{S1} - \\left( {\\phi}_{R1}^{S1} - {\\phi}_{R2}^{S1} \\right) = {\\rho}_{R1}^{S1} - {\\rho}_{R2}^{S1} + c \\left( d{t}_{R1}^{?} - d{t}_{R2}^{?} \\right) + {I}_{R1}^{S1} - {I}_{R2}^{S1} + {T}_{R1}^{S1} - {T}_{R2}^{S1} + \\left( {N}_{R1}^{S1} - {N}_{R2}^{S1} \\right) \\lambda - \\left[{\\rho}_{R1}^{S2} - {\\rho}_{R2}^{S2} + c \\left( d{t}_{R1}^{?} - d{t}_{R2}^{?} \\right) + {I}_{R1}^{S2} - {I}_{R2}^{S2} + {T}_{R1}^{S2} - {T}_{R2}^{S2} + \\left( {N}_{R1}^{S2} - {N}_{R2}^{S2} \\right) \\lambda \\right] \\] <p></p> <p>Applying this combination clock errors are removed, phase ambiguity remain an integer values to be estimated, and tropospheric and ionospheric delays are reduced (they can be neglected with baselines shorter than 15 km). Note that combining twice the observations, the observation error has a variance 4 times the original ones, therefore the observation noise increases. With a proper satellite geometry (few sky obstructions), no cycle-sleeps, and no external disturbing effects (e.g., multipath) the following accuracies can be obtained for static surveys processed with double difference approach.</p> <p></p>"},{"location":"module4/intro/","title":"Introduction to GIS","text":""},{"location":"module4/intro/#what-is-a-gis","title":"What is a GIS?","text":"<p>A GIS (Geographic Information System) is a computerised information system that allows the acquisition, storage, analysis, visualization and exchange of geographical information in the form of geo-referenced data.</p> <p>It then allows data to be associated with their geographical position on the earth's surface and processed in order to extract information.</p> <p>The most common GIS software are:</p> <ul> <li>ArcGIS</li> <li>QGIS</li> <li>GeoMedia</li> <li>SagaGIS ...</li> </ul> <p>Why QGIS?</p> <p>It is:</p> <ul> <li>Free and Open Source</li> <li>Constantly updated -&gt; Every 4 months a new version is released</li> </ul> <p>But it has the great advantage for those working with older versions that it can also handle projects made with newer versions.</p> <p></p>"},{"location":"module4/intro/#main-functionalities","title":"Main functionalities","text":"<ul> <li>Data visualisation:</li> </ul> <p>Vector and raster data can be displayed and superimposed in different formats and map projections.</p> <ul> <li>Data exploration and map print layout creation:</li> </ul> <p>You can compose maps and interactively explore spatial data through an easy-to-use graphical interface.</p> <ul> <li>Creation, editing, management, export:</li> </ul> <p>Spatial analysis of data can be performed </p> <ul> <li>Link to external DBs:</li> </ul> <p>You can view and query data stored on external DBs (e.g., MySQL, PostgreSQL...).</p> <ul> <li>Publication of maps on the web</li> </ul> <p>With native and non-native plugins, QGIS allows you to define settings to make an interactive map to share on web pages.</p> <p>...and much more!</p>"},{"location":"module4/intro/#graphic-user-interface","title":"Graphic User Interface","text":"<p>The GUI of QGIS Desktop is mainly composed of:</p> <ol> <li>Menu toolbar where you can find the main QGIS project features grouped by themes.</li> <li>Attributes and Vector toolbar customizable that includes shortcuts to the most commonly used tools for data manipulation in a GIS environment.</li> <li>Loaded layers list which indicates what data have been included in the project, specifying their characteristics, symbology and visibility.</li> <li>Map view that is the map canvas on which the geographic and geometric component of the data can be evaluated graphically.</li> <li>Status bar indicating the status of any processings initiated and any errors encountered.</li> </ol> <p></p>"},{"location":"module4/intro/#data-management","title":"Data management","text":"<p>How QGIS handles geographic data?</p> <p>What is needed in order to work with QGIS?</p> <ul> <li>Knowing how to handle geographical data in different geographical and/or projected coordinate systems;</li> <li>Knowing how to manage a project;</li> <li>Knowing how to import:<ul> <li>Vector data (e.g. shapefile .shp, GeoJSON .geojson...)</li> <li>Raster data (e.g. .tiff)</li> </ul> </li> <li>Understanding the functionalities of plugin</li> </ul> <p>What is the purpose of a reference system?</p> <p>Reference System -&gt; set of rules that allow us to determine the position in space in a unique way.</p> <p>This geometric concept is even more important in cartography to correctly locate a point in the territory.</p> <p></p>"},{"location":"module4/intro/#geographical-reference-system","title":"Geographical reference system","text":"<p>The concept of geographical reference system is the best known and most comprehensive, and perhaps also one of the most powerful georeferencing systems.</p> <p>It is metric, standard, stable, unique.</p> <p>It uses a well-defined and fixed reference based on:</p> <ul> <li>Earth's axis of rotation</li> <li>centre of mass</li> <li>Greenwich reference meridian</li> <li>Equator</li> </ul> <p>Normally, different R.S. are used for planimetry and altimetry....</p> <ul> <li>planimetry: ellipsoid reference (spherical for small or large scales). Why it is not used for altimetry? No link with the gravity field.</li> </ul> <p></p> <ul> <li>altimetry: geoid reference (no simple analytical description). Why is it not used for planimetry? It is very complex to use in the treatment of observations made for planimetry (due to angles and distances conversion).</li> </ul> <p></p>"},{"location":"module4/intro/#datum","title":"Datum","text":"<p>A datum (geodetic), is a geodetic reference system that allows the position of points on the Earth's surface to be defined in mathematical terms.</p> <p>Datum -&gt; Set of parameters defining the shape of the ellipsoid used and its orientation.</p> <p>A datum is composed by 8 parameters:</p> <ul> <li>2 for the ellipsoid shape;</li> <li>6 for the position and the orientation:<ul> <li>latitude e longitude on the ellipsoid (2)</li> <li>geoid height</li> <li>2 components of the deviation from the vertical</li> <li>azimut of the ellipsoid</li> </ul> </li> </ul>"},{"location":"module4/intro/#projections","title":"Projections","text":"<p>The surface of the earth is curved but there are many reasons for representing it on a plane, even in numerical cartography.</p> <ul> <li>The map used to represent the results of GIS analysis is flat.</li> <li>Flat maps are scanned and used to create GIS data.</li> <li>The raster model is flat (2D)</li> <li>It is not possible to see all the land on a curved surface at once</li> <li>It is much easier to make measurements in the plane (areas, distances, directions)</li> </ul> <p>For this reason, different map projections are also used in numerical cartography.</p> <p></p> <p>Cartographic projections carry coordinates from the ellipsoid of the reference system to the map plane. The two surfaces are not topologically equivalent, so it is not possible to move from ellipsoid to map without deformation.</p> <p>Depending on the geometric shape adopted, the projections are classified as:</p> <ul> <li>Planar projections;</li> <li>Cylindrical projections;</li> <li>Conic projections.</li> </ul> <p>There are many types of map projections, those used in Italy are:</p> <ul> <li>UTM (Universal Trasversal Mercator), globally adopted;</li> <li>Gauss-Boaga, used for the Roma 40 Monte Mario datum;</li> <li>Cassini-Soldner, used for Nuovo Catasto dei Terreni Italiano.</li> </ul> <p>Ultimately, the definition of a reference system is given by:</p> <ul> <li>geographical coordinate systems:</li> </ul> <p>Datum (es. WGS84 or Roma 40 Monte Mario)</p> <ul> <li>sistemi di coordinate proiettate:</li> </ul> <p>Datum + projection system (es. WGS84-UTM32N or Roma 40 Monte Mario - Gauss Boaga Fuso Ovest)</p> <p>In addition, all GIS softwares use geometric parameter registers, the best known of which are the EPSG (European Petroleum Survey Group) codes that unambiguously define the various world reference systems.</p> <p>How does QGIS handle geographical data?</p> <p>The management of reference systems is always a particularly delicate element in a GIS.</p> <p>In QGIS, there are 2 different reference system managements: </p> <ul> <li>Project R.S.</li> <li>Single layer R.S.</li> </ul> <p>QGIS is capable of reprojecting on the fly individual layers using the proj4 libraries, provided that the R.S. of the individual layer is defined.</p> <p>QGIS uses EPSG (European Petroleum Survey Group) codes to uniquely define the different global reference systems.</p>"},{"location":"module4/intro/#rs-settings","title":"RS Settings","text":"<p>In the settings panel (Settings \u2192 Options \u2192 CRS tab) it is possible to define the rules with which the layers RS can be managed.</p> <p></p> <p>It allows you to define which SR to adopt when opening a new project or how to manage layers without information on the reference system used.</p>"},{"location":"module4/intro/#layer-rs","title":"Layer RS","text":"<p>The SR of the individual layer can be managed by right-clicking on the layer.</p> <p></p> <p></p>"},{"location":"module4/intro/#reprojection-shapefile","title":"Reprojection - Shapefile","text":"<p>You can manage the re-projection of vectors: Right click on the layer \u2192 Save features as\u2026</p> <p></p>"},{"location":"module4/intro/#reality-modelling-with-qgis","title":"Reality modelling with QGIS","text":"<p>There are mainly two ways of conceptualising or modelling reality from a geographical point of view by considering objects as:</p> <ul> <li>Discrete objects: can be observed or described in the real world and identified by its position</li> </ul> <p></p> <ul> <li>Distributed objects: represent a quantity whose value is a function of position and can be measured at any location</li> </ul> <p></p>"},{"location":"module4/layout/","title":"Layout","text":"<p>Once the map of interest has been created, it is necessary to create a print layout.</p> <p>When dealing with cartographic layouts, some elements are considered fundamental for the final results:</p> <ul> <li>scale bar</li> <li>north arrow</li> <li>legend for all the layers the require it</li> </ul> <p>Other elements that are strongly suggested to be included:</p> <ul> <li>cartographic grid</li> <li>additional text boxes with info about the project\u200b</li> </ul> <p>QGIS uses the so called Layout Manager as a tool for defining and customizing all the fundamental map elements.\u200b</p> <p>Access this tool by clicking:</p> <p>Project -&gt; New print layout -&gt; define a title -&gt; click OK</p> <p></p> <p>Be careful! If more than a layer is active/visible on the project and you\u2019re interested in creating a layout only for one layer, take care about the fact that only the one of interest is active.</p> <p></p> <p>The extent and units of measurement (for scale and grid, if any) are those of the originale associated QGIS project, so it is a good idea to set the Reference System correctly according to the information to be displayed on the print layout.</p> <p>In order to change the page orientation, click the right button on the center of the object -&gt; Page Properties -&gt; Orientation</p> <p></p>"},{"location":"module4/layout/#insert-the-map","title":"Insert the map","text":"<p>From the Add Item menu, choose Add the map and finalize the insertion of the object inside the page by selecting the map rectangular area within the layout. Similarly, this object can be added to the layout by clicking the corresponding icon on the left bar of the bar composer. The map can then be moved within the frame are after clicking the command (symbol to be inserted HERE)     </p> <p></p> <p>The map display scale is defined in the Item Properties window to the right of the layout composer.</p> <p></p>"},{"location":"module4/layout/#insert-the-scale-bar","title":"Insert the scale bar","text":"<p>Once you have selected Add scale bar from the Add Item men\u00f9 or from the left bar, draw a rectangular area on the map to define the area where you would like to insert the element.</p> <p></p>"},{"location":"module4/layout/#insert-the-north-arrow","title":"Insert the North arrow","text":"<p>Once you have clicked Add North Arrow from the Add Item men\u00f9 or from the left bar, draw a rectangular area on the map to place the arrow on the desired position.</p> <p></p>"},{"location":"module4/layout/#insert-the-legend","title":"Insert the legend","text":"<p>Once you have pressed Add Legend from the Add Item men\u00f9 or from the left bar, draw a rectangular area on the map to insert the legend where you desire.</p> <p></p> <p>[...]</p> <p></p>"},{"location":"module4/layout/#save-the-print-layout","title":"Save the print layout","text":"<p>In the toolbar, click on the highlighted icons to save the composed layout as an image or PDF file.</p> <p>In the save window, select the desired file format.</p> <p></p>"},{"location":"module4/module4/","title":"Overview","text":"<p>The module 4 of the course is dedicated to Geographic Information System (GIS).</p>"},{"location":"module4/module4/#learning-outcomes","title":"Learning outcomes","text":"<p>Through a combination of theoric concepts and hands-on exercise, students will understands the essentials of Geographic Information Systems adoption in the context of environmental studies. Also, learners will get familiar with the manipulation of vector and raster data for quantitative and qualitative analysis with practical applications in a free and open-source GIS environment. Eventually, students will understand how to prepare cartographic output and reports to share the results of their studies.</p>"},{"location":"module4/module4/#table-of-contents","title":"Table of contents","text":"<ol> <li> <p>Introduction: this section illustrates the basic concepts of GIS theory, essential to understand how GIS software operates. Moreover, an overview of the Graphic User Interface of the adopted software is given.</p> </li> <li> <p>Vector data: this page collects a set of guided practical exercises for the manipulation and analysis of vector data in a GIS environment. Illustrated procedures refer to useful routine operations for the study of data collected during the summer school campaign.</p> </li> <li> <p>Raster data: this page collects a set of guided practical exercises for the manipulation and analysis of raster data in a GIS environment. Illustrated procedures refer to useful routine operations for the study of data collected during the summer school campaign.</p> </li> <li> <p>Layout: finally, a simple guide for the creation of a print map layout is provided, highlighting the importance of some essential objects for a proper cartographic representation.</p> </li> </ol>"},{"location":"module4/module4/#software","title":"Software","text":"<p>The adopted software for the practical sessions of this module is QGIS.</p> <p>QGIS (formerly known as Quantum GIS) is a Free and Open Source Geographic Information System. The project was born in May 2002. QGIS is a GIS software available to anyone who owns a PC with commonly used platforms such as macOS, Linux, UNIX, Microsft Windows and, experimentally, Android.</p> <p>QGIS is released under the GNU General Public License (GPL), which makes it possible to modify the source code developed in the C++ language and grants access to a free GIS programme that anyone can freely modify, following precise community guidelines.</p> <p>It is very similar in user interface and functions to equivalent commercial GIS packages. QGIS is maintained by a group of voluntary community developers who publish a new stable version approximately every four months.</p>"},{"location":"module4/module4/#installation","title":"Installation","text":"<p>The version to be used for the course exercises is the current Long Term Release (LTR, most stable version) 3.22, which can be downloaded from the Downloads page of the project website: https://www.qgis.org/it/site/forusers/download.html</p> <p></p> <ol> <li> <p>On the download page of the official site, select the long support version (LTR) of QGIS 3.22 dedicated to your operating system (Windows, macOS, Linux or Android)</p> </li> <li> <p>Save and execute the installation file.</p> </li> <li> <p>Once the procedure is complete, start QGIS Desktop 3.22. The correct installation of the software is verified by displaying the graphic interface.</p> </li> </ol> <p></p>"},{"location":"module4/raster/","title":"Raster data","text":"<p>The raster model is a widely adopted approach for storing and representing information on continuous objects, coded using a set of grid cells, each with its relative value that represents the conditions of the given area covered by the pixel. Values are cells of a grid with certain extensions and a certain resolution.</p> <p>Such data format is particularly useful to represent features whose characteristics are not homogeneous on a given area.</p> <p></p> <p>Some examples of data that are commonly available and distributed as raster are:</p> <ul> <li>Aerial photos, including satellite imagery and orthophotos.</li> <li>Digital Terrain Models (DTM) which can be mainly subdivided into:<ul> <li>Digital Elevation Models (DEM) which is a digital file with ground surface elevation values at regularly spaced inntervals in the horizontal plane;</li> <li>Digital Surface Models (DSM) that represents in digital form the heights of the upper part of the terrain including building, infrastructures and trees without the filtering procedures used to produce DEMs.</li> </ul> </li> <li>Thematic maps representing the variation of geomorphologic characteristics (e.g. geological maps) or coverage (e.g. land cover maps) of a given territory.</li> </ul> <p>In a GIS, each raster layer possesses pixels (cells) of a consistent size, which defines its spatial resolution. This characteristic becomes evident when you observe an image at a reduced scale and subsequently magnify it to a larger scale.</p> <p>Images characterized by a pixel size that encompasses a limited area are referred to as high-resolution images, as they allow for discerning a substantial level of detail within the image. Conversely, images featuring a pixel size that encompasses a larger area are termed low-resolution images, as they exhibit a reduced level of detail.</p> <p>In the following guided tutorial you will learn how to load raster data in the QGIS environment, manipulate and style layers and execute common operations such as clip and raster calculations. The data used for this exercise can be downloaded here and consists of the Belvedere glacier orthophotos and digital elevation models produced during the 2021 and 2022 monitoring campaigns.</p>"},{"location":"module4/raster/#loading-data","title":"Loading data","text":"<p>In the data folder you just downloaded, you can find 7 different files referring to 3 distinct raster data layers.</p> <p>In particular:</p> <ol> <li> <p>All files named [2022_ortofoto_20cm] refers to the orthophoto of the glacier area of interest in a tiff format, as surveyed with UAVs inn 2022. The meaning and role of the different file extensions will be explained in the next steps.</p> </li> <li> <p>[2021_dem_20cm] is the digital elevation model produced at the end of the 2021 survey campaign.</p> </li> <li> <p>[2022_dem_20cm] is the digital elevation model produced at the end of the 2022 survey campaign.</p> </li> </ol> <p>For adding a new raster data layer to a QGIS project, click from the menu bar:</p> <p>Layer &gt; Data source manager</p>"},{"location":"module4/raster/#tif-import","title":"TIF import","text":"<p>Similarly to the import procedure for vector data, select the Raster tab and in the Source section, by clicking the Browser (...) icon, look for the 2022_ortofoto_20cm.tiff file on your laptop. After selecting it, click Add. Hence, close the Data source manager window and check that the orthophoto of the Belvedere glacier area is correctly visible on the map canvas view.</p> <p></p> <p></p> <p>The loaded vector layer will also appear in the QGIS layer section with its name close to a checkboard icon, symbolizing that such data is a raster.</p> <p>The chosen file is a Tag Image File (TIF), a widely adopted format for raster data in GIS environments. Usually, the main file with .tif extension is accompained by another file with the same name and the .tfw extension. Such file saved in plain text format contains information on the georeferencing of the reaster itself. Indeed, they store information on the X and Y pixel size, rotation as well as the global coordinates for the top-left corner of the raster.</p>"},{"location":"module4/raster/#drag-drop-import","title":"Drag &amp; drop import","text":"<p>Exactly like the case of shapefile import, raster files can be imported in QGIS project with the drag&amp;drop shortcut. Try to execute this operation with the DEM rasters: 2021_dem_20cm.tif and 2022_dem_20cm.tif.</p> <p>The expected map canvas view for each DEM is the one depicted below in grayscale.</p> <p></p>"},{"location":"module4/raster/#layer-properties","title":"Layer properties","text":"<p>General information on each layer loaded in a QGIS project can be accessed through the Properties panel. As suggested for vector files, it is recommended to check this information everytime a new layer is added to the page: it helps understanding the source and nature of data as well as if any interpretation issue has affected the layer loading.</p> <p>To view the properties of the raster, right-click on the layer and select Properties.... The newly appeared window contains different tabs. In the next sections some of the most useful for routine procedures will be explained in details foor the case of raster.</p>"},{"location":"module4/raster/#information","title":"Information","text":"<p>This read-only tab summarizes the main information and metadata of the chosen layer. In the case of raster layer it is important to mention, in addition to info field in common with vector data, the information about number of band, band statistics, grid dimensions and pixel size giving important information on the spatial resolution of the raster itself. For example, in the case of all the rasters used in this guided exercise the spatial resolution will be equal to 20 cm, meaning that each pixel of the raster file covers a squared area of 20 x 20 centimeters.</p> <p></p>"},{"location":"module4/raster/#symbology","title":"Symbology","text":"<p>The layer visual representation in the map canvas can be modified as preferred in the Symbology tab. Different type of representation can be chosen from the dropdown menu on top of the window and differs from the vector data ones.</p>"},{"location":"module4/raster/#singleband-gray","title":"Singleband gray","text":"<p>The singleband gray is the default view of raster data when they are loaded on QGIS. It consists in representing them with just one color gradient that goes from black, for lower values, to white, for higher ones.</p> <p></p>"},{"location":"module4/raster/#singleband-pseudocolor","title":"Singleband pseudocolor","text":"<p>The singleband-pseudocolor render type, as the singleband-gray, is suitable for showing quantitative information. It allows to choose a color ramp and also (choosing for Mode: Equal Interval or Quantile) a number of classes there to categorize the different intervals of values to be represented. Choosing for Mode:Continuous the number of classes cannot be manually set.</p> <p></p> <p></p>"},{"location":"module4/raster/#raster-projection","title":"Raster projection","text":"<p>Checking the information table in the raster layers properties it is possible to notice that they're inserted in a reference system that is different from the one adopted for the QGIS project. Hence, it is necessary to execute a reprojection of the layers in the desired reference system (EPSG: 32632 - WGS84 UTM Zone 32 N).</p> <p>In order to do so, access the menu Raster &gt; Projections &gt; Warp (Reproject)....</p> <p>In the newly appeared window, it is required to select the layer to be reprojected, insert information on the source reference system (if available) and on the target one. By default, the adopted resampling method for the raster is the Nearest Neighbour, but it is possible to select many other options from a given list. Once all the required parameters are defined, click Run to perform the transformation.</p> <p></p> <p>Let's repeat the operation with all the raster in order to have all of them inserted in the desired reference system.</p>"},{"location":"module4/raster/#clip-raster","title":"Clip raster","text":"<p>Before proceeding with useful analysis on the raster data of the glacier, it is necessary to clip the rasters using the Belvedere perimeter polygon. In particular, the vector file used in the previous exercise will be adopted as a mask to cut the digital elevation models. This commonly used procedure is particularly useful to define common areas of study, avoiding the influence of external data that are available only in one of the DEM.</p> <p>To perform this, first load again the bbghiacciaio vector layer and then access the menu Raster &gt; Extraction &gt; Clip raster by mask layer.</p> <p>In the plugin window, select the layer you'd like to clip and the vector layer to use as mask for the operation. Then, define the input and output reference systems. Once everything is set, click Run. Repeat the procedure with all the given DEMs.</p> <p></p> <p>The expected output in the map canvas looks similar to the one depicted in the illustration below.</p> <p></p>"},{"location":"module4/raster/#raster-calculation","title":"Raster calculation","text":"<p>Finally, it is time to perform some simple calculation on the volume differences based on the glacier's DEMs computed during the 2022 and 2021 campaigns.</p> <p>QGIS makes available a powerful tool for simple calculation on raster data, the so called Raster Calculator. It is accessible at the menu Raster &gt; Raster calculator.</p> <p>Once its window is visible, it is possible to see listed all the raster bands of the layers loaded in the QGIS project. On the right of the window, it is possible to define the desired options for the output calculated raster: file name, path, format, extension, resolution and reference system. Finally, in the lower section a series of common math operators is present alongside the Expression box where it is possible to formulate the expression for the calculation by inserting the operators as well as the needed raster bands to be used as variables (by simply double clicking on them on the Raster Bands list).</p> <p>To compute the height differences between 2022 and 2021, first double-click the 2022 clipped DEM and then, after inserting the subtraction symbol, double-click the 2021 clipped DEM. Then, click on the OK button.</p> <p></p> <p>At the end of the processing, the output results in the map canvas should look similar to the one depicted in the image below.</p> <p></p> <p>Now it is necessary to calculate the total of the differences, meaning the sum of the values stored in all the raster cells of the output file. This operation can be easily executed through a native plugin of QGIS called Raster layer statistics that can be accessed through the menu Processing &gt; Toolbox &gt; Raster Analysis &gt; Raster Layer Statistics.</p> <p>Select as the input layer the raster of interest, define a name for the output html report file and then click Run.</p> <p></p> <p>At the end of the procedure, open the HTML report that will contain an overview of the descriptive statistics related to the given raster.</p> <p></p> <p>Once you identified the sum value for the raster, simply multiply it for the squared raster pixel resolution (pixel area) in order to obtain the total volume loss of the glacier between the 2 campaign.</p>"},{"location":"module4/vector/","title":"Vector data","text":"<p>Vector data are widely used in GIS environment for the representation of information of discrete objects. Vector model features are particularly useful for representing and storing discrete objects such as buildings, roads, particles, etc.</p> <p>They usually consist of two components:</p> <ol> <li>geometry</li> <li>thematic component</li> </ol> <p>The geometry of each single vector object is defined by one single node or a set of interconnected vertices. Each of such vertices is located in the vector data reference system using a set of x and y. For 2.5D vector data the z coordinate values are also present to represent variables such as height, depth or other.</p> <p>Hence, the vector model can represent geographical entities through the following geometries:</p> <ul> <li>Points: single set of x, y and z values;</li> <li>Lines: ordered set of points whose starting node is different from the last one;</li> <li>Polygons: ordered set of points whose starting node is coincident with the ending one.</li> </ul> <p></p> <p>Each vector object in addition to its geometry has a set of thematic information associated to it, the so called attributes. Attributes of a vector object usually describe its meaning, properties and/or characteristics in the real world that it represent schematically in a GIS environment.</p> <p>One or multiple vector objects are stored in layers. Objects of a GIS layer have the same geometry type and the same set of attributes.</p> <p>In the following guided tutorial you will learn how to load vector data in the QGIS environment, manipulate and style layers and execute common operations such as joins and selectins. The data used for this exercise can be downloaded here and consists of the Belvedere glacier perimeter and GNSS measurements of Ground Control Points (GCPs) collected during the 2022 and 2023 monitoring campaigns.</p>"},{"location":"module4/vector/#loading-data","title":"Loading data","text":"<p>In the data folder you just downloaded, you can find 8 different files referring to 3 distinct vector data layers.</p> <p>In particular:</p> <ol> <li> <p>All files named belvedere_perimeter (6) refers to the representation of the glacier area of interest in a shapefile format. The meaning and role of the different file extensions will be explained in the next steps.</p> </li> <li> <p>gcp_2022 is a comma-separated-values (csv) file that contains the information about the GCPs measured in the 2022 campaign.</p> </li> <li> <p>gcp_2023 is a comma-separated-values (csv) file that contains the information about the GCPs measured in the 2023 campaign.</p> </li> </ol> <p>For adding a new vector data layer to a QGIS project, click from the menu bar:</p> <p>Layer &gt; Data source manager</p>"},{"location":"module4/vector/#shapefile-import","title":"Shapefile import","text":"<p>The Data source manager window represent the main place in QGIS for uploading rigorously not only vector data but also other type of files. Indeed, depending on the nature of the data the user is willing to upload, it offers different tabs with guided procedures. For the case of vector layer, select the Vector tab and in the Source section, by clicking the Browser (...) icon, look for the belvedere_perimeter.shp file on your laptop. After selecting it, click Add. Hence, close the Data source manager window and check that the polygonal shape of the glacier is correctly visible on the map canvas view.</p> <p></p> <p></p> <p>The loaded vector layer will also appear in the QGIS layer section with its name close to the colour with which it is represented on the map canvas.</p> <p>The chosen file is a shapefile, a widely adopted format for vector data that indicates a group of files with the same file name but with different extensions:</p> <ul> <li>.shp referring to the vector geometry;</li> <li>.shx containing the positional index of the vector geometry and allowing flexible and efficient ordered object search inside the layer;</li> <li>.dbf that contain the tabular information, specifically the attribute headers and values for each object in the layer;</li> <li>.prj that store information about the reference system of the vector data; -.cpg containing the required information for the character encoding of the dbf file;</li> <li>.qmd storing the metadata of the vector layer.</li> </ul> <p>It is important to understand that at least the first 3 files (shp, shx, dbf) are needed in order to correctly decode the layer information in a GIS environment, while the others represents additional useful information about the nature and/or source of the data.</p>"},{"location":"module4/vector/#csv-import","title":"CSV import","text":"<p>Another commonly adopted format for vector data creation and sharing of point geometry type is the csv. Such data can be uploaded through the Data source manager with the guided procedure defined in the Delimited Text tab. Similarly to the shapefile procedure, the choice of the file is done with the Browse button (...) on top of the window. For this example, select the gcp_2022.csv file. Then a series of additional information are required for the correct uploading:</p> <ul> <li> <p>File Format asks information about the type of delimiter used in the chosen file for decoding correctly its tabular data. Usually, QGIS is able to detect it automatically. Such information could also be easily understood by inspecting the preview of the data table in the Sample Data section. In this case the delimiter is the semicolon.</p> </li> <li> <p>Record and Field Options is the section where information about header presence or numerical format standard should be indicated.</p> </li> <li> <p>Geometry definition needs information on how the geometry of the point objects is coded in the csv table. First the format of storage has to be chosen (in this case Point coordinates) and then the user is asked to associate detected fields in the table to X, Y and optionally Z or M values. Only with a correct association the software is able to locate the vector layer objects in the reference system space. In this case, X:east field, Y:north field in the Geometry CRS *EPSG:32632 - WGS84 / UTM Zone 32 N.</p> </li> </ul> <p>Once all the sections are filled as indicated, click Add and check that the GCPs points are correctly visible on the map canvas view.</p> <p>As a result, the 2022 GCPs layer will be positioned on top of the glacier one previously loaded.</p> <p></p> <p></p>"},{"location":"module4/vector/#drag-drop-import","title":"Drag &amp; drop import","text":"<p>The presented procedures represent the most rigorous way to import data in QGIS. Indeed, it is possible to load data to your QGIS project through a simple drag and drop on the map canvas in a quicker way, However, it is import to always pay attention to their format because for some type of data this shortcut affect the correct interpretion of vector file. For example, while there are no differences for the shapefile, the result for the csv is very different from the previously explained procedure. With the drag and drop, the csv is loaded as a simple table data layer with no geometry data, thus removing the possibility of exploring the data it contains by exploiting its geometric information.</p> <p>You could try this by using the drag&amp;drop procedure for loading the gcp_2023.csv that will be used later for analysis and calculation.</p>"},{"location":"module4/vector/#layers-management","title":"Layers management","text":"<p>In the Layers section of the QGIS Graphical User Interface you could manage the visibility and appearance of data layers. For example, by unchecking the box close to the symbol of a given data layer, you can temporarily hide the geometries belonging to it. This operation can be particularly useful when you are doing specific analysis on  other layers and you want to evaluate better their graphic result representation on the map canvas.</p> <p>Instead, if you'd like to remove from the project a specific layer, you should right click on it and select the Remove layer.. option. In this case, the layer data are definitively removed from the project and can be evaluated only by reuploading the layer through one of the import procedure previously explained. It is important to note that the removed layer are removed only from the QGIS project and they are not deleted also from their corresponding path folder in your device.</p>"},{"location":"module4/vector/#layer-properties","title":"Layer properties","text":"<p>General information on each layer loaded in a QGIS project can be accessed through the Properties panel. It is recommended to check this information everytime a new layer is added to the page: it helps understanding the source and nature of data as well as if any interpretation issue has affected the layer loading.</p> <p>To view the properties of the shapefile, right-click on the layer and select Properties.... The newly appeared window contains different tabs. In the next sections some of the most useful for routine procedures will be explained in details.</p>"},{"location":"module4/vector/#information","title":"Information","text":"<p>This read-only tab summarizes the main information and metadata of the chosen layer. In particular it includes information about the file path, name, geometry type, geographical extension, total number of objects, reference system, measurement unit, attribute fields...</p> <p>In this section it is always important to check the order of magnitude of the number in the Information from provider - Extent field as a double check on the coordinate values matching the expected order for the assigned reference system.</p> <p></p>"},{"location":"module4/vector/#symbology","title":"Symbology","text":"<p>The layer visual representation in the map canvas can be modified as preferred in the Symbology tab. Different type of representation can be chosen from the dropdown menu on top of the window.</p>"},{"location":"module4/vector/#single-symbol","title":"Single symbol","text":"<p>After selecting Single symbol from the dropdown menu, you access the menu for defining the style for the symbol associated to each object inside the chosen layer. This modality represents all data in the same way. In the options it possible to change color, level of opacity, size, and type of the symbol. Advanced edits (symbol fill, stroke etc) to the style could be done by clicking on Simple Marker*.</p> <p></p>"},{"location":"module4/vector/#categorised","title":"Categorised","text":"<p>The Categorized style option allows to represent data according to the values contained in a chosen layer field. This modality is suggested when the values contained in a field consists of a set of recurring values. In order to apply it, click on the arrow on the value field and select the field with the category and then click on Classify. After exploring the values present in the chosen attribute column inside the layer, QGIS automatically suggest a categorisation with given values and legend descriptions. Such combination can always be modified by clicking on each symbol for styling, or double-clicking on value or legend field for manually typing the values and descriptions.</p> <p>For the case of 2022 GCP points, select the type field as the classification values and style accordingly. This will help identifying more intuitively fixed or moving points on the map canvas.</p> <p></p>"},{"location":"module4/vector/#graduated","title":"Graduated","text":"<p>The Graduated style option allows to represent data according to the values of a continuous scale contained in a chosen layer field. This modality is suggested when the values contained in a field consists of a set of unique values along a continuous scale. In order to apply it, click on the arrow on the value field and select the field with the category and then click on Classify. After exploring the values present in the chosen attribute column inside the layer, QGIS automatically suggest a classification with given values and legend descriptions. Such combination of class can always be modified by clicking on each symbol for styling, or double-clicking on value or legend field for manually typing the values and descriptions. Color ramp for the color method classification che be defined as preferred.</p> <p>When representing vector data with this method, it is important to pay attention to the mode of classification that by default is Equal Count (Quantile). Everytime classification symbology is chosen, first check the Histogram tab, click Load values and evaluate the numeric distribution of the values in the vector layer. Then, in the Classes tab, select the more appropriate mode and number of classes. Everytime an option is changes, it is needed to click Classify for making an edit effective.</p> <p>For the case of 2022 GCP points, select the height field as the classification values and style accordingly. This will help understanding the topology of the study site.</p> <p></p>"},{"location":"module4/vector/#labels","title":"Labels","text":"<p>In order to better identify the different objects of a given layer, it is also possible to insert labels containg values associated to the corresponding object on the map. For example, it is possible to show on the map canvas the label of the ID value associated to each GCP. In order to do so, click on Labels \u2192 select Single Labels and choose which Value to display, in this case id.</p> <p>Additional optional styling for the labels can be done by changing the options of the different section below the Text Sample box.</p> <p></p>"},{"location":"module4/vector/#attribute-table","title":"Attribute table","text":"<p>The so-called attribute table associated to each layer loaded in our QGIS project contains all the thematic information associated to the single objects. Each row of it represents a different object (feature) while each column represents an attribute. Such structure is essential to manage the infoormation linked to the object on the project, as well as to select or query them efficiently.</p> <p>To access the attribute table view, simply right click on the layer of interest and click on Open Attribute Table.</p> <p></p>"},{"location":"module4/vector/#save-a-layer","title":"Save a layer","text":"<p>During the various processing operations executed in a GIS environment, it is often very useful to save in new layers intermediate products as well as final outputs. In order to save a modified layer already loaded in a project, right-click on it and select Export &gt; Save Features as.</p> <p></p> <p>In the newly appeared window, select the desired output file format ad the file name path where you'd like to save your new layer. Then click okay for finalising the operation. Try to do this operation with the gcp_2022 layer (originally available only as csv) and save it as a shapefile.</p>"},{"location":"module4/vector/#joins","title":"Joins","text":"<p>In routine operation in the GIS environment, it is sometimes useful to link information contained in a table layer to another layer with a spatial component. For instance, for project example it is useful to link the GCP coordinate measurements from 2023 to the one of 2022 in order to compute their differences as well as the velocity.</p> <p>In order to do so, in the layer properties of the gcp_2022 shapefile, go to the Joins tab and click on green plus symbol. In the new window it is necessary to define the parameters that allow the correct execution of the join between the gcp_2022 georeferenced layer and the gcp_2023 table that contains the 2023 coordinate measurements. In order to do so it is necessary to identify a common field the univocally identify a record inside a table. In this case, it is the id field that is present in both layers. Hence, it is needed to select it in both Join field and Target field options. In order to make possible the execution of calculation on all the field of the resulting joined attribute table, check the Editable join layer box and then click Ok and Apply in the main properties Join tab.</p> <p></p> <p>As a result, when opening the attribute table of gcp_2022 also contains the attribute columns regarding the 2023 points.</p> <p></p>"},{"location":"module4/vector/#editing-mode","title":"Editing mode","text":"<p>The Attribute Table with its tools represents a powerful instrument in the QGIS environment. Not only it allows to navigate through the data and select them but it enable the possibility to make calculations using attribute values and create new calculated field. In order to do so, it is necessary to activate the Editing mode by clicking on the yellow pencil icon on the top left of the Attribute Table. Then, let's calculate the coordinate differences between the 2 years using the Field Calculator.</p> <p>This tool can be accessed by clicking on the abacus symbol. In this case, we would like to create a nnew field in the layer's attribute table that will contain the value of the differences between 2023 height and 2022 one. Hence, it is needed to check the box for Create new field. Then, define the Output field name and its type (in this case, decimal), as well as the field length and precision (number of decimal digits). Finally, in the white Expression box, insert the formula to calculate the desired output. If layer fields are used as variable in it, it is possible to select the needed one(s) by double clicking them from the Fields and Values list on the right. To finalise the operation, click on OK.</p> <p></p> <p>Repeat the same operation for the calculation of the differences of East and North coordinates between 2023 and 2022. In the end, the attribute table should look similar to the one depicted below.</p> <p></p> <p>Once completed the operations, click on the floppy icon Save edits and eventually exit the editing mode by clicking one more time on the yellow pencil icon Toggle editing mode on the attribute table.</p>"},{"location":"module4/vector/#adding-a-new-field","title":"Adding a new field","text":"<p>While the editing mode is activated on a given layer, it is also possible to add a new field to the attribute table. This is possible by clicking on the New field button.</p> <p></p> <p>In the newly appeared window, it is required to define some options, paying particular attention to the type of value that will be entered in the new field (integer, decimal, text, date, etc.) and the maximum number of characters. Let's add for example a field that allows users to enter note for particular observation on given points.</p> <p></p> <p>To finalise the changes introduced on the attribute table, save and end the editing session.</p>"},{"location":"module4/vector/#delete-field","title":"Delete field","text":"<p>While the editing mode is activated on a given layer, it is also possible to add a new field to the attribute table. This is possible by clicking on the Delete field button. After that, the user is required to select one or more field to be removed from the attribute table.</p> <p></p> <p>To finalize changes, save and end the editing session.</p>"},{"location":"module4/vector/#selection-operations","title":"Selection operations","text":"<p>One of the most common operations executed in a GIS environment is represented by the select/query function. This allow to filter and identify data on given layers depending on specific criteria. The two main types of selection operation in QGIS are:</p> <ul> <li>Select by expression</li> <li>Select by location</li> </ul>"},{"location":"module4/vector/#select-by-expression","title":"Select by expression","text":"<p>Selecting by expression in QGIS means using a set of rules or conditions to select a subset of features from a layer based on the attribute data or geometry of the features.</p> <p>Let's try identify the GCPs that are considered as Fixed point. Such information, for example, is given by the attribute type, where the F value stands for \"Fixed\" while \"M\" for \"Moving\". Hence, in order to highlight the points of interest, it is needed to filter the ones satisfying the criteria (also called expression): type = F.</p> <p>To perform this operation, open the attribute table of gcp_2022 and click on the Select features using an expression button.</p> <p></p> <p>In the new window, insert the expression formulated before for the selection of fixed points. Pay attention to the syntax for field names and values. Then click Select Features to run the procedure.</p> <p></p> <p>As a result, you will see that a certain number of rows in the attribute table is highlighted in blue: those are the records in the layer that satisfy the criteria defined with the expression. The exact number of rows selected is indicated on the header of the attribute table header (in this case, 11).</p> <p></p> <p>Such operation can be executed with any type of field, also using different operations according to the attribute type (string, numeric etc.).</p> <p>Let's clear the current selection (clicking Deselect all features from the layer button) and get ready for executing another simple query.</p> <p></p> <p>Open again the Select features by expression window and try to select the records that are characterised by a difference on the height values greater than 1 meter.</p> <p></p>"},{"location":"module4/vector/#select-by-location","title":"Select by location","text":"<p>Similarly to the Select by expression operation, the Selection by location function allows to filter and identify records based on the spatial relationship between each feature and the features in an additional layer.</p> <p>For instance, let's try to identify GCPs loacted within the Belvedere glacier perimeter. To do so, click Vector &gt; Research Tools &gt; Select by Location....</p> <p>Specify that you desired to select features from the gcp_2022 layer by comparing features that are within the belvedere_perimeter layer. Then, click Run.</p> <p>.</p> <p>The result on the map canvas should be similar to the one depicted below, with GCPs inside the glacier polygon highlighted in yellow.</p> <p>.</p> <p>Are you interested instead on the GCPs located outside the polygon? You can use a simple selection shortcut button named Invert features selection.</p> <p>.</p>"},{"location":"module5/data_preparation/","title":"Data preparation","text":""},{"location":"module5/data_preparation/#download-the-data","title":"Download the data","text":"<p>To get started in this module, download the data from the <code>Mod5_stereo_processing</code> folder in the repository of the course. You can reach the data repository from the following link:</p> <p>https://polimi365-my.sharepoint.com/:f:/g/personal/10462873_polimi_it/EkqPd1sQfvFOkU68BbGlbUQB31mnpaszWBH1fWg0foj_rw?e=DAXR68</p> <p>The data contains a daily sequence of stereo pairs acquired by the two stereo cameras located at the Belvedere Glacier terminus from 02/07/2022 to 31/07/2022. In the data folder, you will also find the calibration files for each camera and the target files for each image.  The target files contain the image coordinates of all the visible targets in each image.  There is also a file containing the world coordinates of all the targets. In the next section, you will learn how to organize the data for the processing.</p>"},{"location":"module5/data_preparation/#data-organization","title":"Data organization","text":"<p>All the data must be copied in a <code>data</code> folder, which must be located in the same folder of the notebook and of the configuration file (you will see below how to prepare it).</p> <p>Your working directory should have the following structure:</p> <pre><code>working directory\n\u251c\u2500\u2500 config.yaml    # Configuration file\n\u251c\u2500\u2500 data/ \n    \u251c\u2500\u2500 img/       # Image folder (one subfolder per camera)\n        \u251c\u2500\u2500 cam1/ \n        \u251c\u2500\u2500 cam2/ \n    \u251c\u2500\u2500 calib/     # Calibration files folder (one file per camera)\n        \u251c\u2500\u2500 cam1.txt\n        \u251c\u2500\u2500 cam2.txt\n    \u251c\u2500\u2500 targets/   # Target files folder (one file per image)\n        \u251c\u2500\u2500 img_cam1_epoch0.txt\n        \u251c\u2500\u2500 img_cam1_epoch1.txt\n        \u251c\u2500\u2500 img_cam1_epoch2.txt\n        ...\n        \u251c\u2500\u2500 img_cam2_epoch0.txt\n        \u251c\u2500\u2500 img_cam2_epoch1.txt\n        \u251c\u2500\u2500 img_cam2_epoch2.txt\n        ...        \n        \u251c\u2500\u2500 targets_world.txt\n\u251c\u2500\u2500 notebook.ipynb  # various notebooks for processing (or python scripts)\n</code></pre> <p>The <code>img</code> folder contains one subfolder per camera.  The <code>calib</code> folder contains the calibration files for each camera.  The <code>targets</code> folder contains the targets files.  Targets file are stored all together in a single folder <code>targets</code> folder. Each target file must be named as with the same name as the image that it belongs to, but with a textfile extension (\".txt\", \".csv\"), and it contains the image coordinates of all the visible targets in that image. Each file must contain the target label and the image coordinates x and y of all the visible targets. For instance, the file named <code>img_cam1_epoch0.txt</code>, where <code>img_cam1_epoch0.jpg</code> is the image file, contains the following data:</p> <pre><code>label,x,y\nF1,1501.8344,3969.0095\nF2,1003.5037,3859.1558\n</code></pre> <p>Additionally, a file containing the world coordinates X,Y, Z of all the targets must be provided. This file should be named <code>targets_world.txt</code> and it must contain the following data:</p> <pre><code>label,X,Y,Z\nF1,-499.8550,402.0301,240.3745\nF2,-302.8139,442.8938,221.9927\n</code></pre> <p>World coordinates must be in a cartesian (e.g., local) or projected (e.g., UTM) coordinate system. </p>"},{"location":"module5/data_preparation/#configuration-file","title":"Configuration file","text":"<p>The <code>config.yaml</code> file contains the configuration parameters for the processing.</p>"},{"location":"module5/getting_started/","title":"Getting started","text":"<p>To get started with the stereo processing, you need to install the ICEpy4D library and the Metashape Python API.</p>"},{"location":"module5/getting_started/#requirements","title":"Requirements","text":"<ul> <li>64-bit Python <code>&gt;= 3.8</code></li> <li>a NVIDIA graphic card with CUDA capability is strongly reccomended.</li> </ul>"},{"location":"module5/getting_started/#install-icepy4d","title":"Install ICEpy4D","text":"<p>Create a new Anaconda environment</p> <pre><code>conda create -n icepy4d python=3.9\nconda activate icepy4d\n</code></pre> <p>Install Icepy4D from the original repository</p> <pre><code>git clone https://github.com/franioli/icepy4d.git\ncd icepy4d\npip install -e .\n</code></pre>"},{"location":"module5/getting_started/#install-metashape-python-api","title":"Install Metashape Python API","text":"<p>For full Bundle Adjustment and dense reconstruction of the terminal ice cliff, you need to install Agisoft Metashape Python API. Metashape Python API can be downloaded from https://www.agisoft.com/downloads/installer/ or use <code>wget</code> (under Linux).</p> <pre><code>wget https://s3-eu-west-1.amazonaws.com/download.agisoft.com/Metashape-1.8.5-cp35.cp36.cp37.cp38-abi3-linux_x86_64.whl\npip install Metashape-1.8.5-cp35.cp36.cp37.cp38-abi3-linux_x86_64.whl\n</code></pre> <p>You need to have a valid Metashape license (and associated license file) to use the API and you need to activate it.</p> <p>The easiest way to get the license file, is by installing the Metashape Professional Edition GUI software (distinct from the Python module) and registering it following the prompts in the software (you need to purchase a license first). Once you have a license file (whether a node-locked or floating license), you need to set the agisoft_LICENSE environment variable (search onilne for instructions for your OS; look for how to permanently set it) to the path to the folder containing the license file (metashape.lic).</p> <p>With Linux (Ubuntu 22.04), to permanently setup agisoft_LICENSE environment variable for floating license, modify your .bashrc file:</p> <pre><code>sudo nano ~/.bashrc\n</code></pre> <p>add the line (replace port and address with your values)</p> <pre><code>export agisoft_LICENSE=\"port\"@\"address\"\n</code></pre> <p>Then you have to execute the .bashrc file:</p> <pre><code>source ~/.bashrc\n</code></pre> <p>Now, you can check if the new environmental variable is present:</p> <pre><code>printenv | grep agisoft\n</code></pre> <p>If so, you are ready to go.</p>"},{"location":"module5/getting_started/#test-icep4d-installation","title":"Test ICEp4D installation","text":"<p>Try to import ICEpy4D package</p> <pre><code>conda activate icepy4d\npython -c \"import icepy4d\"\n</code></pre> <p>If no error is given, ICEpy4D is successfully installed and it can be imported within your script with <code>import icepy4d</code></p> <p>You are now ready to start with the stereo processing.</p>"},{"location":"module5/installation/","title":"Installation guide","text":""},{"location":"module5/installation/#requirements","title":"Requirements","text":"<ul> <li>64-bit Python <code>&gt;= 3.8</code> but <code>&lt; 3.10</code></li> <li>a NVIDIA graphic card with CUDA capability is strongly reccomended.</li> </ul>"},{"location":"module5/installation/#install-icepy4d","title":"Install ICEpy4D","text":"<p>Create a new Anaconda environment</p> <pre><code>conda create -n icepy4d python=3.9\nconda activate icepy4d\n</code></pre> <p>Install Icepy4D from PyPi repository</p> <pre><code>pip install icepy4d\n</code></pre> <p>Install Metashape Python API for Bundle Adjustment and Dense reconstruction. Metashape Python API can be downloaded from https://www.agisoft.com/downloads/installer/ or use <code>wget</code> (under Linux).</p> <pre><code>wget https://s3-eu-west-1.amazonaws.com/download.agisoft.com/Metashape-1.8.5-cp35.cp36.cp37.cp38-abi3-linux_x86_64.whl\npip install Metashape-1.8.5-cp35.cp36.cp37.cp38-abi3-linux_x86_64.whl\n</code></pre> <p>You need to have a valid Metashape license to use the API and you need to activate it (see https://github.com/franioli/metashape for how to do it)</p> <p>Try to import ICEpy4D package</p> <pre><code>conda activate icepy4d\npython -c \"import icepy4d\"\n</code></pre> <p>If no error is given, ICEpy4D is successfully installed and it can be imported within your script with <code>import icepy4d</code></p>"},{"location":"module5/matching/","title":"DL Feature Matching","text":"<p>First, let's load the required modules. Additionally, even though this step is not mandatory, it is suggested to setup a logger to see the output of the matching process. If no logger is setup, the output of the process is suppressed.</p> In\u00a0[1]: Copied! <pre>from icepy4d.core import Image\nfrom icepy4d.utils import setup_logger\nfrom icepy4d.matching import (SuperGlueMatcher, LOFTRMatcher, LightGlueMatcher, Quality, TileSelection, GeometricVerification)\n\nsetup_logger()\n</pre> from icepy4d.core import Image from icepy4d.utils import setup_logger from icepy4d.matching import (SuperGlueMatcher, LOFTRMatcher, LightGlueMatcher, Quality, TileSelection, GeometricVerification)  setup_logger() <pre>Jupyter environment detected. Enabling Open3D WebVisualizer.\n[Open3D INFO] WebRTC GUI backend enabled.\n[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n</pre> <p>We can load the images as numpy arrays. We will use the Image class implemented in ICEpy4D, which allows for creating an Image instance by passing the path to the image file as <code>Image('path_to_image')</code>. Creating the Image instance will read the exif data of the image and store them in the Image object. The actual image value is read when the <code>Image.value</code> proprierty is accessed. Alternatevely, one can also use OpencCV imread function to read the image as a numpy array (pay attention to the channel order, that should be RGB, while Opencv uses BGR).</p> In\u00a0[2]: Copied! <pre>image0 = Image('../data/img/p1/IMG_2650.jpg').value\nimage1 = Image('../data/img/p2/IMG_1125.jpg').value\n\nprint(f\"Image data-type: {type(image0)}\")\nprint(f\"Image0 shape: {image0.shape}\")\nprint(f\"Image1 shape: {image1.shape}\")\n</pre> image0 = Image('../data/img/p1/IMG_2650.jpg').value image1 = Image('../data/img/p2/IMG_1125.jpg').value  print(f\"Image data-type: {type(image0)}\") print(f\"Image0 shape: {image0.shape}\") print(f\"Image1 shape: {image1.shape}\") <pre>Image data-type: &lt;class 'numpy.ndarray'&gt;\nImage0 shape: (4008, 6012, 3)\nImage1 shape: (4008, 6012, 3)\n</pre> In\u00a0[3]: Copied! <pre>matching_cfg = {\n    \"weights\": \"outdoor\",\n    \"keypoint_threshold\": 0.0001,\n    \"max_keypoints\": 8192,\n    \"match_threshold\": 0.2,\n    \"force_cpu\": False,\n}\n\nmatcher = SuperGlueMatcher(matching_cfg)\nmatcher.match(\n    image0,\n    image1,\n    quality=Quality.HIGH,\n    tile_selection=TileSelection.PRESELECTION,\n    grid=[3,2],\n    overlap=200,\n    min_matches_per_tile = 5,\n    do_viz_tiles=False,\n    save_dir = \"./matches/superglue_matches\",\n    geometric_verification=GeometricVerification.PYDEGENSAC,\n    threshold=1.5,\n)\n</pre> matching_cfg = {     \"weights\": \"outdoor\",     \"keypoint_threshold\": 0.0001,     \"max_keypoints\": 8192,     \"match_threshold\": 0.2,     \"force_cpu\": False, }  matcher = SuperGlueMatcher(matching_cfg) matcher.match(     image0,     image1,     quality=Quality.HIGH,     tile_selection=TileSelection.PRESELECTION,     grid=[3,2],     overlap=200,     min_matches_per_tile = 5,     do_viz_tiles=False,     save_dir = \"./matches/superglue_matches\",     geometric_verification=GeometricVerification.PYDEGENSAC,     threshold=1.5, ) <pre>2023-10-03 09:09:38 | [INFO    ] Running inference on device cuda\nLoaded SuperPoint model\nLoaded SuperGlue model (\"outdoor\" weights)\n2023-10-03 09:09:39 | [INFO    ] Matching by tiles...\n2023-10-03 09:09:39 | [INFO    ] Matching tiles by preselection tile selection\n2023-10-03 09:09:39 | [INFO    ]  - Matching tile pair (0, 1)\n2023-10-03 09:09:42 | [INFO    ]  - Matching tile pair (2, 1)\n2023-10-03 09:09:45 | [INFO    ]  - Matching tile pair (2, 2)\n2023-10-03 09:09:48 | [INFO    ]  - Matching tile pair (2, 4)\n2023-10-03 09:09:51 | [INFO    ]  - Matching tile pair (3, 2)\n2023-10-03 09:09:53 | [INFO    ]  - Matching tile pair (3, 3)\n2023-10-03 09:09:56 | [INFO    ]  - Matching tile pair (3, 4)\n2023-10-03 09:09:59 | [INFO    ]  - Matching tile pair (3, 5)\n2023-10-03 09:10:02 | [INFO    ]  - Matching tile pair (4, 4)\n2023-10-03 09:10:04 | [INFO    ]  - Matching tile pair (5, 4)\n2023-10-03 09:10:07 | [INFO    ]  - Matching tile pair (5, 5)\n2023-10-03 09:10:10 | [INFO    ] Restoring full image coordinates of matches...\n2023-10-03 09:10:10 | [INFO    ] Matching by tile completed.\n2023-10-03 09:10:10 | [INFO    ] Matching done!\n2023-10-03 09:10:10 | [INFO    ] Performing geometric verification...\n2023-10-03 09:10:10 | [INFO    ] Pydegensac found 1150 inliers (50.46%)\n2023-10-03 09:10:10 | [INFO    ] Geometric verification done.\n2023-10-03 09:10:10 | [INFO    ] [Timer] | [Matching] preselection=0.864, matching=30.168, geometric_verification=0.022, \nFunction match took 31.0577 seconds\n</pre> Out[3]: <pre>True</pre> <p>The matches with their descriptors and scores are saved in the matcher object. All the results are saved as numpy arrays with float32 dtype. They can be accessed as follows:</p> In\u00a0[4]: Copied! <pre># Get matched keypoints\nmktps0 = matcher.mkpts0\nmktps1 = matcher.mkpts1\n\nprint(f\"Number of matches: {len(mktps0)}\")\nprint(f\"Matches on image0 (first 5):\\n{mktps0[0:5]}\")\nprint(f\"Matches on image1 (first 5):\\n{mktps1[0:5]}\")\n\n# Get descriptors\ndescs0 = matcher.descriptors0\ndescs1 = matcher.descriptors1\nprint(f\"Descriptors shape: {descs0.shape}\") \n\n# Get scores of each matched keypoint\nscores0 = matcher.scores0\nscores1 = matcher.scores1\nprint(f\"Scores shape: {scores0.shape}\")\n\n# Matching confidence\nconfidence = matcher.mconf\nprint(f\"Confidence shape: {confidence.shape}\")\nprint(f\"Confidence (first 5): {confidence[0:5]}\")\n</pre> # Get matched keypoints mktps0 = matcher.mkpts0 mktps1 = matcher.mkpts1  print(f\"Number of matches: {len(mktps0)}\") print(f\"Matches on image0 (first 5):\\n{mktps0[0:5]}\") print(f\"Matches on image1 (first 5):\\n{mktps1[0:5]}\")  # Get descriptors descs0 = matcher.descriptors0 descs1 = matcher.descriptors1 print(f\"Descriptors shape: {descs0.shape}\")   # Get scores of each matched keypoint scores0 = matcher.scores0 scores1 = matcher.scores1 print(f\"Scores shape: {scores0.shape}\")  # Matching confidence confidence = matcher.mconf print(f\"Confidence shape: {confidence.shape}\") print(f\"Confidence (first 5): {confidence[0:5]}\") <pre>Number of matches: 1150\nMatches on image0 (first 5):\n[[   8. 1356.]\n [   8. 1383.]\n [   8. 1384.]\n [  10. 1372.]\n [  11. 1313.]]\nMatches on image1 (first 5):\n[[5342.   98.]\n [5335.  137.]\n [5335.  137.]\n [5341.  122.]\n [5449.    8.]]\nDescriptors shape: (256, 1150)\nScores shape: (1150,)\nConfidence shape: (1150,)\nConfidence (first 5): [0.05422363 0.1714691  0.14908865 0.14837408 0.1697674 ]\n</pre> <p>You can also plot the matches by using the <code>plot_matches</code> function of the ICEpy4D visualization module.</p> In\u00a0[5]: Copied! <pre>from icepy4d.visualization import plot_matches\n\nout = plot_matches(image0=image0, image1=image1, pts0=mktps0, pts1=mktps1, path=\"./matches/superglue_matches.jpg\")\n</pre> from icepy4d.visualization import plot_matches  out = plot_matches(image0=image0, image1=image1, pts0=mktps0, pts1=mktps1, path=\"./matches/superglue_matches.jpg\") In\u00a0[6]: Copied! <pre>matcher = LightGlueMatcher()\nmatcher.match(\n    image0,\n    image1,\n    quality=Quality.HIGH,\n    tile_selection=TileSelection.PRESELECTION,\n    grid=[2, 3],\n    overlap=200,\n    origin=[0, 0],\n    do_viz_matches=True,\n    do_viz_tiles=True,\n    min_matches_per_tile = 3,\n    max_keypoints = 10240,    \n    save_dir=\"./matches/LIGHTGLUE\",\n    geometric_verification=GeometricVerification.PYDEGENSAC,\n    threshold=2,\n    confidence=0.9999,\n)\n</pre> matcher = LightGlueMatcher() matcher.match(     image0,     image1,     quality=Quality.HIGH,     tile_selection=TileSelection.PRESELECTION,     grid=[2, 3],     overlap=200,     origin=[0, 0],     do_viz_matches=True,     do_viz_tiles=True,     min_matches_per_tile = 3,     max_keypoints = 10240,         save_dir=\"./matches/LIGHTGLUE\",     geometric_verification=GeometricVerification.PYDEGENSAC,     threshold=2,     confidence=0.9999, ) <pre>2023-10-03 09:10:13 | [INFO    ] Running inference on device cuda\n2023-10-03 09:10:13 | [INFO    ] Matching by tiles...\n2023-10-03 09:10:13 | [INFO    ] Matching tiles by preselection tile selection\n2023-10-03 09:10:14 | [INFO    ]  - Matching tile pair (0, 2)\n2023-10-03 09:10:15 | [INFO    ]  - Matching tile pair (1, 1)\n2023-10-03 09:10:16 | [INFO    ]  - Matching tile pair (1, 4)\n2023-10-03 09:10:18 | [INFO    ]  - Matching tile pair (2, 4)\n2023-10-03 09:10:19 | [INFO    ]  - Matching tile pair (2, 5)\n2023-10-03 09:10:21 | [INFO    ]  - Matching tile pair (3, 3)\n2023-10-03 09:10:22 | [INFO    ]  - Matching tile pair (4, 3)\n2023-10-03 09:10:24 | [INFO    ]  - Matching tile pair (4, 4)\n2023-10-03 09:10:25 | [INFO    ]  - Matching tile pair (5, 4)\n2023-10-03 09:10:26 | [INFO    ]  - Matching tile pair (5, 5)\n2023-10-03 09:10:28 | [INFO    ] Restoring full image coordinates of matches...\n2023-10-03 09:10:28 | [INFO    ] Matching by tile completed.\n2023-10-03 09:10:28 | [INFO    ] Matching done!\n2023-10-03 09:10:28 | [INFO    ] Performing geometric verification...\n2023-10-03 09:10:28 | [INFO    ] Pydegensac found 1763 inliers (51.66%)\n2023-10-03 09:10:28 | [INFO    ] Geometric verification done.\n2023-10-03 09:10:29 | [INFO    ] [Timer] | [Matching] preselection=0.621, matching=14.147, geometric_verification=0.019, \nFunction match took 16.0027 seconds\n</pre> Out[6]: <pre>True</pre> In\u00a0[7]: Copied! <pre>matcher = LOFTRMatcher()\nmatcher.match(\n    image0,\n    image1,\n    quality=Quality.HIGH,\n    tile_selection=TileSelection.PRESELECTION,\n    grid=[5, 4],\n    overlap=50,\n    save_dir= \"./matches/LOFTR_matches\",\n    geometric_verification=GeometricVerification.PYDEGENSAC,\n    threshold=1.5,\n)\n\nmktps0 = matcher.mkpts0\nmktps1 = matcher.mkpts1\n\nprint(f\"Number of matches: {len(mktps0)}\")\n</pre> matcher = LOFTRMatcher() matcher.match(     image0,     image1,     quality=Quality.HIGH,     tile_selection=TileSelection.PRESELECTION,     grid=[5, 4],     overlap=50,     save_dir= \"./matches/LOFTR_matches\",     geometric_verification=GeometricVerification.PYDEGENSAC,     threshold=1.5, )  mktps0 = matcher.mkpts0 mktps1 = matcher.mkpts1  print(f\"Number of matches: {len(mktps0)}\") <pre>2023-10-03 09:10:29 | [INFO    ] Running inference on device cuda\n2023-10-03 09:10:29 | [INFO    ] Matching by tiles...\n2023-10-03 09:10:29 | [INFO    ] Matching tiles by preselection tile selection\n2023-10-03 09:10:30 | [INFO    ]  - Matching tile pair (1, 1)\n2023-10-03 09:10:30 | [INFO    ]  - Matching tile pair (4, 3)\n2023-10-03 09:10:31 | [INFO    ]  - Matching tile pair (5, 1)\n2023-10-03 09:10:32 | [INFO    ]  - Matching tile pair (5, 2)\n2023-10-03 09:10:33 | [INFO    ]  - Matching tile pair (8, 8)\n2023-10-03 09:10:34 | [INFO    ]  - Matching tile pair (8, 9)\n2023-10-03 09:10:35 | [INFO    ]  - Matching tile pair (9, 9)\n2023-10-03 09:10:36 | [INFO    ]  - Matching tile pair (9, 12)\n2023-10-03 09:10:37 | [INFO    ]  - Matching tile pair (9, 13)\n2023-10-03 09:10:37 | [INFO    ]  - Matching tile pair (10, 10)\n2023-10-03 09:10:38 | [INFO    ]  - Matching tile pair (10, 13)\n2023-10-03 09:10:39 | [INFO    ]  - Matching tile pair (10, 14)\n2023-10-03 09:10:40 | [INFO    ]  - Matching tile pair (11, 10)\n2023-10-03 09:10:41 | [INFO    ]  - Matching tile pair (11, 14)\n2023-10-03 09:10:42 | [INFO    ]  - Matching tile pair (11, 15)\n2023-10-03 09:10:43 | [INFO    ]  - Matching tile pair (12, 16)\n2023-10-03 09:10:44 | [INFO    ]  - Matching tile pair (13, 12)\n2023-10-03 09:10:44 | [INFO    ]  - Matching tile pair (13, 13)\n2023-10-03 09:10:45 | [INFO    ]  - Matching tile pair (13, 17)\n2023-10-03 09:10:46 | [INFO    ]  - Matching tile pair (14, 13)\n2023-10-03 09:10:47 | [INFO    ]  - Matching tile pair (14, 17)\n2023-10-03 09:10:48 | [INFO    ]  - Matching tile pair (15, 14)\n2023-10-03 09:10:49 | [INFO    ]  - Matching tile pair (15, 18)\n2023-10-03 09:10:50 | [INFO    ]  - Matching tile pair (17, 17)\n2023-10-03 09:10:51 | [INFO    ]  - Matching tile pair (18, 17)\n2023-10-03 09:10:52 | [INFO    ] Restoring full image coordinates of matches...\n2023-10-03 09:10:52 | [INFO    ] Matching by tile completed.\n2023-10-03 09:10:52 | [INFO    ] Matching done!\n2023-10-03 09:10:52 | [INFO    ] Performing geometric verification...\n2023-10-03 09:10:53 | [INFO    ] Pydegensac found 3393 inliers (17.91%)\n2023-10-03 09:10:53 | [INFO    ] Geometric verification done.\n2023-10-03 09:10:53 | [INFO    ] [Timer] | [Matching] preselection=0.309, matching=22.242, geometric_verification=0.815, \nFunction match took 23.3744 seconds\nNumber of matches: 3393\n</pre> In\u00a0[\u00a0]: Copied! <pre># Clean up result folders\n\nimport os\nimport shutil\n\nif os.path.exists(\"./matches\"):\n    shutil.rmtree(\"./matches\")\nif os.path.exists(\"./logs\"):\n    shutil.rmtree(\"./logs\")\n</pre> # Clean up result folders  import os import shutil  if os.path.exists(\"./matches\"):     shutil.rmtree(\"./matches\") if os.path.exists(\"./logs\"):     shutil.rmtree(\"./logs\")"},{"location":"module5/matching/#dl-feature-matching","title":"DL Feature Matching\u00b6","text":"<p>This module introduce different algorithms for performing feature matching on pairs of images by using Deep Learning feature matching algorithms, such as SuperGlue, LOFTR or LightGlue.</p> <p>Feature matching consists of extracting corresponding points between two images of the same scene/object. This is a fundamental step in many computer vision applications, such as object detection, tracking, and motion estimation, as well as in the photogrammetric process of image-based 3D reconstruction.</p>"},{"location":"module5/matching/#introduction","title":"Introduction\u00b6","text":""},{"location":"module5/matching/#the-matcher-class","title":"The Matcher class\u00b6","text":"<p>All the matching algorithms implemented in ICEpy4D are implemented as a class, which can be initialized by passing a dictionary of parameters as input. The actual matching is then run by calling the <code>match</code> method of the class instance.</p> <p>Some parameters are common to all the matching algorithms, such as the the <code>Tiling</code> parameters, which are used to split the image in tiles to reduce the memory usage, and the <code>Geometric Verification</code> parameters, which are used to filter out the outliers from the matching results.</p> <p>The common parameters are presented here, while the specific parameters for each algorithm are presented in the corresponding section.</p> <p>When running the matching, additional parameters can be given as arguments to the <code>match</code> method to define the matching behavior. The parameters are the following:</p> <ul> <li>image0: the first image to be matched.</li> <li>image1: the second image to be matched.</li> <li>quality: define the resize factor for the input images. Possible values \"highest\", \"high\" or \"medium\", \"low\". With \"high\", images are matched with full resulution. With \"highest\" images are up-sampled by a factor 2. With \"medium\" and \"low\" images are downsampled respectively by a factor 2 and 4. The default value is \"high\".</li> <li>tile_selection: tile selection approach. Possible values are <code>TileSelection.None</code>, <code>TileSelection.EXHAUSTIVE</code>, <code>TileSelection.GRID</code> or <code>TileSelection.PRESELECTION</code>. Refer to the following \"Tile Section\" section for more information. The default value is <code>TileSelection.PRESELCTION</code>.</li> <li>grid: if tile_selection is not <code>TileSelection.None</code>, this parameter defines the grid size.</li> <li>overlap: if tile_selection is not <code>TileSelection.None</code>, this parameter defines the overlap between tiles.</li> <li>do_viz_matches: if True, the matches are visualized. Default value is False.</li> <li>do_viz_tiles: if True, the tiles are visualized. Default value is False.</li> <li>save_dir: if not None, the matches are saved in the given directory. Default value is None.</li> <li>geometric_verification: defines the geometric verification approach.</li> </ul>"},{"location":"module5/matching/#tile-selection","title":"Tile Selection\u00b6","text":"<p>To guarantee the highest collimation accuracy, by default the matching is performed on full resolution images. However, due to limited memory capacity in mid-class GPUs, high- resolution images captured by DSLR cameras may not fit into GPU memory. To overcome this limitation, ICEPy4D divides the images into smaller regular tiles with maximum dimension of 2000 px, computed over a regular grid. The tile selection can be performed in four different ways:</p> <ol> <li><code>TileSelection.None</code> Images are matched as a whole in just one step. No tiling is performed.</li> <li><code>TileSelection.EXHAUSTIVE</code> All the tiles in the first image are matched with all the tiles in the second image. This approach is very computational demading as the pairs of tiles are all the possible combinations of tiles from the two images and the total number of pairs rises quickly with the number of tiles. Additionally, several spurios matches may be found in tiles that do not overlap in the two images.</li> <li><code>TileSelection.GRID</code> Tiles pairs are selected only based on the position of each tile in the grid, i.e., tile 1 in imageA is matched with tile 1 in imageB, tile 2 in imageA is matched with tile 2 in imageB, and so on. This approach is less computational demanding than the exhaustive one, but it is suitable only for images that are well aligned along a stripe with regular viewing geometry.</li> <li><code>TileSelection.PRESELECTION</code> This is the only actual 'preselection' of the tiles, as the process is carried out in two steps. First, a matching is performed on downsampled images. Subsequently, the full-resolution images are subdivided into regulartiles, and only the tiles that have corresponding features in the low-resolution images are selected as candidates for a second matching step.</li> </ol> <p>When a tile pre-selection approach is chosen, the tile grid must be defined by the <code>tile_grid</code> argument. This is a list of integers that defines the number of tiles along the x and y direction (i.e., number of columns and number of rows). For example, <code>tile_grid=[3,2]</code> defines a grid with 3 columns and 2 rows. Additionally, a parameter specifiyng the overlap between different tiles can be defined by the <code>overlap</code> argument. This is an integer number that defines the number of pixels of overlap between adjacent tiles. For example, <code>overlap=200</code> defines an overlap of 100 pixels between adjacent tiles. The overlap helps to avoid missing matches at the tile boundaries.</p> <p>The following figure shows the tile preselection process. An example of the tiles that are selected for the second matching step are highlighted in green.</p> <p></p>"},{"location":"module5/matching/#geometric-verification","title":"Geometric Verification\u00b6","text":"<p>Geometric verification of the matches is performed by using Pydegensac (Mishkin et al., 2015), that allows for robustly estimate the fundamental matrix. The maximum re-projection error to accept a match is set to 1.5 px by default, but it can be changed by the user. The successfully matched features, together with their descriptors and scores, are saved as a Features object for each camera and stored into the current Epoch object.</p>"},{"location":"module5/matching/#superglue-matching","title":"SuperGlue matching\u00b6","text":"<p>SuperGlue is a Deep Learning-based feature matching algorithm that uses a SuperPoint keypoint detector and a SuperGlue feature matcher. You can find some more information on SuperGlue in the original paper and in the original repository.</p> <p>For running the matching with SuperGlue, a new SuperGlueMatcher object must be initialized. A set of additional parameters can be set when initializing the SuperGlueMatcher object. The parameters are given as a dictionary (see the documentation of the class for more details).</p> <p>The configuration dictionary may contain the following keys:</p> <ul> <li>\"weights\": defines the type of the weights used for SuperGlue inference. It can be either \"indoor\" or \"outdoor\". The default value is \"outdoor\".</li> <li>\"keypoint_threshold\": threshold for the SuperPoint keypoint detector. The default value is 0.001.</li> <li>\"max_keypoints\": maximum number of keypoints to be detected by SuperPoint. If -1, no limit to keypoint detection is set. The default value is -1.</li> <li>\"match_threshold\": threshold for the SuperGlue feature matcher. Default value is 0.3.</li> <li>\"force_cpu\": if True, SuperGlue will run on CPU. Default value is False.</li> <li>\"nms_radius\": radius for non-maximum suppression. Default value is 3.</li> <li>\"sinkhorn_iterations\": number of iterations for the Sinkhorn algorithm. Default value is 20.</li> </ul> <p>If the configuration dictionary is not given, the default values are used.</p>"},{"location":"module5/matching/#lightglue-matching","title":"LightGlue matching\u00b6","text":"<p>LightGlue is a Deep Learning-based feature matching algorithm that uses a SuperPoint or DISK keypoint detectors. It is a recent evolution of the SuperGlue matcher, developed by the Computer Vision Group of ETH Zurich. You can find more information on LightGlue in the original paper and in the original repository.</p> <p>The process of running the matching with LightGlue is very similar to the one of SuperGlue. You just need to initialize a LightGlueMatcher object and run the matching.</p>"},{"location":"module5/matching/#loftr-matching","title":"LOFTR matching\u00b6","text":"<p>The LOFTR matcher shares the same interface as the SuperGlue matcher, therefore the same parameters can be used for the <code>match</code> method. The only difference is in the matcher initialization, which takes no parameters, as default values are defined from Kornia (see the documentation of the class for more details).</p> <p>The matched points can be retrieved as before, but the descriptors are not saved in the matcher object, as they are not computed by LOFTR.</p>"},{"location":"module5/module5/","title":"Stereoprocessing","text":""},{"location":"module5/module5/#overview","title":"Overview","text":"<p>The module 5 of the course is dedicated to an introduction on the stereo processing from fixed-time-lapse cameras.</p>"},{"location":"module5/module5/#learning-outcomes","title":"Learning outcomes","text":"<p>In this module you will learn:</p> <ul> <li>how to perform a feature matching between two images by using Deep Learning algorithms, that are necessary for wide baseline stereo reconstruction;</li> <li>how to perform a stereo reconstruction from a stereo pair of images acquired in a single epoch;</li> <li>how to perform a multitemporal stereo reconstruction from a set of stereo pairs of images acquired in different epochs.</li> </ul>"},{"location":"module5/module5/#table-of-contents","title":"Table of contents","text":"<ol> <li> <p>Getting started: exaplin how to install ICEpy4D that will used for the processing.</p> </li> <li> <p>Data preparation: explains how to download the data and how to organize it for the processing.</p> </li> <li> <p>Feature matching: explain how to perform a feature matching between two images by using Deep Learning algorithms, that are necessary for wide baseline stereo reconstruction.</p> </li> <li> <p>Single epoch stereo reconstruction: explain how to perform a stereo reconstruction from a stereo pair of images acquired in a single epoch.</p> </li> <li> <p>Multitemporal stereo reconstruction: explain how to perform a multitemporal stereo reconstruction from a set of stereo pairs of images acquired in different epochs.</p> </li> </ol>"},{"location":"module5/module5/#software","title":"Software","text":"<p>We will use ICEpy4D Python toolkit for the stereo processing. ICEpy4D is an open-source project for multitemporal photogrammetry developed by the Geodesy and Geomatics Laboratory of Politecnico di Milano and it is available on GitHub at https://github.com/franioli/icepy4d.</p> <p>For the Bundle Adjustment and the dense reconstruction, you also need Agisoft Metashape Professional Edition and the Metashape Python API (see Getting started for more details).</p> <p>A Linux environment is strongly recommended for the processing, but also Windows will work.</p>"},{"location":"module5/multi_epoch_processing/","title":"Multi Epoch Processing","text":"<p>Let's first set up the python environment and define the configuration file</p> In\u00a0[2]: Copied! <pre># Import required standard modules\nimport shutil\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\n# Import required icepy4d4D modules\nfrom icepy4d import core as icepy4d_core\nfrom icepy4d.core import Epoch, Epoches, EpochDataMap\nfrom icepy4d import matching\nfrom icepy4d import sfm\nfrom icepy4d import io\nfrom icepy4d import utils as icepy4d_utils\nfrom icepy4d.metashape import metashape as MS\nfrom icepy4d.utils import initialization as inizialization\n</pre> # Import required standard modules import shutil import sys from pathlib import Path  import numpy as np  # Import required icepy4d4D modules from icepy4d import core as icepy4d_core from icepy4d.core import Epoch, Epoches, EpochDataMap from icepy4d import matching from icepy4d import sfm from icepy4d import io from icepy4d import utils as icepy4d_utils from icepy4d.metashape import metashape as MS from icepy4d.utils import initialization as inizialization In\u00a0[3]: Copied! <pre># Define the path to the configuration file\nCFG_FILE = \"config/config_2022.yaml\"\n</pre> # Define the path to the configuration file CFG_FILE = \"config/config_2022.yaml\" <p>Inizialize all the required variables</p> In\u00a0[4]: Copied! <pre># Parse the configuration file\ncfg_file = Path(CFG_FILE)\ncfg = inizialization.parse_cfg(cfg_file)\n\n# Initialize the timer and logger\ntimer_global = icepy4d_utils.AverageTimer()\nlogger = icepy4d_utils.get_logger()\n\n# Get the list of cameras from the configuration file\ncams = cfg.cams\n\n# Get the list of images from the configuration file\nimages, epoch_dict = inizialization.initialize_image_ds(cfg)\n\n# Initialize an empty Epoches object to store the results of each epoch\nepoches = Epoches(starting_epoch=cfg.proc.epoch_to_process[0])\n</pre> # Parse the configuration file cfg_file = Path(CFG_FILE) cfg = inizialization.parse_cfg(cfg_file)  # Initialize the timer and logger timer_global = icepy4d_utils.AverageTimer() logger = icepy4d_utils.get_logger()  # Get the list of cameras from the configuration file cams = cfg.cams  # Get the list of images from the configuration file images, epoch_dict = inizialization.initialize_image_ds(cfg)  # Initialize an empty Epoches object to store the results of each epoch epoches = Epoches(starting_epoch=cfg.proc.epoch_to_process[0]) <pre>\n================================================================\nICEpy4D\nImage-based Continuos monitoring of glaciers' Evolution with low-cost stereo-cameras and Deep Learning photogrammetry\n2023 - Francesco Ioli - francesco.ioli@polimi.it\n================================================================\n\n</pre> <pre>\nAn exception has occurred, use %tb to see the full traceback.\n\nSystemExit: Configuration file does not exist! Aborting.\n</pre> <pre>/home/francesco/miniconda3/envs/icepy4d/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n</pre> In\u00a0[6]: Copied! <pre># Build the EpochDataMap object find pairs of coheval images for each epoch\nepoch_map = EpochDataMap(cfg.paths.image_dir, time_tolerance_sec=1200)\n\nprint(f\"Epoch 0  Timestamp: {epoch_map[0].timestamp}\")\nprint(f\"\\t Images: {epoch_map[0].images}\")\nprint(f\"\\t Delta t from master camera (Cam 0): {epoch_map[0].dt}\")\n</pre> # Build the EpochDataMap object find pairs of coheval images for each epoch epoch_map = EpochDataMap(cfg.paths.image_dir, time_tolerance_sec=1200)  print(f\"Epoch 0  Timestamp: {epoch_map[0].timestamp}\") print(f\"\\t Images: {epoch_map[0].images}\") print(f\"\\t Delta t from master camera (Cam 0): {epoch_map[0].dt}\") <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n/home/francesco/phd/icepy4d/notebooks/mutlitemporal_workflow.ipynb Cell 8 line 2\n      &lt;a href='vscode-notebook-cell:/home/francesco/phd/icepy4d/notebooks/mutlitemporal_workflow.ipynb#X44sZmlsZQ%3D%3D?line=0'&gt;1&lt;/a&gt; # Build the EpochDataMap object find pairs of coheval images for each epoch\n----&gt; &lt;a href='vscode-notebook-cell:/home/francesco/phd/icepy4d/notebooks/mutlitemporal_workflow.ipynb#X44sZmlsZQ%3D%3D?line=1'&gt;2&lt;/a&gt; epoch_map = EpochDataMap(cfg.paths.image_dir, time_tolerance_sec=1200)\n      &lt;a href='vscode-notebook-cell:/home/francesco/phd/icepy4d/notebooks/mutlitemporal_workflow.ipynb#X44sZmlsZQ%3D%3D?line=3'&gt;4&lt;/a&gt; print(f\"Epoch 0  Timestamp: {epoch_map[0].timestamp}\")\n      &lt;a href='vscode-notebook-cell:/home/francesco/phd/icepy4d/notebooks/mutlitemporal_workflow.ipynb#X44sZmlsZQ%3D%3D?line=4'&gt;5&lt;/a&gt; print(f\"\\t Images: {epoch_map[0].images}\")\n\nNameError: name 'cfg' is not defined</pre> In\u00a0[\u00a0]: Copied! <pre># Set id to process\nep = cfg.proc.epoch_to_process[0]\n\n# Define paths to the epoch directory\nepoch_name = epoch_map.get_timestamp_str(ep)\nepochdir = cfg.paths.results_dir / epoch_name\n\n# Build a dictionary of images containing the name of the cameras as keys and the image path as values\nim_epoch = epoch_map.get_images(ep)\n</pre> # Set id to process ep = cfg.proc.epoch_to_process[0]  # Define paths to the epoch directory epoch_name = epoch_map.get_timestamp_str(ep) epochdir = cfg.paths.results_dir / epoch_name  # Build a dictionary of images containing the name of the cameras as keys and the image path as values im_epoch = epoch_map.get_images(ep) <p>The stereo processing is carried out for each epoch in order to find matched features, estimating camera pose, and triangulating the 3D points. The output of this step is a set of 3D points and their corresponding descriptors.</p> <p>The processing for all the epoches is then iterated in a big loop.</p> In\u00a0[4]: Copied! <pre># Initialize a timer to measure the processing time\ntimer = icepy4d_utils.AverageTimer()\n\n# Get epoch id to process\nep = cfg.proc.epoch_to_process[0]\n\n# Define paths to the epoch directory\nepoch_name = epoch_map.get_timestamp(ep)\nepochdir = cfg.paths.results_dir / epoch_name\n\n# Load an existing epoch or create a new one\nif cfg.proc.load_existing_results:\n    try:\n        # Load existing epcoh from pickle file\n        epoch = Epoch.read_pickle(epochdir / f\"{epoch_name}.pickle\")\n\n    except:\n        logger.error(\n            f\"Unable to load epoch {epoch_name} from pickle file. Creating new epoch...\"\n        )\n        epoch = inizialization.initialize_epoch(\n            cfg=cfg, images=images, epoch_id=ep, epoch_dir=epochdir\n        )\n\nelse:\n    # Create new epoch object\n    epoch = inizialization.initialize_epoch(\n        cfg=cfg, images=images, epoch_id=ep, epoch_dir=epochdir\n    )\n</pre> # Initialize a timer to measure the processing time timer = icepy4d_utils.AverageTimer()  # Get epoch id to process ep = cfg.proc.epoch_to_process[0]  # Define paths to the epoch directory epoch_name = epoch_map.get_timestamp(ep) epochdir = cfg.paths.results_dir / epoch_name  # Load an existing epoch or create a new one if cfg.proc.load_existing_results:     try:         # Load existing epcoh from pickle file         epoch = Epoch.read_pickle(epochdir / f\"{epoch_name}.pickle\")      except:         logger.error(             f\"Unable to load epoch {epoch_name} from pickle file. Creating new epoch...\"         )         epoch = inizialization.initialize_epoch(             cfg=cfg, images=images, epoch_id=ep, epoch_dir=epochdir         )  else:     # Create new epoch object     epoch = inizialization.initialize_epoch(         cfg=cfg, images=images, epoch_id=ep, epoch_dir=epochdir     ) In\u00a0[5]: Copied! <pre># Define matching parameters\nmatching_quality = matching.Quality.HIGH\ntile_selection = matching.TileSelection.PRESELECTION\ntiling_grid = [4, 3]\ntiling_overlap = 200\ngeometric_verification = matching.GeometricVerification.PYDEGENSAC\ngeometric_verification_threshold = 1\ngeometric_verification_confidence = 0.9999\nmatch_dir = epoch.epoch_dir / \"matching\"\n\n# Create a new matcher object\nmatcher = matching.SuperGlueMatcher(cfg.matching)\nmatcher.match(\n    epoch.images[cams[0]].value,\n    epoch.images[cams[1]].value,\n    quality=matching_quality,\n    tile_selection=tile_selection,\n    grid=tiling_grid,\n    overlap=tiling_overlap,\n    do_viz_matches=True,\n    do_viz_tiles=False,\n    save_dir=match_dir,\n    geometric_verification=geometric_verification,\n    threshold=geometric_verification_threshold,\n    confidence=geometric_verification_confidence,\n)\ntimer.update(\"matching\")\n</pre> # Define matching parameters matching_quality = matching.Quality.HIGH tile_selection = matching.TileSelection.PRESELECTION tiling_grid = [4, 3] tiling_overlap = 200 geometric_verification = matching.GeometricVerification.PYDEGENSAC geometric_verification_threshold = 1 geometric_verification_confidence = 0.9999 match_dir = epoch.epoch_dir / \"matching\"  # Create a new matcher object matcher = matching.SuperGlueMatcher(cfg.matching) matcher.match(     epoch.images[cams[0]].value,     epoch.images[cams[1]].value,     quality=matching_quality,     tile_selection=tile_selection,     grid=tiling_grid,     overlap=tiling_overlap,     do_viz_matches=True,     do_viz_tiles=False,     save_dir=match_dir,     geometric_verification=geometric_verification,     threshold=geometric_verification_threshold,     confidence=geometric_verification_confidence, ) timer.update(\"matching\") <pre>2023-09-03 13:19:12 | [INFO    ] Running inference on device cuda\nLoaded SuperPoint model\nLoaded SuperGlue model (\"outdoor\" weights)\n2023-09-03 13:19:13 | [INFO    ] Matching by tiles...\n2023-09-03 13:19:13 | [INFO    ] Matching tiles by preselection tile selection\n2023-09-03 13:19:14 | [INFO    ] Matching completed.\n2023-09-03 13:19:14 | [INFO    ]  - Matching tile pair (3, 2)\n2023-09-03 13:19:16 | [INFO    ]  - Matching tile pair (4, 7)\n2023-09-03 13:19:18 | [INFO    ]  - Matching tile pair (5, 7)\n2023-09-03 13:19:20 | [INFO    ]  - Matching tile pair (5, 8)\n2023-09-03 13:19:23 | [INFO    ]  - Matching tile pair (6, 6)\n2023-09-03 13:19:25 | [INFO    ]  - Matching tile pair (6, 9)\n2023-09-03 13:19:27 | [INFO    ]  - Matching tile pair (7, 6)\n2023-09-03 13:19:29 | [INFO    ]  - Matching tile pair (7, 7)\n2023-09-03 13:19:31 | [INFO    ]  - Matching tile pair (7, 9)\n2023-09-03 13:19:33 | [INFO    ]  - Matching tile pair (7, 10)\n2023-09-03 13:19:36 | [INFO    ]  - Matching tile pair (8, 7)\n2023-09-03 13:19:38 | [INFO    ]  - Matching tile pair (8, 8)\n2023-09-03 13:19:40 | [INFO    ]  - Matching tile pair (8, 10)\n2023-09-03 13:19:42 | [INFO    ]  - Matching tile pair (8, 11)\n2023-09-03 13:19:44 | [INFO    ]  - Matching tile pair (9, 9)\n2023-09-03 13:19:46 | [INFO    ]  - Matching tile pair (10, 9)\n2023-09-03 13:19:49 | [INFO    ]  - Matching tile pair (10, 10)\n2023-09-03 13:19:51 | [INFO    ]  - Matching tile pair (11, 10)\n2023-09-03 13:19:53 | [INFO    ] Restoring full image coordinates of matches...\n2023-09-03 13:19:53 | [INFO    ] Matching by tile completed.\n2023-09-03 13:19:53 | [INFO    ] Matching done!\n2023-09-03 13:19:53 | [INFO    ] Performing geometric verification...\n2023-09-03 13:19:53 | [INFO    ] Pydegensac found 2012 inliers (36.58%)\n2023-09-03 13:19:53 | [INFO    ] Geometric verification done.\n2023-09-03 13:19:55 | [INFO    ] [Timer] | [Matching] preselection=1.014, matching=39.122, geometric_verification=0.449, \nFunction match took 41.6525 seconds\n</pre> <p>Extract the matched features from the Matcher object and save them in the current Epoch object</p> In\u00a0[\u00a0]: Copied! <pre># Define a dictionary with empty Features objects for each camera, which will be filled with the matched keypoints, descriptors and scores\nf = {cam: icepy4d_classes.Features() for cam in cams}\n\n# Stack matched keypoints, descriptors and scores into Features objects\nf[cams[0]].append_features_from_numpy(\n    x=matcher.mkpts0[:, 0],\n    y=matcher.mkpts0[:, 1],\n    descr=matcher.descriptors0,\n    scores=matcher.scores0,\n)\nf[cams[1]].append_features_from_numpy(\n    x=matcher.mkpts1[:, 0],\n    y=matcher.mkpts1[:, 1],\n    descr=matcher.descriptors1,\n    scores=matcher.scores1,\n)\n\n# Store the dictionary with the features in the Epoch object\nepoch.features = f\n</pre> # Define a dictionary with empty Features objects for each camera, which will be filled with the matched keypoints, descriptors and scores f = {cam: icepy4d_classes.Features() for cam in cams}  # Stack matched keypoints, descriptors and scores into Features objects f[cams[0]].append_features_from_numpy(     x=matcher.mkpts0[:, 0],     y=matcher.mkpts0[:, 1],     descr=matcher.descriptors0,     scores=matcher.scores0, ) f[cams[1]].append_features_from_numpy(     x=matcher.mkpts1[:, 0],     y=matcher.mkpts1[:, 1],     descr=matcher.descriptors1,     scores=matcher.scores1, )  # Store the dictionary with the features in the Epoch object epoch.features = f <p>First, perform Relative orientation of the two cameras by using the matched features and the a-priori camera interior orientation.</p> In\u00a0[7]: Copied! <pre># Initialize RelativeOrientation class with a list containing the two\n# cameras and a list contaning the matched features location on each camera.\nrelative_ori = sfm.RelativeOrientation(\n    [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],\n    [\n        epoch.features[cams[0]].kpts_to_numpy(),\n        epoch.features[cams[1]].kpts_to_numpy(),\n    ],\n)\nrelative_ori.estimate_pose(\n    threshold=cfg.matching.pydegensac_threshold,\n    confidence=0.999999,\n    scale_factor=np.linalg.norm(\n        cfg.georef.camera_centers_world[0] - cfg.georef.camera_centers_world[1]\n    ),\n)\n# Store result in camera 1 object\nepoch.cameras[cams[1]] = relative_ori.cameras[1]\n\ncfg.georef.camera_centers_world\n</pre> # Initialize RelativeOrientation class with a list containing the two # cameras and a list contaning the matched features location on each camera. relative_ori = sfm.RelativeOrientation(     [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],     [         epoch.features[cams[0]].kpts_to_numpy(),         epoch.features[cams[1]].kpts_to_numpy(),     ], ) relative_ori.estimate_pose(     threshold=cfg.matching.pydegensac_threshold,     confidence=0.999999,     scale_factor=np.linalg.norm(         cfg.georef.camera_centers_world[0] - cfg.georef.camera_centers_world[1]     ), ) # Store result in camera 1 object epoch.cameras[cams[1]] = relative_ori.cameras[1]  cfg.georef.camera_centers_world <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[7], line 10\n      1 # Initialize RelativeOrientation class with a list containing the two\n      2 # cameras and a list contaning the matched features location on each camera.\n      3 relative_ori = sfm.RelativeOrientation(\n      4     [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],\n      5     [\n   (...)\n      8     ],\n      9 )\n---&gt; 10 relative_ori.estimate_pose(\n     11     threshold=cfg.matching.pydegensac_threshold,\n     12     confidence=0.999999,\n     13     scale_factor=np.linalg.norm(\n     14         cfg.georef.camera_centers_world[0] - cfg.georef.camera_centers_world[1]\n     15     ),\n     16 )\n     17 # Store result in camera 1 object\n     18 epoch.cameras[cams[1]] = relative_ori.cameras[1]\n\nFile ~/phd/icepy4d/src/icepy4d/sfm/two_view_geometry.py:81, in RelativeOrientation.estimate_pose(self, threshold, confidence, scale_factor)\n     75 assert self.cameras[0].extrinsics is not None, print(\n     76     \"Extrinsics matrix is not available for camera 0. Please, compute it before running RelativeOrientation estimation.\"\n     77 )\n     79 # Estimate Realtive Pose with Essential Matrix\n     80 # R, t make up a tuple that performs a change of basis from the first camera's coordinate system to the second camera's coordinate system.\n---&gt; 81 R, t, valid = estimate_pose(\n     82     self.features[0],\n     83     self.features[1],\n     84     self.cameras[0].K,\n     85     self.cameras[1].K,\n     86     thresh=threshold,\n     87     conf=confidence,\n     88 )\n     89 logging.info(f\"Relative Orientation - valid points: {valid.sum()}/{len(valid)}\")\n     91 # If the scaling factor is given, scale the stereo model\n\nTypeError: cannot unpack non-iterable NoneType object</pre> In\u00a0[16]: Copied! <pre>camera_baseline = np.linalg.norm(\n        cfg.georef.camera_centers_world[0] - cfg.georef.camera_centers_world[1]\n    )\nimage = epoch.images[cams[0]].value\n</pre> camera_baseline = np.linalg.norm(         cfg.georef.camera_centers_world[0] - cfg.georef.camera_centers_world[1]     ) image = epoch.images[cams[0]].value In\u00a0[17]: Copied! <pre># Relative orientation\nfeature0 = epoch.features[cams[0]].kpts_to_numpy()\nfeature1 = epoch.features[cams[1]].kpts_to_numpy()\n\nrelative_ori = sfm.RelativeOrientation(\n    [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],\n    [feature0, feature1],\n)\nrelative_ori.estimate_pose(scale_factor=camera_baseline)\nepoch.cameras[cams[1]] = relative_ori.cameras[1]\n\n# Triangulation\ntriang = sfm.Triangulate(\n    [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],\n    [feature0, feature1],\n)\npoints3d = triang.triangulate_two_views(\n    compute_colors=True, image=image, cam_id=1\n)\n</pre> # Relative orientation feature0 = epoch.features[cams[0]].kpts_to_numpy() feature1 = epoch.features[cams[1]].kpts_to_numpy()  relative_ori = sfm.RelativeOrientation(     [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],     [feature0, feature1], ) relative_ori.estimate_pose(scale_factor=camera_baseline) epoch.cameras[cams[1]] = relative_ori.cameras[1]  # Triangulation triang = sfm.Triangulate(     [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],     [feature0, feature1], ) points3d = triang.triangulate_two_views(     compute_colors=True, image=image, cam_id=1 )      <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[17], line 9\n      3 feature1 = epoch.features[cams[1]].kpts_to_numpy()\n      5 relative_ori = sfm.RelativeOrientation(\n      6     [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],\n      7     [feature0, feature1],\n      8 )\n----&gt; 9 relative_ori.estimate_pose(\n     10     scale_factor=camera_baseline,\n     11 )\n     12 epoch.cameras[cams[1]] = relative_ori.cameras[1]\n     14 # Traingulation\n\nFile ~/phd/icepy4d/src/icepy4d/sfm/two_view_geometry.py:81, in RelativeOrientation.estimate_pose(self, threshold, confidence, scale_factor)\n     75 assert self.cameras[0].extrinsics is not None, print(\n     76     \"Extrinsics matrix is not available for camera 0. Please, compute it before running RelativeOrientation estimation.\"\n     77 )\n     79 # Estimate Realtive Pose with Essential Matrix\n     80 # R, t make up a tuple that performs a change of basis from the first camera's coordinate system to the second camera's coordinate system.\n---&gt; 81 R, t, valid = estimate_pose(\n     82     self.features[0],\n     83     self.features[1],\n     84     self.cameras[0].K,\n     85     self.cameras[1].K,\n     86     thresh=threshold,\n     87     conf=confidence,\n     88 )\n     89 logging.info(f\"Relative Orientation - valid points: {valid.sum()}/{len(valid)}\")\n     91 # If the scaling factor is given, scale the stereo model\n\nTypeError: cannot unpack non-iterable NoneType object</pre> <p>Triangulate points into the object space</p> In\u00a0[14]: Copied! <pre>triang = sfm.Triangulate(\n    [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],\n    [\n        epoch.features[cams[0]].kpts_to_numpy(),\n        epoch.features[cams[1]].kpts_to_numpy(),\n    ],\n)\npoints3d = triang.triangulate_two_views(\n    compute_colors=True, image=images[cams[1]].read_image(ep).value, cam_id=1\n)\n\n# Update timer\ntimer.update(\"triangulation\")\n</pre> triang = sfm.Triangulate(     [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],     [         epoch.features[cams[0]].kpts_to_numpy(),         epoch.features[cams[1]].kpts_to_numpy(),     ], ) points3d = triang.triangulate_two_views(     compute_colors=True, image=images[cams[1]].read_image(ep).value, cam_id=1 )  # Update timer timer.update(\"triangulation\") <pre>2023-08-29 17:34:37 | [INFO    ] Point triangulation succeded: 1.0.\n2023-08-29 17:34:37 | [INFO    ] Point colors interpolated\n</pre> <p>Perform an absolute orientation of the current solution (i.e., cameras' exterior orientation and 3D points) by using the ground control points.</p> <p>The coordinates of the two cameras are used as additional ground control points for estimating a Helmert transformation.</p> In\u00a0[13]: Copied! <pre># Get targets available in all cameras. The Labels of valid targets are returned as second element by the get_image_coor_by_label() method\nvalid_targets = epoch.targets.get_image_coor_by_label(\n    cfg.georef.targets_to_use, cam_id=0\n)[1]\n\n# Check if the same targets are available in all cameras\nfor id in range(1, len(cams)):\n    assert (\n        valid_targets\n        == epoch.targets.get_image_coor_by_label(\n            cfg.georef.targets_to_use, cam_id=id\n        )[1]\n    ), f\"\"\"epoch {ep} - {epoch_map.get_timestamp(ep)}: \n    Different targets found in image {id} - {images[cams[id]][ep]}\"\"\"\n\n# Check if there are enough targets\nassert len(valid_targets) &gt; 1, f\"Not enough targets found in epoch {ep}\"\n\n# If not all the targets defined in the config file are found, log a warning and use only the valid targets\nif valid_targets != cfg.georef.targets_to_use:\n    logger.warning(f\"Not all targets found. Using onlys {valid_targets}\")\n\n# Get image and object coordinates of valid targets\nimage_coords = [\n    epoch.targets.get_image_coor_by_label(valid_targets, cam_id=id)[0]\n    for id, cam in enumerate(cams)\n]\nobj_coords = epoch.targets.get_object_coor_by_label(valid_targets)[0]\n\n# Perform absolute orientation\nabs_ori = sfm.Absolute_orientation(\n    (epoch.cameras[cams[0]], epoch.cameras[cams[1]]),\n    points3d_final=obj_coords,\n    image_points=image_coords,\n    camera_centers_world=cfg.georef.camera_centers_world,\n)\nT = abs_ori.estimate_transformation_linear(estimate_scale=True)\npoints3d = abs_ori.apply_transformation(points3d=points3d)\nfor i, cam in enumerate(cams):\n    epoch.cameras[cam] = abs_ori.cameras[i]\n\n# Convert the 3D points to an icepy4d Points object\npts = icepy4d_classes.Points()\npts.append_points_from_numpy(\n    points3d,\n    track_ids=epoch.features[cams[0]].get_track_ids(),\n    colors=triang.colors,\n)\n\n# Store the points in the Epoch object\nepoch.points = pts\n\n# Update timer\ntimer.update(\"absolute orientation\")\n</pre> # Get targets available in all cameras. The Labels of valid targets are returned as second element by the get_image_coor_by_label() method valid_targets = epoch.targets.get_image_coor_by_label(     cfg.georef.targets_to_use, cam_id=0 )[1]  # Check if the same targets are available in all cameras for id in range(1, len(cams)):     assert (         valid_targets         == epoch.targets.get_image_coor_by_label(             cfg.georef.targets_to_use, cam_id=id         )[1]     ), f\"\"\"epoch {ep} - {epoch_map.get_timestamp(ep)}:      Different targets found in image {id} - {images[cams[id]][ep]}\"\"\"  # Check if there are enough targets assert len(valid_targets) &gt; 1, f\"Not enough targets found in epoch {ep}\"  # If not all the targets defined in the config file are found, log a warning and use only the valid targets if valid_targets != cfg.georef.targets_to_use:     logger.warning(f\"Not all targets found. Using onlys {valid_targets}\")  # Get image and object coordinates of valid targets image_coords = [     epoch.targets.get_image_coor_by_label(valid_targets, cam_id=id)[0]     for id, cam in enumerate(cams) ] obj_coords = epoch.targets.get_object_coor_by_label(valid_targets)[0]  # Perform absolute orientation abs_ori = sfm.Absolute_orientation(     (epoch.cameras[cams[0]], epoch.cameras[cams[1]]),     points3d_final=obj_coords,     image_points=image_coords,     camera_centers_world=cfg.georef.camera_centers_world, ) T = abs_ori.estimate_transformation_linear(estimate_scale=True) points3d = abs_ori.apply_transformation(points3d=points3d) for i, cam in enumerate(cams):     epoch.cameras[cam] = abs_ori.cameras[i]  # Convert the 3D points to an icepy4d Points object pts = icepy4d_classes.Points() pts.append_points_from_numpy(     points3d,     track_ids=epoch.features[cams[0]].get_track_ids(),     colors=triang.colors, )  # Store the points in the Epoch object epoch.points = pts  # Update timer timer.update(\"absolute orientation\") <pre>2023-08-29 17:34:18 | [WARNING ] Warning: target T2 is not present on camera 0.\n2023-08-29 17:34:18 | [WARNING ] Warning: target F10_2 is not present on camera 0.\n2023-08-29 17:34:18 | [WARNING ] Warning: target T2 is not present on camera 1.\n2023-08-29 17:34:18 | [WARNING ] Warning: target F10_2 is not present on camera 1.\n2023-08-29 17:34:18 | [WARNING ] Not all targets found. Using onlys ['F2', 'F12', 'F13']\n2023-08-29 17:34:18 | [INFO    ] Point triangulation succeded: 1.0.\n</pre> <p>Save the current Epoch object as a pickle file.</p> In\u00a0[9]: Copied! <pre># Save epoch as a pickle object\nif epoch.save_pickle(f\"{epoch.epoch_dir}/{epoch}.pickle\"):\n    logger.info(f\"{epoch} saved successfully\")\nelse:\n    logger.error(f\"Unable to save {epoch}\")\n</pre> # Save epoch as a pickle object if epoch.save_pickle(f\"{epoch.epoch_dir}/{epoch}.pickle\"):     logger.info(f\"{epoch} saved successfully\") else:     logger.error(f\"Unable to save {epoch}\") <pre>2023-08-29 18:32:30 | [INFO    ] 2022-05-01_14:01:15 saved successfully\n</pre> <p>Stack all the processing of a single epoch into a function and iterate over all the epoches</p> In\u00a0[7]: Copied! <pre># Define processing for single epoch\ndef process_epoch(epoch, cfg, timer) -&gt; Epoch:\n\n    cams = cfg.cams\n    epochdir = epoch.epoch_dir\n    match_dir = epochdir / \"matching\"\n   \n    # Matching\n    matching_quality = matching.Quality.HIGH\n    tile_selection = matching.TileSelection.PRESELECTION\n    tiling_grid = [4, 3]\n    tiling_overlap = 200\n    geometric_verification = matching.GeometricVerification.PYDEGENSAC\n    geometric_verification_threshold = 1\n    geometric_verification_confidence = 0.9999\n    matcher = matching.SuperGlueMatcher(cfg.matching)    \n    matcher.match(\n        epoch.images[cams[0]].value,\n        epoch.images[cams[1]].value,\n        quality=matching_quality,\n        tile_selection=tile_selection,\n        grid=tiling_grid,\n        overlap=tiling_overlap,\n        do_viz_matches=True,\n        do_viz_tiles=False,\n        save_dir=match_dir,\n        geometric_verification=geometric_verification,\n        threshold=geometric_verification_threshold,\n        confidence=geometric_verification_confidence,\n    )\n    f = {cam: icepy4d_classes.Features() for cam in cams}\n    f[cams[0]].append_features_from_numpy(\n        x=matcher.mkpts0[:, 0],\n        y=matcher.mkpts0[:, 1],\n        descr=matcher.descriptors0,\n        scores=matcher.scores0,\n    )\n    f[cams[1]].append_features_from_numpy(\n        x=matcher.mkpts1[:, 0],\n        y=matcher.mkpts1[:, 1],\n        descr=matcher.descriptors1,\n        scores=matcher.scores1,\n    )\n    epoch.features = f\n    timer.update(\"matching\")\n    \n    # Relative orientation\n    relative_ori = sfm.RelativeOrientation(\n    [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],\n    [\n        epoch.features[cams[0]].kpts_to_numpy(),\n        epoch.features[cams[1]].kpts_to_numpy(),\n    ],\n    )\n    relative_ori.estimate_pose(\n    threshold=cfg.matching.pydegensac_threshold,\n    confidence=0.999999,\n    scale_factor=np.linalg.norm(\n        cfg.georef.camera_centers_world[0] - cfg.georef.camera_centers_world[1]\n    ),\n    )\n    epoch.cameras[cams[1]] = relative_ori.cameras[1]\n    timer.update(\"relative orientation\")\n\n    # Triangulation\n    triang = sfm.Triangulate(\n        [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],\n        [\n            epoch.features[cams[0]].kpts_to_numpy(),\n            epoch.features[cams[1]].kpts_to_numpy(),\n        ],\n    )\n    points3d = triang.triangulate_two_views(\n        compute_colors=True, image=images[cams[1]].read_image(ep).value, cam_id=1\n    )\n    timer.update(\"triangulation\")    \n\n    # Absolute orientation\n    valid_targets = epoch.targets.get_image_coor_by_label(\n        cfg.georef.targets_to_use, cam_id=0\n    )[1]\n    for id in range(1, len(cams)):\n        assert (\n            valid_targets\n            == epoch.targets.get_image_coor_by_label(\n                cfg.georef.targets_to_use, cam_id=id\n            )[1]\n        ), f\"\"\"epoch {ep} - {epoch_map.get_timestamp(ep)}: \n        Different targets found in image {id} - {images[cams[id]][ep]}\"\"\"\n    assert len(valid_targets) &gt; 1, f\"Not enough targets found in epoch {ep}\"\n    if valid_targets != cfg.georef.targets_to_use:\n        logger.warning(f\"Not all targets found. Using onlys {valid_targets}\")\n\n    image_coords = [\n        epoch.targets.get_image_coor_by_label(valid_targets, cam_id=id)[0]\n        for id, cam in enumerate(cams)\n    ]\n    obj_coords = epoch.targets.get_object_coor_by_label(valid_targets)[0]\n\n    abs_ori = sfm.Absolute_orientation(\n        (epoch.cameras[cams[0]], epoch.cameras[cams[1]]),\n        points3d_final=obj_coords,\n        image_points=image_coords,\n        camera_centers_world=cfg.georef.camera_centers_world,\n    )\n    T = abs_ori.estimate_transformation_linear(estimate_scale=True)\n    points3d = abs_ori.apply_transformation(points3d=points3d)\n    for i, cam in enumerate(cams):\n        epoch.cameras[cam] = abs_ori.cameras[i]\n\n    pts = icepy4d_classes.Points()\n    pts.append_points_from_numpy(\n        points3d,\n        track_ids=epoch.features[cams[0]].get_track_ids(),\n        colors=triang.colors,\n    )\n    epoch.points = pts\n    timer.update(\"absolute orientation\") \n\n    # Save epoch as a pickle object\n    if epoch.save_pickle(f\"{epoch.epoch_dir}/{epoch}.pickle\"):\n        logger.info(f\"{epoch} saved successfully\")\n    else:\n        logger.error(f\"Unable to save {epoch}\")\n\n    return epoch\n</pre> # Define processing for single epoch def process_epoch(epoch, cfg, timer) -&gt; Epoch:      cams = cfg.cams     epochdir = epoch.epoch_dir     match_dir = epochdir / \"matching\"         # Matching     matching_quality = matching.Quality.HIGH     tile_selection = matching.TileSelection.PRESELECTION     tiling_grid = [4, 3]     tiling_overlap = 200     geometric_verification = matching.GeometricVerification.PYDEGENSAC     geometric_verification_threshold = 1     geometric_verification_confidence = 0.9999     matcher = matching.SuperGlueMatcher(cfg.matching)         matcher.match(         epoch.images[cams[0]].value,         epoch.images[cams[1]].value,         quality=matching_quality,         tile_selection=tile_selection,         grid=tiling_grid,         overlap=tiling_overlap,         do_viz_matches=True,         do_viz_tiles=False,         save_dir=match_dir,         geometric_verification=geometric_verification,         threshold=geometric_verification_threshold,         confidence=geometric_verification_confidence,     )     f = {cam: icepy4d_classes.Features() for cam in cams}     f[cams[0]].append_features_from_numpy(         x=matcher.mkpts0[:, 0],         y=matcher.mkpts0[:, 1],         descr=matcher.descriptors0,         scores=matcher.scores0,     )     f[cams[1]].append_features_from_numpy(         x=matcher.mkpts1[:, 0],         y=matcher.mkpts1[:, 1],         descr=matcher.descriptors1,         scores=matcher.scores1,     )     epoch.features = f     timer.update(\"matching\")          # Relative orientation     relative_ori = sfm.RelativeOrientation(     [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],     [         epoch.features[cams[0]].kpts_to_numpy(),         epoch.features[cams[1]].kpts_to_numpy(),     ],     )     relative_ori.estimate_pose(     threshold=cfg.matching.pydegensac_threshold,     confidence=0.999999,     scale_factor=np.linalg.norm(         cfg.georef.camera_centers_world[0] - cfg.georef.camera_centers_world[1]     ),     )     epoch.cameras[cams[1]] = relative_ori.cameras[1]     timer.update(\"relative orientation\")      # Triangulation     triang = sfm.Triangulate(         [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],         [             epoch.features[cams[0]].kpts_to_numpy(),             epoch.features[cams[1]].kpts_to_numpy(),         ],     )     points3d = triang.triangulate_two_views(         compute_colors=True, image=images[cams[1]].read_image(ep).value, cam_id=1     )     timer.update(\"triangulation\")          # Absolute orientation     valid_targets = epoch.targets.get_image_coor_by_label(         cfg.georef.targets_to_use, cam_id=0     )[1]     for id in range(1, len(cams)):         assert (             valid_targets             == epoch.targets.get_image_coor_by_label(                 cfg.georef.targets_to_use, cam_id=id             )[1]         ), f\"\"\"epoch {ep} - {epoch_map.get_timestamp(ep)}:          Different targets found in image {id} - {images[cams[id]][ep]}\"\"\"     assert len(valid_targets) &gt; 1, f\"Not enough targets found in epoch {ep}\"     if valid_targets != cfg.georef.targets_to_use:         logger.warning(f\"Not all targets found. Using onlys {valid_targets}\")      image_coords = [         epoch.targets.get_image_coor_by_label(valid_targets, cam_id=id)[0]         for id, cam in enumerate(cams)     ]     obj_coords = epoch.targets.get_object_coor_by_label(valid_targets)[0]      abs_ori = sfm.Absolute_orientation(         (epoch.cameras[cams[0]], epoch.cameras[cams[1]]),         points3d_final=obj_coords,         image_points=image_coords,         camera_centers_world=cfg.georef.camera_centers_world,     )     T = abs_ori.estimate_transformation_linear(estimate_scale=True)     points3d = abs_ori.apply_transformation(points3d=points3d)     for i, cam in enumerate(cams):         epoch.cameras[cam] = abs_ori.cameras[i]      pts = icepy4d_classes.Points()     pts.append_points_from_numpy(         points3d,         track_ids=epoch.features[cams[0]].get_track_ids(),         colors=triang.colors,     )     epoch.points = pts     timer.update(\"absolute orientation\")       # Save epoch as a pickle object     if epoch.save_pickle(f\"{epoch.epoch_dir}/{epoch}.pickle\"):         logger.info(f\"{epoch} saved successfully\")     else:         logger.error(f\"Unable to save {epoch}\")      return epoch In\u00a0[8]: Copied! <pre>epoch = process_epoch(epoch, cfg, timer)\n</pre> epoch = process_epoch(epoch, cfg, timer) <pre>2023-08-29 18:40:56 | [INFO    ] Running inference on device cuda\nLoaded SuperPoint model\nLoaded SuperGlue model (\"outdoor\" weights)\n2023-08-29 18:40:57 | [INFO    ] Matching by tiles...\n2023-08-29 18:40:57 | [INFO    ] Matching tiles by preselection tile selection\n2023-08-29 18:40:57 | [INFO    ] Matching completed.\n2023-08-29 18:40:57 | [INFO    ]  - Matching tile pair (3, 2)\n2023-08-29 18:40:59 | [INFO    ]  - Matching tile pair (4, 7)\n2023-08-29 18:41:02 | [INFO    ]  - Matching tile pair (5, 7)\n2023-08-29 18:41:04 | [INFO    ]  - Matching tile pair (5, 8)\n2023-08-29 18:41:06 | [INFO    ]  - Matching tile pair (6, 6)\n2023-08-29 18:41:09 | [INFO    ]  - Matching tile pair (6, 9)\n2023-08-29 18:41:11 | [INFO    ]  - Matching tile pair (7, 6)\n2023-08-29 18:41:13 | [INFO    ]  - Matching tile pair (7, 7)\n2023-08-29 18:41:16 | [INFO    ]  - Matching tile pair (7, 9)\n2023-08-29 18:41:18 | [INFO    ]  - Matching tile pair (7, 10)\n2023-08-29 18:41:20 | [INFO    ]  - Matching tile pair (8, 7)\n2023-08-29 18:41:23 | [INFO    ]  - Matching tile pair (8, 8)\n2023-08-29 18:41:25 | [INFO    ]  - Matching tile pair (8, 10)\n2023-08-29 18:41:27 | [INFO    ]  - Matching tile pair (8, 11)\n2023-08-29 18:41:30 | [INFO    ]  - Matching tile pair (9, 9)\n2023-08-29 18:41:32 | [INFO    ]  - Matching tile pair (10, 9)\n2023-08-29 18:41:34 | [INFO    ]  - Matching tile pair (10, 10)\n2023-08-29 18:41:37 | [INFO    ]  - Matching tile pair (11, 10)\n2023-08-29 18:41:39 | [INFO    ] Restoring full image coordinates of matches...\n2023-08-29 18:41:39 | [INFO    ] Matching by tile completed.\n2023-08-29 18:41:39 | [INFO    ] Matching done!\n2023-08-29 18:41:39 | [INFO    ] Performing geometric verification...\n2023-08-29 18:41:40 | [INFO    ] Pydegensac found 2011 inliers (36.56%)\n2023-08-29 18:41:40 | [INFO    ] Geometric verification done.\n2023-08-29 18:41:41 | [INFO    ] [Timer] | [Matching] preselection=0.314, matching=42.250, geometric_verification=0.390, \nFunction match took 44.0152 seconds\n2023-08-29 18:41:41 | [INFO    ] Relative Orientation - valid points: 1874/2011\n2023-08-29 18:41:41 | [INFO    ] Relative orientation Succeded.\n2023-08-29 18:41:41 | [INFO    ] Point triangulation succeded: 1.0.\n2023-08-29 18:41:41 | [INFO    ] Point colors interpolated\n2023-08-29 18:41:41 | [WARNING ] Warning: target T2 is not present on camera 0.\n2023-08-29 18:41:41 | [WARNING ] Warning: target F10_2 is not present on camera 0.\n2023-08-29 18:41:41 | [WARNING ] Warning: target T2 is not present on camera 1.\n2023-08-29 18:41:41 | [WARNING ] Warning: target F10_2 is not present on camera 1.\n2023-08-29 18:41:41 | [WARNING ] Not all targets found. Using onlys ['F2', 'F12', 'F13']\n2023-08-29 18:41:41 | [INFO    ] Point triangulation succeded: 1.0.\n2023-08-29 18:41:41 | [INFO    ] 2022-05-01_14:01:15 saved successfully\n</pre> <pre>Warning: License lost, attempting to recover (license status -21)\nWarning: Failed to recover license (attempt 1 of 5, license status -21)\nWarning: Failed to recover license (attempt 2 of 5, license status -21)\nWarning: Failed to recover license (attempt 3 of 5, license status -21)\nWarning: Failed to recover license (attempt 4 of 5, license status -21)\nWarning: Failed to recover license (attempt 5 of 5, license status -43)\nLicense successfully recovered\n</pre> In\u00a0[\u00a0]: Copied! <pre># Add epoch to epoches object\nepoches.add_epoch(epoch)\n</pre> # Add epoch to epoches object epoches.add_epoch(epoch) In\u00a0[\u00a0]: Copied! <pre>logger.info(\"------------------------------------------------------\")\nlogger.info(\"Processing started:\")\ntimer = icepy4d_utils.AverageTimer()\niter = 0  # necessary only for printing the number of processed iteration\n</pre> logger.info(\"------------------------------------------------------\") logger.info(\"Processing started:\") timer = icepy4d_utils.AverageTimer() iter = 0  # necessary only for printing the number of processed iteration"},{"location":"module5/multi_epoch_processing/#multi-epoch-processing","title":"Multi Epoch Processing\u00b6","text":""},{"location":"module5/multi_epoch_processing/#initialization","title":"Initialization\u00b6","text":""},{"location":"module5/multi_epoch_processing/#icepy4d-main-structures","title":"ICEpy4D main structures\u00b6","text":"<p>For the processing, you have to initialize all the required variables. This procedure is the same also for multi-epoch processing.</p>"},{"location":"module5/multi_epoch_processing/#epochdatamap","title":"EpochDataMap\u00b6","text":"<p>The <code>EpochDataMap</code> class is a critical structure for multi-epoch processing in ICEpy4D. It helps organize and manage data for each epoch, including timestamps, associated images, and time differences. <code>EpochDataMap</code> is a dictionary that contains the information the timestamp (i.e., date and time) of each epoch and the images belonging to that epoch. The purpose of this class is to give a east-to-use tool for associating the timestamp of each epoch to the images belonging to that epoch.</p> <p>The <code>EpochDataMap</code> allows for automatically define the association of the images to the epochs, based on the timestamp of the images. The images are associated to the epoch with the closest timestamp. The <code>EpochDataMap</code> first select the master camera (by default, the first camera). Then, for each image of the master camera, it looks for the closest image taken by all the other cameras (i.e., named 'slave cameras') based on their timestamp. As the cameras may not be perfectly syncronized, a time tolerance can be defined to allow for a maximum time difference between the images of different cameras (by default, 180 seconds). If the time difference between a slave image and the master image is larger than the time tolerance, the slave image is not associated to the current epoch. It may be possible that one epoch contains only the image of the master camera (e.g., due to a disruption of the other cameras during that day), therefore a minimum number of images can be defined for an epoch to be included in the <code>EpochDataMap</code> (by default, 2 images). If less than the minimum number of images are associated to an epoch, the epoch is discarded.</p> <ol> <li><p>The EpochDataMap structure</p> <p>The key of the dictionary is the epoch number (an integer number, starting from 0). The values of the dictionary are the <code>EpochData</code> class are again dictionaries that contains the timestamp of the epoch, the images associated at that epoch and the time difference between the timestamp of each camera and that of the master camera (by default, the first camera). For instance, a value of the <code>EpochDataMap</code> dictionary is the following:</p> <pre>epoch_data_map[0] = {\n    'timestamp': datetime.datetime(2023, 6, 23, 9, 59, 58), # timestamp of the first epoch\n    'images': { \n        'p1': Image data/img/p1/p1_20230623_095958_IMG_0845.JPG,\n        'p2': Image data/img/p2/p2_20230623_100000_IMG_0582.JPG\n        }, # images of the first epoch\n    'dt': {'p1': datetime.timedelta(0), 'p2': datetime.timedelta(seconds=2)} # time difference between each camera and the master camera\n}\n</pre> <p>For accessing the data inside the dictionary for each epoch, you can use the <code>dot notation</code> for getting the timestamp, the image dictionary and the time differences.</p> <pre>epoch_map[0].timestamp \nepoch_map[0].images \nepoch_map[0].dt\n</pre> <p>The epoch timestamp (<code>epoch_map[n].timestamp</code>) is taken from the timestamp of the master camera, and it is stored as a datetime object.</p> <pre><code>epoch_map[0].timestamp = datetime.datetime(2023, 6, 23, 9, 59, 58)\n</code></pre> <p>The images of each epoch is again a dictionary with the camera names (i.e., the name of the folder containing the image sequence) as keys and Image objects as values.</p> <pre>epoch_map[0].images['p1'] = Image(\"data/img/p1/p1_20230623_095958_IMG_0845.JPG\")\nepoch_map[0].images['p2'] = Image(\"data/img/p2/p2_20230623_100000_IMG_0582.JPG\")\n</pre> </li> <li><p>How to inizialize a EpochDataMap</p> <p>Initialize the <code>EpochDataMap</code> object by providing the image directory and optional parameters like the time tolerance (maximum time difference allowed between images from different cameras) and minimum number of images required for an epoch to be included.</p> <pre>epoch_map = EpochDataMap('path_to_image_directory', time_tolerance_sec=180, min_images=2)\n</pre> <p>If not specified, the time tolerance is set to 180 seconds and the minimum number of images is set to 2.</p> </li> </ol>"},{"location":"module5/multi_epoch_processing/#stereo-processing","title":"Stereo Processing\u00b6","text":""},{"location":"module5/multi_epoch_processing/#load-or-create-a-new-epoch-object","title":"Load or create a new Epoch object\u00b6","text":""},{"location":"module5/multi_epoch_processing/#feature-matching-with-superglue","title":"Feature matching with SuperGlue\u00b6","text":""},{"location":"module5/multi_epoch_processing/#scene-reconstruction","title":"Scene reconstruction\u00b6","text":""},{"location":"module5/multi_epoch_processing/#big-loop-over-the-epoches","title":"Big loop over the epoches\u00b6","text":""},{"location":"module5/single_epoch_stereo_reconstruction/","title":"Stereo reconstruction","text":"<p>Let's first set up the python environment by importing the required libraries.</p> In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\n# Import required standard modules\nfrom pathlib import Path\n\nimport numpy as np\n\n# Import required icepy4d4D modules\nfrom icepy4d import core as icecore\nfrom icepy4d.core import Epoch\nfrom icepy4d import matching\nfrom icepy4d import sfm\nfrom icepy4d import io\nfrom icepy4d import utils\nfrom icepy4d.metashape import metashape as MS\nfrom icepy4d.utils import initialization\n</pre> %load_ext autoreload %autoreload 2  # Import required standard modules from pathlib import Path  import numpy as np  # Import required icepy4d4D modules from icepy4d import core as icecore from icepy4d.core import Epoch from icepy4d import matching from icepy4d import sfm from icepy4d import io from icepy4d import utils from icepy4d.metashape import metashape as MS from icepy4d.utils import initialization <pre>Jupyter environment detected. Enabling Open3D WebVisualizer.\n[Open3D INFO] WebRTC GUI backend enabled.\n[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n</pre> <p>First, you have to define the path to the configuration file (<code>.yaml</code> file). This file contains all the paths and parameters needed to run the code. See the <code>config.yaml</code> file in the nootebook folder for an example and refer to the documentation for how to prepare all the data for ICEpy4D. Additionally, you can setup a logger for the code to print out some information and a timer to measure the runtime of the code.</p> In\u00a0[18]: Copied! <pre># Parse the configuration file\nCFG_FILE = \"config.yaml\"\n\n# Parse the configuration file\ncfg_file = Path(CFG_FILE)\ncfg = initialization.parse_cfg(cfg_file, ignore_errors=True)\n\n# Initialize the logger\nlogger = utils.get_logger()\n\n# Initialize a timer to measure the processing time\ntimer = utils.AverageTimer()\n\n# Get the list of cameras from the configuration file\ncams = cfg.cams\n</pre> # Parse the configuration file CFG_FILE = \"config.yaml\"  # Parse the configuration file cfg_file = Path(CFG_FILE) cfg = initialization.parse_cfg(cfg_file, ignore_errors=True)  # Initialize the logger logger = utils.get_logger()  # Initialize a timer to measure the processing time timer = utils.AverageTimer()  # Get the list of cameras from the configuration file cams = cfg.cams <pre>\n================================================================\nICEpy4D\nImage-based Continuos monitoring of glaciers' Evolution with low-cost stereo-cameras and Deep Learning photogrammetry\n2023 - Francesco Ioli - francesco.ioli@polimi.it\n================================================================\n\n2023-10-03 10:41:41 | [INFO    ] Configuration file: config.yaml\n2023-10-03 10:41:41 | [INFO    ] Epoch_to_process set to a pair of values. Expanding it for a range of epoches from epoch 0 to 158.\n</pre> In\u00a0[3]: Copied! <pre># Build a dictionary of images containing the name of the cameras as keys and Image objects as values\nim_epoch = {\n    cams[0]: icecore.Image(\"../data/img/p1/IMG_2637.jpg\"),\n    cams[1]: icecore.Image(\"../data/img/p2/IMG_1112.jpg\")\n}\n\n# Get epoch timestamp as the timestamp of the first image and define epoch directory\nepoch_timestamp = im_epoch[cams[0]].timestamp\nepochdir = cfg.paths.results_dir / epoch_timestamp\n</pre> # Build a dictionary of images containing the name of the cameras as keys and Image objects as values im_epoch = {     cams[0]: icecore.Image(\"../data/img/p1/IMG_2637.jpg\"),     cams[1]: icecore.Image(\"../data/img/p2/IMG_1112.jpg\") }  # Get epoch timestamp as the timestamp of the first image and define epoch directory epoch_timestamp = im_epoch[cams[0]].timestamp epochdir = cfg.paths.results_dir / epoch_timestamp <p>We now have to load/define all the other information for the 3D reconstruction. In particular, this includes the camera objects (by loading their pre-calibrated intrinsics orientation) and the GCPs (by loading their coordinates in the object space and their projections in the image space).</p> In\u00a0[4]: Copied! <pre># Load cameras\ncams_ep = {}\nfor cam in cams:\n    calib = icecore.Calibration(cfg.paths.calibration_dir / f\"{cam}.txt\")\n    cams_ep[cam] = calib.to_camera()\n\n# Load targets\ntarget_paths = [\n    cfg.georef.target_dir\n    / (im_epoch[cam].stem + cfg.georef.target_file_ext)\n    for cam in cams\n]\ntarg_ep = icecore.Targets(\n    im_file_path=target_paths,\n    obj_file_path=cfg.georef.target_dir\n    / cfg.georef.target_world_file,\n)\n\n# Create empty features\nfeat_ep = {cam: icecore.Features() for cam in cams}\n</pre> # Load cameras cams_ep = {} for cam in cams:     calib = icecore.Calibration(cfg.paths.calibration_dir / f\"{cam}.txt\")     cams_ep[cam] = calib.to_camera()  # Load targets target_paths = [     cfg.georef.target_dir     / (im_epoch[cam].stem + cfg.georef.target_file_ext)     for cam in cams ] targ_ep = icecore.Targets(     im_file_path=target_paths,     obj_file_path=cfg.georef.target_dir     / cfg.georef.target_world_file, )  # Create empty features feat_ep = {cam: icecore.Features() for cam in cams} <p>Now we can create an Epoch object by passing all the information that we have previously defined:</p> In\u00a0[5]: Copied! <pre># Create the epoch object\nepoch = Epoch(\n    timestamp=epoch_timestamp,\n    images=im_epoch,\n    cameras=cams_ep,\n    features=feat_ep,\n    targets=targ_ep,\n    epoch_dir=epochdir,\n)\nprint(f\"Epoch: {epoch}\")\n</pre> # Create the epoch object epoch = Epoch(     timestamp=epoch_timestamp,     images=im_epoch,     cameras=cams_ep,     features=feat_ep,     targets=targ_ep,     epoch_dir=epochdir, ) print(f\"Epoch: {epoch}\") <pre>Epoch: 2022-05-01_14-01-15\n</pre> <p>The information stored inside the epoch object can be easily accessed as attributes of the object. For example, to get the camera objects, you can access the <code>cameras</code> attribute of the epoch object.</p> <pre>epoch.timestamp # Gives the timestamp of the epoch\nepoch.cameras # Gives a dictionary with the camera name as key and the camera object as value\nepoch.images # GIves a dictionary with the camera name as key and the image object as value\nepoch.features # Gives a dictionary with the camera name as key and the features object as value\nepoch.targets # Gives a Target object containing both the coordinates in the object space and the projections in the image space\n</pre> In\u00a0[6]: Copied! <pre>print(epoch.timestamp)\nprint(epoch.cameras)\nprint(epoch.images)\nprint(epoch.features)\nprint(epoch.targets)\n</pre> print(epoch.timestamp) print(epoch.cameras) print(epoch.images) print(epoch.features) print(epoch.targets) <pre>2022-05-01 14:01:15\n{'p1': Camera (f=6621.743457206283, img_size=(6012.0, 4008.0), 'p2': Camera (f=9267.892627662095, img_size=(6012.0, 4008.0)}\n{'p1': Image ../data/img/p1/IMG_2637.jpg, 'p2': Image ../data/img/p2/IMG_1112.jpg}\n{'p1': Features with 0 features, 'p2': Features with 0 features}\n&lt;icepy4d.core.targets.Targets object at 0x7f915b22b4f0&gt;\n</pre> In\u00a0[7]: Copied! <pre>matcher = matching.LightGlueMatcher()\nmatcher.match(\n    epoch.images[cams[0]].value,\n    epoch.images[cams[1]].value,\n    quality=matching.Quality.HIGH,\n    tile_selection= matching.TileSelection.PRESELECTION,\n    grid=[2, 3],\n    overlap=200,\n    origin=[0, 0],\n    do_viz_matches=True,\n    do_viz_tiles=True,\n    min_matches_per_tile = 3,\n    max_keypoints = 8196,    \n    save_dir=epoch.epoch_dir / \"matching\",\n    geometric_verification=matching.GeometricVerification.PYDEGENSAC,\n    threshold=2,\n    confidence=0.9999,\n)\ntimer.update(\"matching\")\n</pre> matcher = matching.LightGlueMatcher() matcher.match(     epoch.images[cams[0]].value,     epoch.images[cams[1]].value,     quality=matching.Quality.HIGH,     tile_selection= matching.TileSelection.PRESELECTION,     grid=[2, 3],     overlap=200,     origin=[0, 0],     do_viz_matches=True,     do_viz_tiles=True,     min_matches_per_tile = 3,     max_keypoints = 8196,         save_dir=epoch.epoch_dir / \"matching\",     geometric_verification=matching.GeometricVerification.PYDEGENSAC,     threshold=2,     confidence=0.9999, ) timer.update(\"matching\") <pre>2023-10-03 10:30:44 | [INFO    ] Running inference on device cuda\n</pre> <pre>2023-10-03 10:30:45 | [INFO    ] Matching by tiles...\n2023-10-03 10:30:45 | [INFO    ] Matching tiles by preselection tile selection\n2023-10-03 10:30:46 | [INFO    ]  - Matching tile pair (1, 1)\n2023-10-03 10:30:48 | [INFO    ]  - Matching tile pair (1, 4)\n2023-10-03 10:30:49 | [INFO    ]  - Matching tile pair (2, 4)\n2023-10-03 10:30:50 | [INFO    ]  - Matching tile pair (2, 5)\n2023-10-03 10:30:52 | [INFO    ]  - Matching tile pair (3, 3)\n2023-10-03 10:30:53 | [INFO    ]  - Matching tile pair (4, 3)\n2023-10-03 10:30:54 | [INFO    ]  - Matching tile pair (4, 4)\n2023-10-03 10:30:56 | [INFO    ]  - Matching tile pair (5, 4)\n2023-10-03 10:30:57 | [INFO    ]  - Matching tile pair (5, 5)\n2023-10-03 10:30:58 | [INFO    ] Restoring full image coordinates of matches...\n2023-10-03 10:30:58 | [INFO    ] Matching by tile completed.\n2023-10-03 10:30:58 | [INFO    ] Matching done!\n2023-10-03 10:30:58 | [INFO    ] Performing geometric verification...\n2023-10-03 10:30:59 | [INFO    ] Pydegensac found 1061 inliers (36.49%)\n2023-10-03 10:30:59 | [INFO    ] Geometric verification done.\n2023-10-03 10:31:00 | [INFO    ] [Timer] | [Matching] preselection=1.626, matching=12.153, geometric_verification=0.146, \nFunction match took 15.1186 seconds\n</pre> <p>You can now extract the matched features from the Matcher object and save them in the current Epoch object</p> In\u00a0[8]: Copied! <pre># Define a dictionary with empty Features objects for each camera, which will be filled with the matched keypoints, descriptors and scores\nf = {cam: icecore.Features() for cam in cams}\n\n# Stack matched keypoints, descriptors and scores into Features objects\nf[cams[0]].append_features_from_numpy(\n    x=matcher.mkpts0[:, 0],\n    y=matcher.mkpts0[:, 1],\n    descr=matcher.descriptors0,\n    scores=matcher.scores0,\n)\nf[cams[1]].append_features_from_numpy(\n    x=matcher.mkpts1[:, 0],\n    y=matcher.mkpts1[:, 1],\n    descr=matcher.descriptors1,\n    scores=matcher.scores1,\n)\n\n# Store the dictionary with the features in the Epoch object\nepoch.features = f\n</pre> # Define a dictionary with empty Features objects for each camera, which will be filled with the matched keypoints, descriptors and scores f = {cam: icecore.Features() for cam in cams}  # Stack matched keypoints, descriptors and scores into Features objects f[cams[0]].append_features_from_numpy(     x=matcher.mkpts0[:, 0],     y=matcher.mkpts0[:, 1],     descr=matcher.descriptors0,     scores=matcher.scores0, ) f[cams[1]].append_features_from_numpy(     x=matcher.mkpts1[:, 0],     y=matcher.mkpts1[:, 1],     descr=matcher.descriptors1,     scores=matcher.scores1, )  # Store the dictionary with the features in the Epoch object epoch.features = f In\u00a0[10]: Copied! <pre># Compute the camera baseline from a-priori camera positions\nbaseline = np.linalg.norm(\n    cfg.georef.camera_centers_world[0] - cfg.georef.camera_centers_world[1]\n)\n\n# Initialize RelativeOrientation class with a list containing the two\n# cameras and a list contaning the matched features location on each camera.\nrelative_ori = sfm.RelativeOrientation(\n    [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],\n    [\n        epoch.features[cams[0]].kpts_to_numpy(),\n        epoch.features[cams[1]].kpts_to_numpy(),\n    ],\n)\n\n# Estimate the relative orientation\nrelative_ori.estimate_pose(\n    threshold=cfg.matching.pydegensac_threshold,\n    confidence=0.999999,\n    scale_factor=baseline,\n)\n\n# Store result in camera 1 object\nepoch.cameras[cams[1]] = relative_ori.cameras[1]\n\ntimer.update(\"relative orientation\")\n</pre> # Compute the camera baseline from a-priori camera positions baseline = np.linalg.norm(     cfg.georef.camera_centers_world[0] - cfg.georef.camera_centers_world[1] )  # Initialize RelativeOrientation class with a list containing the two # cameras and a list contaning the matched features location on each camera. relative_ori = sfm.RelativeOrientation(     [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],     [         epoch.features[cams[0]].kpts_to_numpy(),         epoch.features[cams[1]].kpts_to_numpy(),     ], )  # Estimate the relative orientation relative_ori.estimate_pose(     threshold=cfg.matching.pydegensac_threshold,     confidence=0.999999,     scale_factor=baseline, )  # Store result in camera 1 object epoch.cameras[cams[1]] = relative_ori.cameras[1]  timer.update(\"relative orientation\") <pre>2023-10-03 10:33:11 | [INFO    ] Relative Orientation - valid points: 963/1061\n2023-10-03 10:33:11 | [INFO    ] Relative orientation Succeded.\n</pre> In\u00a0[13]: Copied! <pre>triang = sfm.Triangulate(\n    [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],\n    [\n        epoch.features[cams[0]].kpts_to_numpy(),\n        epoch.features[cams[1]].kpts_to_numpy(),\n    ],\n)\npoints3d = triang.triangulate_two_views(\n    compute_colors=True, image=epoch.images[cams[1]].value, cam_id=1\n)\n\n# Update timer\ntimer.update(\"triangulation\")\n</pre> triang = sfm.Triangulate(     [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],     [         epoch.features[cams[0]].kpts_to_numpy(),         epoch.features[cams[1]].kpts_to_numpy(),     ], ) points3d = triang.triangulate_two_views(     compute_colors=True, image=epoch.images[cams[1]].value, cam_id=1 )  # Update timer timer.update(\"triangulation\") <pre>2023-10-03 10:40:02 | [INFO    ] Point triangulation succeded: 1.0.\n2023-10-03 10:40:02 | [INFO    ] Point colors interpolated\n</pre> In\u00a0[24]: Copied! <pre># Extract the image coordinates of the targets from the Targets object\nimage_coords = [\n    epoch.targets.get_image_coor_by_label(cfg.georef.targets_to_use, cam_id=id)[0] for id, cam in enumerate(cams)\n]\nprint(f\"Targets coordinates on image 0:\\n{image_coords[0]}\")\nprint(f\"Targets coordinates on image 1:\\n{image_coords[1]}\")\n\nobj_coords = epoch.targets.get_object_coor_by_label(cfg.georef.targets_to_use)[0]\nprint(f\"Targets coordinates in object space:\\n{obj_coords}\")\n</pre> # Extract the image coordinates of the targets from the Targets object image_coords = [     epoch.targets.get_image_coor_by_label(cfg.georef.targets_to_use, cam_id=id)[0] for id, cam in enumerate(cams) ] print(f\"Targets coordinates on image 0:\\n{image_coords[0]}\") print(f\"Targets coordinates on image 1:\\n{image_coords[1]}\")  obj_coords = epoch.targets.get_object_coor_by_label(cfg.georef.targets_to_use)[0] print(f\"Targets coordinates in object space:\\n{obj_coords}\") <pre>Targets coordinates on image 0:\n[[4002.709  3543.0627]\n [1611.3804 1420.486 ]\n [4671.8179 3465.3052]]\nTargets coordinates on image 1:\n[[1003.5037 3859.1558]\n [5694.3535  620.8673]\n [1565.0667 3927.6331]]\nTargets coordinates in object space:\n[[  49.6488  192.0875   71.7466]\n [-532.7409  391.02    238.8015]\n [  51.1682  210.4649   70.9032]]\n</pre> <p>You can now perform the absolute orientation in a similar way as before:</p> In\u00a0[25]: Copied! <pre># Initialize AbsoluteOrientation object with a list containing the two\nabs_ori = sfm.Absolute_orientation(\n    (epoch.cameras[cams[0]], epoch.cameras[cams[1]]),\n    points3d_final=obj_coords,\n    image_points=image_coords,\n    camera_centers_world=cfg.georef.camera_centers_world,\n)\n\n# Estimate the absolute orientation transformation\nT = abs_ori.estimate_transformation_linear(estimate_scale=True)\n\n# Transform the 3D points\npoints3d = abs_ori.apply_transformation(points3d=points3d)\n</pre> # Initialize AbsoluteOrientation object with a list containing the two abs_ori = sfm.Absolute_orientation(     (epoch.cameras[cams[0]], epoch.cameras[cams[1]]),     points3d_final=obj_coords,     image_points=image_coords,     camera_centers_world=cfg.georef.camera_centers_world, )  # Estimate the absolute orientation transformation T = abs_ori.estimate_transformation_linear(estimate_scale=True)  # Transform the 3D points points3d = abs_ori.apply_transformation(points3d=points3d) <pre>2023-10-03 10:49:29 | [INFO    ] Point triangulation succeded: 1.0.\n</pre> <p>You can now save the estimated camera positions and the 3D points into the current Epoch object. The 3D coordinates of the points in the object space can be saved as a ICEpy4D Points object, as follows:</p> In\u00a0[28]: Copied! <pre># Store the absolute orientation transformation in the camera objects\nfor i, cam in enumerate(cams):\n    epoch.cameras[cam] = abs_ori.cameras[i]\n\n# Convert the 3D points to an icepy4d Points object\npts = icecore.Points()\npts.append_points_from_numpy(\n    points3d,\n    track_ids=epoch.features[cams[0]].get_track_ids(),\n    colors=triang.colors,\n)\n\n# Store the points in the Epoch object\nepoch.points = pts\n\n# Update timer\ntimer.update(\"absolute orientation\")\n</pre> # Store the absolute orientation transformation in the camera objects for i, cam in enumerate(cams):     epoch.cameras[cam] = abs_ori.cameras[i]  # Convert the 3D points to an icepy4d Points object pts = icecore.Points() pts.append_points_from_numpy(     points3d,     track_ids=epoch.features[cams[0]].get_track_ids(),     colors=triang.colors, )  # Store the points in the Epoch object epoch.points = pts  # Update timer timer.update(\"absolute orientation\") <p>You can save the current Epoch object as a pickle file in the previously defined epoch directory.</p> In\u00a0[27]: Copied! <pre># Save epoch as a pickle object\nif epoch.save_pickle(f\"{epoch.epoch_dir}/{epoch}.pickle\"):\n    logger.info(f\"{epoch} saved successfully\")\nelse:\n    logger.error(f\"Unable to save {epoch}\")\n</pre> # Save epoch as a pickle object if epoch.save_pickle(f\"{epoch.epoch_dir}/{epoch}.pickle\"):     logger.info(f\"{epoch} saved successfully\") else:     logger.error(f\"Unable to save {epoch}\") <pre>2023-10-03 10:50:49 | [INFO    ] 2022-05-01_14-01-15 saved successfully\n</pre>"},{"location":"module5/single_epoch_stereo_reconstruction/#stereo-reconstruction","title":"Stereo reconstruction\u00b6","text":"<p>This notebook introduce you to the procedure of building a 3D reconstruction from two stereo cameras. The procedure is as follows:</p> <ol> <li>Create a <code>Epoch</code> object that stores all the relevant information for the 3D reconstruction. This includes:<ul> <li>Load the images from the two cameras taken at the same time</li> <li>Create the camera objects given the pre-calibrated intrinsic parameters</li> <li>Load information about the targets</li> </ul> </li> <li>Detect and match features on the two images</li> <li>Perform a relative orientation between the two cameras based on the matched tie points (optionally, you can also set the scale of the reconstruction, e.g., by giving the camera baseline)</li> <li>Triangulate the tie points into the object space</li> <li>Perform an absolute orientation to refine the camera positions and the 3D points</li> <li>(optional) Perform a bundle adjustment to refine the camera positions and the 3D points by using Agisoft Metashape</li> </ol>"},{"location":"module5/single_epoch_stereo_reconstruction/#initialization","title":"Initialization\u00b6","text":""},{"location":"module5/single_epoch_stereo_reconstruction/#the-epoch-class","title":"The Epoch class\u00b6","text":"<p>An Epoch object is the main object that stores all the information about the 3D reconstruction. It is initialized by defining the timestamp of the epoch and, optionally, by storing the other information about the epoch (e.g., images, cameras, tie points etc...).</p> <p>Let's first define a dictionary that contains the information about all the images that we are going to process. This dictionary has the camera name as key and an <code>Image</code> object as value. The <code>Image</code> object is an ICEpy4D object that manage the image in a lazy way (i.e., it doesn't load the image into memory, but read only image metadata). An image object is initialized by passing the path to the image to the constructor, and it automatically reads the image timestamp and other exif metadata.</p> <pre>image = Image('path_to_image')\n</pre> <p>To get the image timestamp, you can access the <code>timestamp</code> attribute of the image object.</p> <pre>image.timestamp\n</pre>"},{"location":"module5/single_epoch_stereo_reconstruction/#stereo-processing","title":"Stereo Processing\u00b6","text":"<p>The stereo processing is carried out for each epoch in order to find matched features, estimating camera pose, and triangulating the 3D points. The output of this step is a set of 3D points and their corresponding descriptors.</p> <p>The same procedure is then iterated in a big loop for all the epoches in a multitemporal processing (refer to the <code>multitemporal_workflow.ipynb</code> notebook).</p>"},{"location":"module5/single_epoch_stereo_reconstruction/#feature-matching-with-lightglue","title":"Feature matching with LightGlue\u00b6","text":"<p>Wide-baseline feature matching is performed using the LightGlue algorithm. Refer to the <code>matching.ipynb</code> notebook for more details about the matching process and explanation of the parameters.</p>"},{"location":"module5/single_epoch_stereo_reconstruction/#3d-scene-reconstruction","title":"3D Scene reconstruction\u00b6","text":""},{"location":"module5/single_epoch_stereo_reconstruction/#relative-orientation","title":"Relative orientation\u00b6","text":"<p>First, perform Relative orientation of the two cameras by using the matched features and the a-priori camera interior orientation.</p> <p>To perform the relative orientation, you have to define a <code>RelativeOrientation</code> object by passing first a list containing the two camera objects and then a list containing the matched features on each image. The matched features are Nx2 numpy arrary containing the x-y pixel coordinates of the matched features.</p> <pre>relative_orientation = RelativeOrientation([camera1, camera2], [features1, features2])\n</pre> <p>To get the pixel coordinates of the matched features as numpy arrays you can use the <code>kpts_to_numpy()</code> method of a Features object (that is now stored into the current Epoch object).</p> <pre>epoch.features[cams[0]].kpts_to_numpy()\n</pre> <p>The relative orientation is then performed by calling the <code>estimate_pose</code> method of the object. You can pass some additional parameters such as the camera baseline (in meters) to scale the reconstruction.</p> <pre>relative_orientation.estimate_pose(scale_factor=camera_baseline)\n</pre>"},{"location":"module5/single_epoch_stereo_reconstruction/#triangulation","title":"Triangulation\u00b6","text":"<p>You can now triangulate the tie points (i.e., the matched features) into the object space. Similarly as before, you have to define a <code>Triangulation</code> object by passing first a list containing the two camera objects and then a list containing the matched features on each image. The matched features are Nx2 numpy arrary containing the x-y pixel coordinates of the matched features.</p> <pre>triangulation = Triangulation([camera1, camera2], [features1, features2])\n</pre> <p>The triangulation is then performed by calling the <code>triangulate</code> method of the object.</p> <pre>triangulation.triangulate()\n</pre> <p>You can decide if you want to compute the point colors by interpolating them from one of the two images. If so, you have to pass the index of the image that you want to use for the color interpolation. For example, to interpolate colors from image 1, you can do:</p> <pre>triangulation.triangulate(compute_colors=True, image=epoch.images[cams[1]].value, cam_id=1)\n</pre>"},{"location":"module5/single_epoch_stereo_reconstruction/#absolute-orientation","title":"Absolute orientation\u00b6","text":"<p>Now, you can perform an absolute orientation of the current solution (i.e., cameras' exterior orientation and 3D points) by using the ground control points.</p> <p>The coordinates of the two cameras are used as additional ground control points for estimating a Helmert transformation.</p> <p>You need first to extract the image and object coordinates of the targets from the Target object, stored into the current Epoch. Alternatively, you can also define manually the coordinates of the targets as numpy arrays. To extract the image coordinates of the targets, you can use the <code>get_image_coor_by_label()</code> method of the Target object, by passing the list of the target labels that you want to extract and the camera id they are referred to.</p> <pre>epoch.targets.get_image_coor_by_label(cfg.georef.targets_to_use, cam_id=id)\n</pre> <p>To get the targets object coordinates, you can use the <code>get_object_coor_by_label()</code> method of the Target object, by passing the list of the target labels that you want to extract.</p> <pre>epoch.targets.get_object_coor_by_label(cfg.georef.targets_to_use)\n</pre> <p>Eventually, you need to have 3 numpy array, all with the number of rows (the number of targets).</p>"},{"location":"module6/custom-viewer/","title":"Customise the viewer","text":"<p>Once loaded the point cloud in the Web Viewer as described here, it is possible to add custom functionalities, enriching the 3D scene with additional entities that could support both environment exploration and project storytelling. In particular, we will see example on how to include: 1. Georeferenced annotations 2. Drone images oriented on the pointcloud</p>"},{"location":"module6/custom-viewer/#inserting-annotations","title":"Inserting annotations","text":"<p>Adding custom annotations is particularly useful if it is needed to highlights particular position of the scene (measurement stations, base camp location) or if it is necessary to integrate actions or media. Indeed, having a simple familiarity with HTML code, it is possible to further customise the description label associated to an annotation.</p> <p>Generally, in a normal exploration session of the viewer, users can add georeferenced position by simply clicking the annotation icon on the Potree sidebar on the Tools &gt; Measurement section.</p> <p></p> <p>Once clicked, it is necessary to navigate in the viewer, look for the desired perspective on the scene and then right-click again on the point where you'd like to position the annotation. A black label with the text Annotation Title will then be added to the scene. The new element will also be visile in the Scene &gt; Annotations section in the sidebar. By clicking on it there, in the Properties it will be possible to type the desired Title and Description.</p> <p></p> <p>This solution represents the easiest way to include annotations but it is temporary: when the user refreshes the page, reloading the content of the Potree page, the annotations inserted during that session will disappear.</p> <p>If the aim is to insert permanently some annotations in the scene, it is needed to code them inside the source. To achieve this, create a new file named annotations.js inside the js folder. Then call it inside the index.html structure:</p> <pre><code>&lt;body&gt;\n...\n&lt;!-- Import ANNOTATIONS--&gt;\n&lt;script src=\"js/annotations.js\"&gt;&lt;/script&gt;\n&lt;/body&gt;\n</code></pre> <p>Now, let's define some annotations inside the annotations.js file with the help of the viewer. With localhost/belvedere-example open, navigate in the viewer and position yourself with the perspective you'd like to have when you click on a given annotation. For example, let's try to configure the annotation described earlier in the code.</p> <p>Before working on the code, explore the point cloud in the viewer, activate the Point Measurement Tool and double-click in correspondence of the point where you'd like to locate the annotation. Hence, explore the Scene section in the Potree Sidebar and select the point measurement element. In the lower part of the section now you see the details of the measurement as well as the clicked point coordinates. Click on the copy icon next to the coordinates values: you will need this data to position your new annotation.</p> <p></p> <p>Hence, to complete the procedure, you need to define the camera view to be set when the annotation is clicked in Potree. In order to do this, rotate and move the model view and look for the desired perspective. Then, in the scene section of the sidebar, click on Camera: you will make visible a new Properties panel in which the coordinates linked to the camera position and camera target location that defines the actual view in the scene will be displayed.</p> <p></p> <p>Then, open the annotations.js in your preferred text editor. Copy and paste the following JS code:</p> <pre><code>/* Annotations definition */\nfunction createAnnotation(scene, titleText, position, cameraPosition, cameraTarget, description) {\n    // Create title element\n    let titleElement = $(`&lt;span&gt;${titleText}&lt;/span&gt;`);\n    // Create Potree.Annotation instance\n    let annotation = new Potree.Annotation({\n        position: position,\n        title: titleElement,\n        cameraPosition: cameraPosition,\n        cameraTarget: cameraTarget,\n        description: description\n    });\n    // Set the annotation to be visible\n    annotation.visible = true;\n    // Add the annotation to the scene\n    scene.annotations.add(annotation);\n    // Override toString method for title element\n    titleElement.toString = () =&gt; titleText;\n}\n</code></pre> <p>This code snippet defines a function that creates an annotation with a specified title, position, camera settings, and description and adds it to a 3D scene using the Potree library. Given such definition, a new annotation can be finally created, using the info collected through the viewer and copying and pasting them in the correct rows.</p> <pre><code>...\n// Annotation example 1\ncreateAnnotation(\n    scene,\n    \"North-western lobe\",\n    [416441.129, 5091064.330, 1867.242], //Pasted from the Point Measurement Tool\n    [416559.997, 5091132.073, 1920.620], //Pasted from the camera object (position)\n    [416439.927, 5091060.932, 1869.774], //Pasted from the camera object (target)\n    'This is the glacier section we surveyed with a terrestrial laser scanner as well as with UAVs'\n)\n</code></pre> <p>Once saved, such an edit will allows to easily view and access the defined annotation at each new session. If additional annotations are needed, simply copy and paste the last code snippet, changing title and coordinates according to the case study needs.</p>"},{"location":"module6/custom-viewer/#inserting-oriented-images","title":"Inserting oriented images","text":"<p>Once loaded the point cloud in the Web Viewer, it is also possible to include in the Viewer oriented cameras. This is particularly useful for showing particular portions of the site and highlighting details on pictures taken from the drone and used for the reconstruction of the 3D model.</p> <p>In order to load the images in the viewer platform, first copy and paste in the img_selected the folder containing:</p> <ul> <li> <p>Oriented images files: they could be in any desired file formats: jpg, tif etc. Be sure that images are undistorted.</p> </li> <li> <p>calib.xml: this file includes information on the parameters of the camera adopted for taking the pictures used for the photogrammetric reconstruction. Be sure that width and height values match the ones of the chosen pictures. All the other parameters are set to 0 except the focal length. For example:</p> </li> </ul> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;calibration&gt;\n  &lt;projection&gt;frame&lt;/projection&gt;\n  &lt;width&gt;8192&lt;/width&gt;\n  &lt;height&gt;5460&lt;/height&gt;\n  &lt;f&gt;8215.93777&lt;/f&gt;\n  &lt;cx&gt;0&lt;/cx&gt;\n  &lt;cy&gt;0&lt;/cy&gt;\n  &lt;b1&gt;0&lt;/b1&gt;\n  &lt;b2&gt;0&lt;/b2&gt;\n  &lt;k1&gt;0&lt;/k1&gt;\n  &lt;k2&gt;0&lt;/k2&gt;\n  &lt;k3&gt;0&lt;/k3&gt;\n  &lt;date&gt;2022-05-26T08:27:27Z&lt;/date&gt;\n&lt;/calibration&gt;\n</code></pre> <ul> <li>cameras_eo.txt: this file in the first row contains the information about the coordinate system in which the images and the model have been georeferenced. Then, information about position and rotation of each single image file are listed associated to the filenames. Be sure that rotation angles are defined as Omega, Phi and Kappa. Example:</li> </ul> <pre><code># CoordinateSystem: PROJCS[\"WGS 84 / UTM zone 32N\",GEOGCS[\"WGS 84\",DATUM[\"World Geodetic System 1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],TOWGS84[0,0,0,0,0,0,0],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.01745329251994328,AUTHORITY[\"EPSG\",\"9102\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\",AUTHORITY[\"EPSG\",\"9807\"]],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",9],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AUTHORITY[\"EPSG\",\"32632\"]]\n#Label X Y Z Omega Phi Kappa X_est Y_est Z_est Omega_est Phi_est Kappa_est\nDJI_20221123144400_0268.jpg       593656.566250 5089108.835697 909.422444 69.797275 -11.168079 -4.525146\nDJI_20221123144407_0271.jpg       593656.275265 5089107.376767 908.167646 76.005119 -11.476563 -3.278955\nDJI_20221123144437_0284.jpg       593667.208196 5089110.605787 904.912258 116.932215 51.336722 -22.059546\nDJI_20221123142343_0084.jpg       593661.475374 5089132.074970 917.423793 -52.683726 25.008452 161.764827\n</code></pre> <p>Once the files are copied in the target folder - in this example img_selected - it's time to create and modify the dedicated orientedcameras.js file in the js folder according to the need of the case.</p> <p>In the first lines of the script it is needed to declare the paths of both the camera parameters and oriented images files.</p> <p>Then, the OrientedImageLoader function is applied and the images chunk is added to the scene. A useful tip could be defining also an images.name associated to the loaded chunk: this will help if advanced functions for hiding specific images or elements are later implemented in the template.</p> <pre><code>/* Loading oriented images chunks */\n/* First chunk of images */\n/* Setting the paths for camera parameters and images list */\nconst cameraParams1 = \"img-selected/calib.xml\";\nconst imageParams1 = \"img-selected/cameras_eo.txt\";\n\nPotree.OrientedImageLoader.load(cameraParams1, imageParams1, viewer).then(images =&gt; {\n    images.visible = true; // change this to false if you'd like to hide images at first loading of the page\n    viewer.scene.addOrientedImages(images);\n    images.name = 'chunk1';\n});\n</code></pre> <p>If you'd like to define another oriented images chunk, copy the entire code block of the first chunk and paste it right in first row below it in the js script. Then modify it according to your needs.</p> <p>N.B.: Variable and constant names should be unique!</p> <p>Now, refreshing the viewer page results in displaying several small black rectangles bordered in green, one for each oriented image inserted in the scene. Their position and orientation in space are based on the parameters passed through the images_eo.tc file. Also, if the oriented images integration has been successfull, you should be able to see in the Sidebar &gt; Scene section a new images entity in the list.</p> <p></p> <p>When clicking on one of the rectangles, the user will enter the picture visualisation mode, accessing the selected oriented images aligned to the point clouds. This means that not only it is possible to analyse in high resolution the images that originated the cloud through the photogrammetric reconstruction, but also to execute measurements with the support of the image and reproject them in 3D on the point cloud.</p> <p></p>"},{"location":"module6/getting-started/","title":"Getting started","text":""},{"location":"module6/getting-started/#preparing-the-test-environment","title":"Preparing the test environment","text":"<p>This section will cover how to test locally a Potree web page before deploying it to a server.</p> <p>Due to strict security policies in browsers, it is not possible to open potree html files directly on your pc because potree needs permission to load files. You have to put all necessary source files and the pointcloud on a webserver to view the result. You can, however, install a local webserver on your pc. XAMPP, which contains Apache Webserver as well as PHP and MySQL, is the suggested solution for testing locally potree pages.</p> <p>After you\u2019ve installed and started Apache/XAMPP, you can access files in your htdocs directory through a localhost URL. Assuming your htdocs directory is C:xampphtdocs, you can access it in your browser with:</p> <p><code>http://localhost</code></p>"},{"location":"module6/getting-started/#choosing-a-text-editor","title":"Choosing a text editor","text":"<p>This module is strongly focused in coding, despite if no prior knowledge of HTML, CSS and JS is needed. For this reason, a proper and efficient text editor is highly recommended. We suggest to use Visual Studio Code with Prettier extension installed.</p>"},{"location":"module6/intro/","title":"Introduction","text":"<p>Potree is a free open-source WebGL based point cloud renderer for large point clouds, developed at the Institute of Computer Graphics and Algorithms, TU Wien, Austria. There are 3 ways for accessing Potree:</p> <ul> <li> <p>Potree Desktop: Desktop version of Potree. Allows drag&amp;drop of point clouds into the viewer (https://github.com/potree/PotreeDesktop/releases)</p> </li> <li> <p>Potree Converter: Convert your point cloud to the Potree format (https://github.com/potree/PotreeConverter/releases)</p> </li> <li> <p>Potree Develop: Edit and develop several potree examples (https://github.com/potree/potree/)</p> </li> </ul>"},{"location":"module6/intro/#potree-desktop","title":"Potree Desktop","text":"<p>A desktop/portable version of the web-based point cloud viewer Potree, thanks to Electron. This versione allows you to load converted point clouds from your hard disk or external drive. It\u2019s also portable, so you can put your models together with the viewer on a USB drive and open it wherever you go. It\u2019s only been tested on Windows at the moment. It may not work on other systems or you may only be able to use it on the same Operating System that you\u2019ve initially built it on. You can also drag&amp;drop cloud.js files into the window to add point clouds to the scene.</p>"},{"location":"module6/intro/#installation","title":"Installation","text":"<p>Download the Potree ,zip files for Windows from this link: https://github.com/potree/PotreeDesktop/releases</p>"},{"location":"module6/intro/#graphic-user-interface","title":"Graphic User Interface","text":"<p>Once you downloaded the installer .zip, extract all the files and execute PotreeDesktop.bat. Then, a new window will appear with the main Graphic User Interface of Potree.</p> <p></p> <p>The Potree GUI is made of 2 components:</p> <ul> <li> <p>Sidebar: on the left, it includes all the main features and tools for point-clouds elaborations in the Potree environment.</p> </li> <li> <p>Viewer: on the right, it is the actual space for visually exploring and navigating the point-clouds.</p> </li> </ul>"},{"location":"module6/intro/#pointcloud-conversion","title":"Pointcloud Conversion","text":"<p>PotreeDesktop provides also a user-friendly interface for converting pointclouds in a Potree-compatible format. In order to do this, you can simply drag&amp;drop the desired poincloud file (in a .las/.laz format) inside the viewer window. In a new window, after checking that the output target folder and the input files directory are defined as desired, it is required to select the PotreeConverter version to be adopted for the processing. Version 2.0 is the suggested one, generating only 3 files instead of thousands to millions. Click on the Start Conversion button to continue.</p> <p></p> <p>After the processing, the pointcloud is loaded in the viewer and the converted files are available in the previously defined output target directory.</p> <p></p>"},{"location":"module6/intro/#potree-converter","title":"Potree Converter","text":"<p>PotreeConverter generates an octree LOD structure for streaming and real-time rendering of massive point clouds. The results can be viewed in web browser with Potree. The conversion operation produces a total of 3 files, with better support for standard LAS attributes and arbitrary extra attributes.</p>"},{"location":"module6/intro/#getting-started","title":"Getting started","text":"<p>Download and extract the PotreeConverter files for Windwos from this link: https://github.com/potree/PotreeConverter/releases</p> <p></p> <p>Be sure that the PotreeConverter.exe is present inside the unzipped folder.</p> <p>For making easier the conversion process, it is suggested to copy and paste in the unzipped PotreeConverter folder the pointcloud to be converted in a .las/.laz format.</p> <p>For Windows users:</p> <p>For starting the conversion, you can write \u201ccmd\u201d in File Explorer Address Bar and press enter. The cmd shell will be opened having as referenced directory the PotreeConverter folder. In order to trigger the conversion, modify the following code according to your needs and then paste it in the cmd shell:</p> <p><code>.\\PotreeConverter.exe pointcloud.las -o output -p index</code></p> <p>Where:</p> <ul> <li> <p>PotreeConverter.exe specify the executable file for the convertion;</p> </li> <li> <p>pointcloud.las select the point cloud (las or laz) to be converter. This format is valid in case the point cloud file is located in the same folder of the converter exe. Otherwise it is necessary to specify the complete path of the file;</p> </li> <li> <p>output is the name of the directory where to save the converted point cloud. In this case too, if the target folder is outside the converter one, it is necessary to specify the entire path.</p> </li> <li> <p>index is the default name to be given to the output file.</p> </li> </ul> <p>After launching, the conversion is executed and details about each step of the the processing are reported on the cmd shell window.</p> <p></p> <p>If successful, the conversion procedure gives as output a folder named as defined in the command (in this case output) containing 3 elements:</p> <ol> <li> <p>a folder named libs that contains all the required libraries and scripts to make Potree operative (e.g. three.js, openlayers\u2026);</p> </li> <li> <p>a folder named pointclouds containing an index directory that collects that 4 files resulting from the conversion of the original pointclouds. The metadata.json is the file called in the Potree environment when loading the pointcloud to the scene;</p> </li> <li> <p>an html file called index.html that includes a basic structure for a web page with a Potree viewer.</p> </li> </ol>"},{"location":"module6/intro/#potree-develop","title":"Potree Develop","text":"<p>For more details about the codes and libraries on which Potree is built, it is recommended to check the official Github repository: https://github.com/potree/potree. Many examples on how to implement Potree functionalities and customize them are available on the example folder with formatted html files dedicated to each case.</p>"},{"location":"module6/module6/","title":"Overview","text":"<p>The module 6 of the course is dedicated to defining a WebGL-based visualisation of 3D georeferenced data using Potree.</p>"},{"location":"module6/module6/#learning-outcomes","title":"Learning outcomes","text":"<p>Through a combination of theoric concepts and aa guided application, students will understands the basics of implementing a web platform for 3D data visualisation. In particular, they will be introuced to some basic concept of HTML, CSS and JS, setting up a web page that will visualise the 2022 pointcloud, allowing interactive exploration, measuring and cross-section extraction with simple clicks.</p>"},{"location":"module6/module6/#table-of-contents","title":"Table of contents","text":"<ol> <li> <p>Introduction: this section aims to provide an overview of the context of the tool used, describing its general features and references.</p> </li> <li> <p>Getting started: this step details how students need to navigate through the developing and testing environment for Potree, presenting the needed set up.</p> </li> <li> <p>3D Viewer implementation: this section illustrates how to define the basic settings and input data for a simple pointcloud viewer with Potree, introducting also useful native elaboration features for the case study of the glacier.</p> </li> <li> <p>Customise the viewer: this part additionally guides students on how to implement additional useful features in Potree, such as georeferenced annotations and oriented images.</p> </li> </ol>"},{"location":"module6/module6/#software","title":"Software","text":"<p>The adopted tool for the practical sessions of this module is Potree: https://github.com/potree/potree</p> <p>Potree is a free open-source WebGL based point cloud renderer for large point clouds, developed at the Institute of Computer Graphics and Algorithms, TU Wien, Austria.</p> <p>Project details</p> <ul> <li> <p>The multi-res-octree algorithms used by the viewer were developed at the Vienna University of Technology by Michael Wimmer and Claus Scheiblauer as part of the Scanopy Project.</p> </li> <li> <p>Three.js, the WebGL 3D rendering library on which potree is built.</p> </li> <li> <p>plas.io point cloud viewer. LAS and LAZ support have been taken from the laslaz.js implementation of plas.io. Thanks to Uday Verma and Howard Butler for this!</p> </li> <li> <p>Harvest4D Potree currently runs as Master Thesis under the Harvest4D Project</p> </li> <li> <p>Christian Boucheny (EDL developer) and Daniel Girardeau-Montaut (CloudCompare). The EDL shader was adapted from the CloudCompare source code!</p> </li> </ul>"},{"location":"module6/web-viewer/","title":"3D Viewer implementation","text":"<p>In order to start designing the web page that is going to host the Potree-based viewer, start creating a new folder within C:\\xampp\\htdocs. Name it according to the name you want to associate to the url. For example, if you'll name it belvedere-example, when you'll access the test web page you will have to search for localhost/belvedere-example on the browser.</p> <p>The folder you have just created will contain all the files and assets needed to enable the Potree viewer. In particular:</p> <ul> <li> <p>index.html: this file with HTML extension will be the homepage of the 3D web viewer, containing the basic settings for the GUI and including the paths to additional external files for style (css) and/or functionalities (js).</p> </li> <li> <p>libs folder: it contains many subfolders for all libraries' dependencies for making functionable the viewer.</p> </li> <li> <p>licenses folder: it includes license specifications for the libraries used in the development.</p> </li> <li> <p>css folder: it contains file(s) that define the style and appearance of the web page.</p> </li> <li> <p>js folder: this includes all the scripting JavaScript files that enable native and/or custom functionalities and actions on the web page.</p> </li> <li> <p>pointclouds folder: inside this space converted pointclouds are saved with their ancillary files.</p> </li> <li> <p>img_selected folder: it containes oriented images that the viewer developer is willing to integrate on the platform. Together with the picture files, camera certificates and images orientation parameters are saved in this folder.</p> </li> </ul>"},{"location":"module6/web-viewer/#defining-indexhtml","title":"Defining index.html","text":"<p>The index.html file includes the main settings for the web page that contains the custom Potree viewer. At the top of the file, the doctype declaration for HTML file is included as follows:</p> <pre><code> &lt;!DOCTYPE html&gt;\n</code></pre> <p>Hence, the root of the page is initialised with the <code>html</code> tag, that includes the other main tag of an html page - <code>head</code> and <code>body</code> - as well as the indication to the browser on how to interprete the language of the text in the page.</p> <pre><code>&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    ...\n&lt;/head&gt;\n&lt;body&gt;\n    ...\n&lt;/body&gt;\n&lt;/html&gt;  \n</code></pre> <p>The <code>head</code> tag houses the metadata of your website. For example, information contained in this part defines the title that will appear on the browser window when the page is loaded as well as other important metadata regarding the content and/or the author(s) of the page. Connection to stylesheets (.css files) are also inserted here. Most of the information inserted will remain invisible to normal visitors of the page but represent relevant information for browsers and search engines.</p> <p>These settings need to be defined in the first lines in the head element as follows:</p> <pre><code>&lt;head&gt;\n    &lt;!-- Specifying the character encoding for the document --&gt;\n    &lt;meta charset=\"utf-8\"&gt;\n    &lt;!-- Providing a shrt description for the page content --&gt;\n    &lt;meta name=\"description\" content=\"Simple Potree viewer\"&gt;\n    &lt;!-- Specifying the author of the document --&gt;\n    &lt;meta name=\"author\" content=\"Your name here\"&gt;\n    &lt;!-- Declaring needed information for website responsivity --&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=no\"&gt;\n    &lt;!-- Setting the title of the page --&gt;\n    &lt;title&gt;Belvedere glacier&lt;/title&gt;\n    &lt;!--Linking the needed stylesheet of the libraries adopted in the rest of the code--&gt;\n    &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"./libs/potree/potree.css\"&gt;\n    &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"./libs/jquery-ui/jquery-ui.min.css\"&gt;\n    &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"./libs/openlayers3/ol.css\"&gt;\n    &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"./libs/spectrum/spectrum.css\"&gt;\n    &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"./libs/jstree/themes/mixed/style.css\"&gt;\n    &lt;!-- Custom styles for this template --&gt;\n    &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"./css/style.css\"&gt;\n&lt;/head&gt;\n</code></pre> <p>Instead, the <code>body</code> section includes all the elements that represents the core content of the page, for example headings, texts, multimedia, links etc. It also containes the references to scripting .js and dependencies libraries.</p> <pre><code>&lt;body&gt;\n    &lt;!-- JS Dependencies for Potree --&gt;\n    &lt;script src=\"./libs/jquery/jquery-3.1.1.min.js\"&gt;&lt;/script&gt;\n    &lt;script src=\"./libs/spectrum/spectrum.js\"&gt;&lt;/script&gt;\n    &lt;script src=\"./libs/jquery-ui/jquery-ui.min.js\"&gt;&lt;/script&gt;\n    &lt;script src=\"./libs/other/BinaryHeap.js\"&gt;&lt;/script&gt;\n    &lt;script src=\"./libs/tween/tween.min.js\"&gt;&lt;/script&gt;\n    &lt;script src=\"./libs/d3/d3.js\"&gt;&lt;/script&gt;\n    &lt;script src=\"./libs/proj4/proj4.js\"&gt;&lt;/script&gt;\n    &lt;script src=\"./libs/openlayers3/ol.js\"&gt;&lt;/script&gt;\n    &lt;script src=\"./libs/i18next/i18next.js\"&gt;&lt;/script&gt;\n    &lt;script src=\"./libs/jstree/jstree.js\"&gt;&lt;/script&gt;\n    &lt;script src=\"./libs/potree/potree.js\"&gt;&lt;/script&gt;\n    &lt;script src=\"./libs/plasio/js/laslaz.js\"&gt;&lt;/script&gt;\n    &lt;!-- INCLUDE ADDITIONAL DEPENDENCIES HERE --&gt;\n&lt;/body&gt;\n</code></pre> <p>With the given settings, the index page its initialised and ready to be modified in order to insert custom elements for the potree Viewer.</p>"},{"location":"module6/web-viewer/#initialising-potree-viewer","title":"Initialising Potree viewer","text":"<p>In order to set up the basic Potree environment a first <code>div</code> element needs to be inserted in the body section.</p> <pre><code>&lt;body&gt;\n    ...\n    &lt;!--Loading settings for Potree viewer--&gt;\n    &lt;div class=\"potree_container\" style=\"position: relative; height:100%; width: 100%;\"&gt;\n        &lt;div id=\"potree_render_area\"&gt;\n        &lt;/div&gt;\n        &lt;div id=\"potree_sidebar_container\" style=\"width: 50%; height: 100%;\"&gt; &lt;/div&gt;\n    &lt;/div&gt;\n    ...\n&lt;/body&gt;\n</code></pre> <p>The div \"potree_container\" class element set up the position of the overall Potree container in the web page. Hence, the div element with the id \"potree_render_area\" is nested in the div container. It is a placeholder for the actual rendering area where the point cloud data will be visualized. Eventually, the div element which has the id \"potree_sidebar_container\" is a container for a sidebar thatcontains additional controls and tools related to the Potree viewer. </p>"},{"location":"module6/web-viewer/#loading-and-visualising-point-clouds","title":"Loading and visualising point clouds","text":"<p>Also, make sure you have the point cloud of the glacier in .las format. Once this product is obtained, you could convert the .las cloud using one of the method described previously. At the end of the procedure you will obtain a folder with the following structure:</p> <pre><code>converted_pointcloud_folder\n|\n\u2502   hierarchy.bin\n\u2502   metadata.json\n|   octree.bin  \n</code></pre> <p>Copy the whole folder and paste it inside the pointclouds folder. In order to include pointcloud(s) in the viewer, create a new .js file in the js folder and name it pointcloud.js. It will include all the detailed information for loading pointcloud properly in Potree. Then, open the pointcloud.js file with a text editor. Let's start by inserting:</p> <pre><code>/* Loading Potree viewer in the Potree Render Area defined in index.html */\nwindow.viewer = new Potree.Viewer(document.getElementById(\"potree_render_area\"));\n</code></pre> <p>This line will assign to the global variable viewer a new instance of the Potree viewer that is contained in the div of class potree_render_area in the index.html file.</p> <p>Then, the default settings for the viewer are defined:</p> <pre><code>...\n/* Defining appearance settings for rendering in the viewer */\nviewer.setEDLEnabled(true); // Enabling Eye-Dome-Lighting option\nviewer.setFOV(60); // Defining Field of view\nviewer.setPointBudget(2_000_000); // Defining point budget\nviewer.setDescription(\"Explore the oriented images of the model on a desktop browser.\"); // Setting a description to be shown on top of viewer\n</code></pre> <p>The sidebar structure is then defined as follows:</p> <pre><code>/* Loading the settings for the Potree sidebar */\nviewer.loadGUI(() =&gt; {\n    viewer.setLanguage('en');\n    viewer.toggleSidebar();\n    $(\"#menu_appearance\").next().show();\n    $(\"#menu_tools\").next().show();\n    /* Creating a new sidebar section for credits */\n    let section = $(`&lt;h3 id=\"menu_meta\" class=\"accordion-header ui-widget\"&gt;&lt;span&gt;Credits&lt;/span&gt;&lt;/h3&gt;&lt;div class=\"accordion-content ui-widget pv-menu-list\"&gt;&lt;/div&gt;`);\n    let content = section.last();\n    content.html(`\n    &lt;div class=\"pv-menu-list\"&gt;\n        &lt;li&gt;INSERT TEXT HERE&lt;/li&gt;\n    &lt;/div&gt;\n    `);\n    content.hide();\n    section.first().click(() =&gt; content.slideToggle());\n    section.insertBefore($('#menu_appearance'));\n});\n</code></pre> <p>Finally, the pointcloud is loaded by first creating a new Potree scene, setting it as the current scene of the viewer and then calling the .loadPointcloud() method.</p> <pre><code>...\n/* Define scene for the bridge */\nlet belvederescene = new Potree.Scene();\n/* Set scene to be loaded in the Potree Viewer */\nviewer.setScene(belvederescene);\n/* Loading point cloud data and its setting for rendering in Potree Viewer */\nPotree.loadPointCloud(\"./pointclouds/converted_pointcloud_folder/metadata.json\", \"Glacier cloud\", e =&gt; {\n    let pointcloud = e.pointcloud;\n    let material = pointcloud.material;\n    pointcloud.projection = \"+proj=utm +zone=32 +datum=WGS84 +units=m +no_defs +type=crs\";\n    material.size = 0.6;\n    material.pointSizeType = Potree.PointSizeType.ADAPTIVE;\n    material.shape = Potree.PointShape.CIRCLE;\n    material.activeAttributeName = \"rgba\"; // change this value to \"classification\" and uncomment the next 2 lines if you desire to show the classified point cloud\n    // material.intensityRange = [1, 100];\n    // material.gradient = Potree.Gradients.RAINBOW;\n    belvederescene.addPointCloud(pointcloud);\n    viewer.setFrontView();\n});\n</code></pre> <p>Now, in the <code>body</code> tag in the index.html file includes a link to the script file you just defined.</p> <pre><code>&lt;body&gt;\n...\n&lt;!-- Import POINTCLOUD--&gt;\n&lt;script src=\"js/pointcloud.js\"&gt;&lt;/script&gt;\n&lt;/body&gt;\n</code></pre> <p>Now, with XAMPP activated, access the localhost/belvedere-example address on your preferred browser. The basic viewer template for Belvedere is ready and visible. The OpenStreetMap webmap widget is also visible thanks to the definition of the reference system of the cloud in the pointcloud.js file.</p> <p></p>"}]}